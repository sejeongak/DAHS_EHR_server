{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedType, DistributedDataParallelKwargs\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import LongformerTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from collections import Counter\n",
    "import math\n",
    "from datasets import EHR_Longformer_Dataset\n",
    "from models.longformernormal import LongformerPretrainNormal, LongformerFinetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedType\n",
    "import os\n",
    "from utils.utils import seed_everything\n",
    "from transformers import LongformerTokenizer\n",
    "from datasets import EHR_Longformer_Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from models.longformernormal import LongformerPretrainNormal\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ExponentialLR, LambdaLR, CosineAnnealingWarmRestarts\n",
    "from pretrain_train import train\n",
    "import logging\n",
    "import sys\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "    \n",
    "# Required parameters\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"mortality90_CE_loss_reducelr_dropout\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"./results\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--checkpoint_dir\", type=str, default=\"./checkpoints\")\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument(\"--mode\", type=str, default=\"mortality90\")\n",
    "parser.add_argument(\"--vocab_size\", type=int, default=50265)\n",
    "parser.add_argument(\"--itemid_size\", type=int, default=4016)\n",
    "parser.add_argument(\"--unit_size\", type=int, default=60)\n",
    "parser.add_argument(\"--gender_size\", type=int, default=2)\n",
    "parser.add_argument(\"--continuous_size\", type=int, default=3)\n",
    "parser.add_argument(\"--task_size\", type=int, default=5)\n",
    "parser.add_argument(\"--max_position_embeddings\", type=int, default=5000)\n",
    "parser.add_argument(\"--max_age\", type=int, default=100)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--resume\", type=bool, default=False)\n",
    "parser.add_argument(\"--pin_memory\", type=bool, default=True)\n",
    "parser.add_argument(\"--nodes\", type=int, default=1)\n",
    "parser.add_argument(\"--gpus\", type=int, default=2)\n",
    "parser.add_argument(\"--start_epoch\", type=int, default=0)\n",
    "parser.add_argument(\"--epochs\", type=int, default=200)\n",
    "parser.add_argument(\"--log_every_n_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--acc\", type=int, default=1)\n",
    "parser.add_argument(\"--resume_checkpoint\", type=str, default=None)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "parser.add_argument(\"--embedding_size\", type=int, default=768)\n",
    "parser.add_argument(\"--num_hidden_layers\", type=int, default=12)\n",
    "parser.add_argument(\"--num_attention_heads\", type=int, default=6)\n",
    "parser.add_argument(\"--intermediate_size\", type=int, default=3072)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--dropout_prob\", type=float, default=0.3)\n",
    "parser.add_argument(\"--lora_dropout\", type=float, default=0.2)\n",
    "parser.add_argument(\"--classifier_dropout\", type=float, default=0.4)\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--gpu_mixed_precision\", type=bool, default=True)\n",
    "parser.add_argument(\"--patience\", type=int, default=10)\n",
    "parser.add_argument(\"--num_labels\", type=int, default=2)\n",
    "parser.add_argument(\"--use_lora\", type=bool, default=True)\n",
    "parser.add_argument(\"--gamma\", type=float, default=2.0)\n",
    "parser.add_argument(\"--beta\", type=float, default=0.99)\n",
    "parser.add_argument('--lora_weight', type=int, default=10)\n",
    "parser.add_argument('--classifier_weight', type=int, default=50)\n",
    "parser.add_argument(\"--loss\", type=str, default=\"cross_entropy\")\n",
    "parser.add_argument(\"--pretrain\", action='store_true', default=True)\n",
    "parser.add_argument(\"--clip_interval\", type=int, default=10)\n",
    "parser.add_argument(\"--pretrain_path\", type=str, default=\"best_pretrain_model_after_masking_35epoch.pth\")\n",
    "\n",
    "\n",
    "args = parser.parse_args({})\n",
    "args.attention_window = [512] * args.num_hidden_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "kwargs_handlers = [DistributedDataParallelKwargs(find_unused_parameters=True)]\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\" if args.gpu_mixed_precision else \"no\", kwargs_handlers=kwargs_handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def configure_optimizers(model, args):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['initial_lr'] = args.learning_rate\n",
    "    \n",
    "    # n_warmup_steps = int(n_steps * 0.1)\n",
    "    # n_decay_steps = n_steps - n_warmup_steps\n",
    "    \n",
    "    # warmup = LinearLR(optimizer, \n",
    "    #                     start_factor=0.01,\n",
    "    #                     end_factor=1.0,\n",
    "    #                     total_iters=n_warmup_steps)\n",
    "    \n",
    "    # decay = LinearLR(optimizer,\n",
    "    #                     start_factor=1.0,\n",
    "    #                     end_factor=0.01,\n",
    "    #                     total_iters=n_decay_steps)\n",
    "    \n",
    "    # scheduler = SequentialLR(optimizer, \n",
    "    #                             schedulers=[warmup, decay],\n",
    "    #                             milestones=[n_warmup_steps])\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=500, T_mult=2)\n",
    "    if args.resume:\n",
    "        scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
    "                                                first_cycle_steps=2690,\n",
    "                                                cycle_mult=1.5,\n",
    "                                                max_lr=0.0001,\n",
    "                                                min_lr=0.000001,\n",
    "                                                warmup_steps=269,\n",
    "                                                gamma=0.9,\n",
    "                                                last_epoch=args.resume_epoch\n",
    "                                                )\n",
    "    else:\n",
    "        scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
    "                                                first_cycle_steps=2690,\n",
    "                                                cycle_mult=1.5,\n",
    "                                                max_lr=0.0001,\n",
    "                                                min_lr=0.000001,\n",
    "                                                warmup_steps=269,\n",
    "                                                gamma=0.9,\n",
    "                                                )\n",
    "\n",
    "    return optimizer, {\"scheduler\": scheduler, \"interval\": \"step\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and checkpoint have mismatching keys.\n",
      "Keys in model but not in checkpoint: set()\n",
      "Keys in checkpoint but not in model: {'model.longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.6.attention.self.value.bias', 'model.longformer.encoder.layer.6.intermediate.dense.weight', 'model.longformer.encoder.layer.3.output.dense.weight', 'model.longformer.encoder.layer.6.attention.self.query_global.bias', 'model.longformer.encoder.layer.7.attention.self.value_global.weight', 'model.longformer.encoder.layer.11.attention.self.query.weight', 'model.longformer.encoder.layer.5.intermediate.dense.weight', 'model.longformer.encoder.layer.1.attention.self.value.bias', 'model.longformer.encoder.layer.1.attention.self.key_global.bias', 'model.longformer.encoder.layer.7.attention.self.value_global.bias', 'model.longformer.encoder.layer.5.intermediate.dense.bias', 'model.longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.9.attention.self.key_global.weight', 'model.longformer.encoder.layer.11.attention.output.dense.weight', 'model.longformer.encoder.layer.11.output.LayerNorm.weight', 'model.longformer.encoder.layer.11.output.LayerNorm.bias', 'model.longformer.encoder.layer.11.attention.self.query.bias', 'model.longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.8.attention.self.value.bias', 'model.longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.9.attention.self.value.bias', 'model.longformer.encoder.layer.9.intermediate.dense.bias', 'model.longformer.encoder.layer.1.output.dense.bias', 'model.longformer.encoder.layer.7.output.LayerNorm.weight', 'model.longformer.encoder.layer.10.attention.self.key.bias', 'model.longformer.encoder.layer.11.attention.self.key_global.weight', 'model.longformer.encoder.layer.8.intermediate.dense.weight', 'model.longformer.encoder.layer.10.output.dense.bias', 'model.longformer.encoder.layer.5.attention.self.key.bias', 'model.longformer.encoder.layer.3.attention.self.value.weight', 'model.longformer.encoder.layer.11.attention.self.query_global.bias', 'model.longformer.encoder.layer.7.attention.self.query.bias', 'model.longformer.encoder.layer.7.attention.self.value.weight', 'model.longformer.encoder.layer.9.attention.self.value.weight', 'model.longformer.encoder.layer.5.output.dense.bias', 'model.longformer.encoder.layer.10.attention.self.query_global.weight', 'model.longformer.encoder.layer.10.output.LayerNorm.bias', 'model.longformer.encoder.layer.8.intermediate.dense.bias', 'model.longformer.encoder.layer.1.attention.output.dense.weight', 'model.longformer.encoder.layer.1.attention.self.key.weight', 'model.longformer.encoder.layer.1.attention.self.value.weight', 'model.longformer.encoder.layer.6.output.dense.bias', 'model.longformer.encoder.layer.6.attention.self.value.weight', 'model.longformer.encoder.layer.8.output.dense.bias', 'model.longformer.encoder.layer.7.attention.self.query_global.bias', 'model.longformer.encoder.layer.10.attention.output.dense.bias', 'model.longformer.encoder.layer.6.attention.self.query.weight', 'model.longformer.encoder.layer.5.attention.self.value_global.weight', 'model.longformer.encoder.layer.7.attention.self.key.bias', 'model.longformer.encoder.layer.3.attention.output.dense.weight', 'model.longformer.encoder.layer.2.intermediate.dense.bias', 'model.longformer.encoder.layer.6.intermediate.dense.bias', 'model.longformer.encoder.layer.2.attention.output.dense.weight', 'model.longformer.encoder.layer.9.output.dense.bias', 'model.longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.2.attention.self.query_global.bias', 'model.longformer.encoder.layer.9.attention.self.key.bias', 'model.longformer.encoder.layer.1.output.dense.weight', 'model.longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.11.attention.self.value_global.weight', 'model.longformer.encoder.layer.4.attention.self.query.bias', 'model.longformer.encoder.layer.9.output.LayerNorm.weight', 'model.longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.1.attention.self.query.weight', 'model.longformer.encoder.layer.2.attention.self.value.weight', 'model.longformer.encoder.layer.7.attention.self.key_global.bias', 'model.longformer.encoder.layer.7.output.LayerNorm.bias', 'model.longformer.encoder.layer.8.attention.self.value.weight', 'model.longformer.encoder.layer.5.attention.self.query.weight', 'model.longformer.encoder.layer.6.attention.self.key_global.weight', 'model.longformer.encoder.layer.2.attention.output.dense.bias', 'model.longformer.encoder.layer.10.attention.self.value_global.bias', 'model.longformer.encoder.layer.3.attention.self.value_global.bias', 'model.longformer.encoder.layer.11.attention.self.value_global.bias', 'model.longformer.encoder.layer.4.attention.output.dense.weight', 'model.longformer.encoder.layer.1.output.LayerNorm.bias', 'model.longformer.encoder.layer.8.attention.self.query_global.bias', 'model.longformer.encoder.layer.1.attention.self.query_global.weight', 'model.longformer.encoder.layer.8.attention.self.query.bias', 'model.longformer.encoder.layer.8.attention.output.dense.bias', 'model.longformer.encoder.layer.10.attention.self.value.weight', 'model.longformer.encoder.layer.1.intermediate.dense.bias', 'model.longformer.encoder.layer.2.attention.self.key.bias', 'model.longformer.encoder.layer.5.output.LayerNorm.bias', 'model.longformer.encoder.layer.3.attention.self.key.weight', 'model.longformer.encoder.layer.10.attention.output.dense.weight', 'model.longformer.encoder.layer.3.attention.self.key_global.bias', 'model.longformer.encoder.layer.7.attention.self.key_global.weight', 'model.longformer.encoder.layer.11.intermediate.dense.weight', 'model.longformer.encoder.layer.2.output.LayerNorm.weight', 'model.longformer.encoder.layer.7.intermediate.dense.weight', 'model.longformer.encoder.layer.3.attention.self.value.bias', 'model.longformer.encoder.layer.4.attention.self.query_global.weight', 'model.longformer.encoder.layer.9.attention.self.value_global.bias', 'model.longformer.encoder.layer.2.attention.self.value_global.weight', 'model.longformer.encoder.layer.11.output.dense.bias', 'model.longformer.encoder.layer.3.attention.output.dense.bias', 'model.longformer.encoder.layer.11.attention.self.value.weight', 'model.longformer.encoder.layer.10.intermediate.dense.weight', 'model.longformer.encoder.layer.11.attention.self.value.bias', 'model.longformer.encoder.layer.8.attention.self.key.bias', 'model.longformer.encoder.layer.4.attention.self.value_global.weight', 'model.longformer.encoder.layer.10.attention.self.key.weight', 'model.longformer.encoder.layer.7.attention.self.key.weight', 'model.longformer.encoder.layer.6.attention.self.query_global.weight', 'model.longformer.encoder.layer.2.output.dense.weight', 'model.longformer.encoder.layer.5.attention.self.key.weight', 'model.longformer.encoder.layer.9.output.LayerNorm.bias', 'model.longformer.encoder.layer.10.attention.self.query.bias', 'model.longformer.encoder.layer.9.intermediate.dense.weight', 'model.longformer.encoder.layer.9.output.dense.weight', 'model.longformer.encoder.layer.4.output.dense.weight', 'model.longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.11.output.dense.weight', 'model.longformer.encoder.layer.3.output.LayerNorm.bias', 'model.longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.3.intermediate.dense.bias', 'model.longformer.encoder.layer.10.attention.self.query_global.bias', 'model.longformer.encoder.layer.7.attention.self.query.weight', 'model.longformer.encoder.layer.7.attention.output.dense.weight', 'model.longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.1.attention.self.key.bias', 'model.longformer.encoder.layer.11.attention.self.key.weight', 'model.longformer.encoder.layer.6.attention.self.value_global.bias', 'model.longformer.encoder.layer.11.attention.self.key.bias', 'model.longformer.encoder.layer.10.attention.self.key_global.weight', 'model.longformer.encoder.layer.7.attention.output.dense.bias', 'model.longformer.encoder.layer.6.attention.self.key.bias', 'model.longformer.encoder.layer.8.output.dense.weight', 'model.longformer.encoder.layer.9.attention.self.key.weight', 'model.longformer.encoder.layer.4.attention.self.key_global.weight', 'model.longformer.encoder.layer.2.intermediate.dense.weight', 'model.longformer.encoder.layer.3.attention.self.key.bias', 'model.longformer.encoder.layer.9.attention.self.value_global.weight', 'model.longformer.encoder.layer.11.attention.self.query_global.weight', 'model.longformer.encoder.layer.4.attention.self.value_global.bias', 'model.longformer.encoder.layer.5.attention.self.value.weight', 'model.longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.7.attention.self.query_global.weight', 'model.longformer.encoder.layer.8.attention.self.query.weight', 'model.longformer.encoder.layer.8.attention.self.value_global.bias', 'model.longformer.encoder.layer.4.attention.self.key.weight', 'model.longformer.encoder.layer.8.output.LayerNorm.weight', 'model.longformer.encoder.layer.3.attention.self.query_global.weight', 'model.longformer.encoder.layer.5.output.dense.weight', 'model.longformer.encoder.layer.7.attention.self.value.bias', 'model.longformer.encoder.layer.4.attention.self.value.weight', 'model.longformer.encoder.layer.4.attention.self.key.bias', 'model.longformer.encoder.layer.9.attention.output.dense.weight', 'model.longformer.encoder.layer.5.attention.self.key_global.bias', 'model.longformer.encoder.layer.10.attention.self.value_global.weight', 'model.longformer.encoder.layer.1.attention.self.query.bias', 'model.longformer.encoder.layer.2.attention.self.query.bias', 'model.longformer.encoder.layer.4.attention.self.query_global.bias', 'model.longformer.encoder.layer.4.intermediate.dense.weight', 'model.longformer.encoder.layer.3.intermediate.dense.weight', 'model.longformer.encoder.layer.8.attention.self.key_global.bias', 'model.longformer.encoder.layer.8.attention.output.dense.weight', 'model.longformer.encoder.layer.6.attention.self.key.weight', 'model.longformer.encoder.layer.8.attention.self.key.weight', 'model.longformer.encoder.layer.1.attention.self.value_global.weight', 'model.longformer.encoder.layer.3.attention.self.value_global.weight', 'model.longformer.encoder.layer.11.intermediate.dense.bias', 'model.longformer.encoder.layer.2.output.dense.bias', 'model.longformer.encoder.layer.2.attention.self.query_global.weight', 'model.longformer.encoder.layer.6.attention.self.query.bias', 'model.longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.1.attention.self.value_global.bias', 'model.longformer.encoder.layer.8.attention.self.key_global.weight', 'model.longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.9.attention.output.dense.bias', 'model.longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.10.output.dense.weight', 'model.longformer.encoder.layer.10.attention.self.key_global.bias', 'model.longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.1.attention.self.query_global.bias', 'model.longformer.encoder.layer.9.attention.self.key_global.bias', 'model.longformer.encoder.layer.4.output.LayerNorm.weight', 'model.longformer.encoder.layer.1.attention.self.key_global.weight', 'model.longformer.encoder.layer.5.attention.self.value_global.bias', 'model.longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.4.attention.output.dense.bias', 'model.longformer.encoder.layer.5.attention.self.query_global.weight', 'model.longformer.encoder.layer.6.output.dense.weight', 'model.longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.2.attention.self.key_global.weight', 'model.longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.1.intermediate.dense.weight', 'model.longformer.encoder.layer.4.intermediate.dense.bias', 'model.longformer.encoder.layer.4.output.LayerNorm.bias', 'model.longformer.encoder.layer.9.attention.self.query.weight', 'model.longformer.encoder.layer.8.output.LayerNorm.bias', 'model.longformer.encoder.layer.7.output.dense.bias', 'model.longformer.encoder.layer.2.attention.self.query.weight', 'model.longformer.encoder.layer.2.attention.self.key.weight', 'model.longformer.encoder.layer.4.attention.self.value.bias', 'model.longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.4.attention.self.query.weight', 'model.longformer.encoder.layer.7.intermediate.dense.bias', 'model.longformer.encoder.layer.9.attention.self.query_global.weight', 'model.longformer.encoder.layer.2.attention.self.value_global.bias', 'model.longformer.encoder.layer.5.attention.output.dense.weight', 'model.longformer.encoder.layer.8.attention.self.query_global.weight', 'model.longformer.encoder.layer.11.attention.output.dense.bias', 'model.longformer.encoder.layer.3.attention.self.query.weight', 'model.longformer.encoder.layer.9.attention.self.query.bias', 'model.longformer.encoder.layer.3.attention.self.key_global.weight', 'model.longformer.encoder.layer.2.attention.self.key_global.bias', 'model.longformer.encoder.layer.4.attention.self.key_global.bias', 'model.longformer.encoder.layer.9.attention.self.query_global.bias', 'model.longformer.encoder.layer.6.output.LayerNorm.weight', 'model.longformer.encoder.layer.6.output.LayerNorm.bias', 'model.longformer.encoder.layer.3.output.dense.bias', 'model.longformer.encoder.layer.11.attention.self.key_global.bias', 'model.longformer.encoder.layer.3.output.LayerNorm.weight', 'model.longformer.encoder.layer.2.attention.self.value.bias', 'model.longformer.encoder.layer.5.attention.self.value.bias', 'model.longformer.encoder.layer.5.attention.output.dense.bias', 'model.longformer.encoder.layer.1.output.LayerNorm.weight', 'model.longformer.encoder.layer.10.attention.self.query.weight', 'model.longformer.encoder.layer.10.output.LayerNorm.weight', 'model.longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'model.longformer.encoder.layer.6.attention.self.value_global.weight', 'model.longformer.encoder.layer.4.output.dense.bias', 'model.longformer.encoder.layer.3.attention.self.query_global.bias', 'model.longformer.encoder.layer.5.attention.self.query.bias', 'model.longformer.encoder.layer.5.attention.self.key_global.weight', 'model.longformer.encoder.layer.1.attention.output.dense.bias', 'model.longformer.encoder.layer.10.attention.self.value.bias', 'model.longformer.encoder.layer.5.attention.self.query_global.bias', 'model.longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'model.longformer.encoder.layer.6.attention.output.dense.weight', 'model.longformer.encoder.layer.7.output.dense.weight', 'model.longformer.encoder.layer.6.attention.output.dense.bias', 'model.longformer.encoder.layer.5.output.LayerNorm.weight', 'model.longformer.encoder.layer.6.attention.self.key_global.bias', 'model.longformer.encoder.layer.10.intermediate.dense.bias', 'model.longformer.encoder.layer.2.output.LayerNorm.bias', 'model.longformer.encoder.layer.8.attention.self.value_global.weight', 'model.longformer.encoder.layer.3.attention.self.query.bias'}\n"
     ]
    }
   ],
   "source": [
    "model = LongformerPretrainNormal(\n",
    "        vocab_size=args.vocab_size,\n",
    "        itemid_size=args.itemid_size,\n",
    "        max_position_embeddings=args.max_position_embeddings,\n",
    "        unit_size=args.unit_size,\n",
    "        continuous_size=args.continuous_size,\n",
    "        task_size=args.task_size,\n",
    "        max_age=args.max_age,\n",
    "        gender_size=args.gender_size,\n",
    "        embedding_size=args.embedding_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        dropout_prob=args.dropout_prob,\n",
    "        gpu_mixed_precision=args.gpu_mixed_precision,\n",
    "    ).to(args.device)\n",
    "\n",
    "optimizer, scheduler = configure_optimizers(model, args,)\n",
    "\n",
    "model_path = \"./results/best_pretrain_model.pth\"\n",
    "checkpoint_path = f\"./results/best_pretrain_model_after_masking_{args.resume_epoch}epoch.pth\"\n",
    "logging.info(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=args.device, weights_only=True)\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint['model_state_dict'].items():\n",
    "    if k.startswith('module.module.'):\n",
    "        new_state_dict[k[14:]] = v  \n",
    "    elif k.startswith('module.'):\n",
    "        new_state_dict[k[7:]] = v \n",
    "    else:\n",
    "        new_state_dict[k] = v  \n",
    "filtered_state_dict = {k: v for k, v in new_state_dict.items()}\n",
    "\n",
    "# model.load_state_dict(filtered_state_dict)\n",
    "\n",
    "\n",
    "model_keys = set(model.state_dict().keys())\n",
    "\n",
    "# 체크포인트의 state_dict 키\n",
    "checkpoint_keys = set(filtered_state_dict.keys())\n",
    "\n",
    "# 모델과 체크포인트의 키가 일치하는지 확인\n",
    "if model_keys == checkpoint_keys:\n",
    "    print(\"Model state_dict and checkpoint state_dict keys match.\")\n",
    "else:\n",
    "    print(\"Model and checkpoint have mismatching keys.\")\n",
    "    print(\"Keys in model but not in checkpoint:\", model_keys - checkpoint_keys)\n",
    "    print(\"Keys in checkpoint but not in model:\", checkpoint_keys - model_keys)\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# print(\"Resume model loaded successfully.\")\n",
    "\n",
    "# args.start_epoch = args.resume_epoch + 1\n",
    "# logging.info(f\"Resuming from epoch {args.start_epoch}\")\n",
    "\n",
    "# lr = scheduler[\"scheduler\"].get_lr()\n",
    "# for param_group, lr_val in zip(optimizer.param_groups, lr):\n",
    "#     param_group['lr'] = lr_val\n",
    "    \n",
    "# logging.info(f\"Resumed learning rates: {lr}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LongformerPretrainNormal(\n\u001b[1;32m      2\u001b[0m         vocab_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      3\u001b[0m         itemid_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mitemid_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         gpu_mixed_precision\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgpu_mixed_precision,\n\u001b[1;32m     17\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 19\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mcheckpoint_path\u001b[49m, map_location\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(checkpoint['model_state_dict'].keys())\u001b[39;00m\n\u001b[1;32m     22\u001b[0m new_state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_path' is not defined"
     ]
    }
   ],
   "source": [
    "model = LongformerPretrainNormal(\n",
    "        vocab_size=args.vocab_size,\n",
    "        itemid_size=args.itemid_size,\n",
    "        max_position_embeddings=args.max_position_embeddings,\n",
    "        unit_size=args.unit_size,\n",
    "        continuous_size=args.continuous_size,\n",
    "        task_size=args.task_size,\n",
    "        max_age=args.max_age,\n",
    "        gender_size=args.gender_size,\n",
    "        embedding_size=args.embedding_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        dropout_prob=args.dropout_prob,\n",
    "        gpu_mixed_precision=args.gpu_mixed_precision,\n",
    "    ).to(args.device)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=args.device, weights_only=True)\n",
    "\n",
    "# print(checkpoint['model_state_dict'].keys())\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint['model_state_dict'].items():\n",
    "    if k.startswith('module.module.'):\n",
    "        new_state_dict[k[14:]] = v\n",
    "\n",
    "print(set(model.state_dict().keys()) - set(new_state_dict.keys()))\n",
    "if set(model.state_dict().keys()) == set(new_state_dict.keys()):\n",
    "    print(\"Model state_dict and checkpoint state_dict keys match.\")\n",
    "else:\n",
    "    print(\"Model and checkpoint have mismatching keys.\")\n",
    "    print(\"Keys in model but not in checkpoint:\", set(model.state_dict().keys()) - set(new_state_dict.keys()))\n",
    "    print(\"Keys in checkpoint but not in model:\", set(new_state_dict.keys()) - set(model.state_dict().keys()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 적용 전 학습 가능한 파라미터 수: 31680176\n",
      "LoRA 적용 후 학습 가능한 파라미터 수: 49152\n",
      "학습 가능한 파라미터 수 감소 비율: 99.84%\n",
      "메모리 사용량: 196608\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,  # Assuming this is for sequence classification\n",
    "        inference_mode=False,  # Set to True if using for inference only\n",
    "        r=8,  # Rank of the low-rank matrices\n",
    "        lora_alpha=32,  # Scaling factor for the low-rank matrices\n",
    "        lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "        target_modules=[\"query\", \"value\"], # Target attention layers\n",
    "    )\n",
    "before_memory = torch.cuda.memory_allocated()\n",
    "trainable_params_before_lora = count_trainable_parameters(pretrained_model)\n",
    "print(f\"LoRA 적용 전 학습 가능한 파라미터 수: {trainable_params_before_lora}\")\n",
    "model_with_lora = get_peft_model(pretrained_model, peft_config)\n",
    "after_memory = torch.cuda.memory_allocated()\n",
    "trainable_params_after_lora = count_trainable_parameters(model_with_lora)\n",
    "print(f\"LoRA 적용 후 학습 가능한 파라미터 수: {trainable_params_after_lora}\")\n",
    "# 파라미터 수 감소 비율 계산\n",
    "reduction_ratio = (trainable_params_before_lora - trainable_params_after_lora) / trainable_params_before_lora * 100\n",
    "print(f\"학습 가능한 파라미터 수 감소 비율: {reduction_ratio:.2f}%\")\n",
    "\n",
    "print(f\"메모리 사용량: {after_memory - before_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.concept_embedding.procedure_embedding.weight\n",
      "embeddings.concept_embedding.medication_embedding.weight\n",
      "embeddings.concept_embedding.chart_embedding.weight\n",
      "embeddings.position_embedding.position_embeddings.weight\n",
      "embeddings.time_embedding.w\n",
      "embeddings.time_embedding.b\n",
      "embeddings.time_embedding.freqs\n",
      "embeddings.time_embedding.projection.weight\n",
      "embeddings.time_embedding.projection.bias\n",
      "embeddings.value_embedding.value_embedding.0.weight\n",
      "embeddings.value_embedding.value_embedding.0.bias\n",
      "embeddings.value_embedding.value_embedding.2.weight\n",
      "embeddings.value_embedding.value_embedding.2.bias\n",
      "embeddings.unit_embedding.unit_embedding.weight\n",
      "embeddings.continuous_embedding.embedding.weight\n",
      "embeddings.age_embedding.age_embedding.weight\n",
      "embeddings.gender_embedding.gender_embedding.weight\n",
      "embeddings.task_embedding.task_embedding.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "model.embeddings.word_embeddings.weight\n",
      "model.embeddings.token_type_embeddings.weight\n",
      "model.embeddings.LayerNorm.weight\n",
      "model.embeddings.LayerNorm.bias\n",
      "model.embeddings.position_embeddings.weight\n",
      "model.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "model.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "model.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "model.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "model.encoder.layer.0.attention.self.key.weight\n",
      "model.encoder.layer.0.attention.self.key.bias\n",
      "model.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "model.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "model.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "model.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "model.encoder.layer.0.attention.self.query_global.weight\n",
      "model.encoder.layer.0.attention.self.query_global.bias\n",
      "model.encoder.layer.0.attention.self.key_global.weight\n",
      "model.encoder.layer.0.attention.self.key_global.bias\n",
      "model.encoder.layer.0.attention.self.value_global.weight\n",
      "model.encoder.layer.0.attention.self.value_global.bias\n",
      "model.encoder.layer.0.attention.output.dense.weight\n",
      "model.encoder.layer.0.attention.output.dense.bias\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.0.intermediate.dense.weight\n",
      "model.encoder.layer.0.intermediate.dense.bias\n",
      "model.encoder.layer.0.output.dense.weight\n",
      "model.encoder.layer.0.output.dense.bias\n",
      "model.encoder.layer.0.output.LayerNorm.weight\n",
      "model.encoder.layer.0.output.LayerNorm.bias\n",
      "model.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "model.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "model.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "model.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "model.encoder.layer.1.attention.self.key.weight\n",
      "model.encoder.layer.1.attention.self.key.bias\n",
      "model.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "model.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "model.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "model.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "model.encoder.layer.1.attention.self.query_global.weight\n",
      "model.encoder.layer.1.attention.self.query_global.bias\n",
      "model.encoder.layer.1.attention.self.key_global.weight\n",
      "model.encoder.layer.1.attention.self.key_global.bias\n",
      "model.encoder.layer.1.attention.self.value_global.weight\n",
      "model.encoder.layer.1.attention.self.value_global.bias\n",
      "model.encoder.layer.1.attention.output.dense.weight\n",
      "model.encoder.layer.1.attention.output.dense.bias\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.1.intermediate.dense.weight\n",
      "model.encoder.layer.1.intermediate.dense.bias\n",
      "model.encoder.layer.1.output.dense.weight\n",
      "model.encoder.layer.1.output.dense.bias\n",
      "model.encoder.layer.1.output.LayerNorm.weight\n",
      "model.encoder.layer.1.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, p in finetune_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "finetune_model = LongformerFinetune(\n",
    "    pretrained_model=model_with_lora,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    num_labels=2,\n",
    "    learning_rate=5e-5,\n",
    "    classifier_dropout=0.1,\n",
    "    use_lora=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003, -0.8854, -2.0977,  ...,  0.8637, -1.0634,  0.6415],\n",
      "        [ 0.1859, -0.1252,  2.1717,  ...,  1.8789, -1.4072,  1.0232],\n",
      "        ...,\n",
      "        [ 1.2455, -0.2159, -0.3655,  ...,  2.3189,  0.3499, -0.0739],\n",
      "        [-1.6800,  0.5436, -0.6656,  ..., -1.4946, -0.7375,  0.3359],\n",
      "        [ 0.0520,  0.7949, -0.9065,  ..., -0.3273, -1.7657,  1.3392]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3414, -0.0902, -1.5275,  ..., -0.1391,  0.7844, -1.3369],\n",
      "        [ 0.0913, -2.1859, -0.9842,  ...,  0.6543,  0.1514,  0.7890],\n",
      "        ...,\n",
      "        [ 0.4680, -1.4265,  0.1359,  ..., -1.3171,  0.4589, -0.2161],\n",
      "        [ 0.5415,  1.4428,  0.6588,  ...,  0.5114,  0.2453,  1.0027],\n",
      "        [ 0.0988,  1.5918, -1.1827,  ..., -0.6837, -1.6430, -1.1480]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4577, -1.9488,  0.7330,  ...,  0.9435, -0.3041, -1.0402],\n",
      "        [-1.4386,  1.1002, -0.7581,  ..., -1.0915,  1.5659, -1.3064],\n",
      "        ...,\n",
      "        [ 0.7296, -0.1827, -0.8971,  ..., -1.4972,  0.8602,  0.9778],\n",
      "        [ 0.1030,  0.0437, -0.2763,  ..., -0.9553,  0.7520, -1.3320],\n",
      "        [-0.0067,  0.0365,  0.9786,  ..., -0.0265, -0.7701,  0.1738]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.6009e-01,  8.1525e-01,  ...,  1.0000e+00,\n",
      "          1.0491e-08,  1.0000e+00],\n",
      "        [ 9.0930e-01, -3.7260e-01,  9.4424e-01,  ...,  1.0000e+00,\n",
      "          2.0983e-08,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 9.5625e-01, -9.4867e-01,  2.9119e-01,  ...,  1.0000e+00,\n",
      "          5.2426e-05,  1.0000e+00],\n",
      "        [ 2.7050e-01, -7.9335e-01,  9.4855e-01,  ...,  1.0000e+00,\n",
      "          5.2436e-05,  1.0000e+00],\n",
      "        [-6.6395e-01,  5.9971e-02,  8.0744e-01,  ...,  1.0000e+00,\n",
      "          5.2446e-05,  1.0000e+00]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-1.1017e+00,  2.8092e-01, -5.7544e-01, -4.5157e-01, -4.7050e-01,\n",
      "         -8.0783e-01,  1.8774e-01,  1.6805e+00,  3.9865e-02,  2.4598e-01,\n",
      "          1.2668e+00, -3.0689e-01,  2.9672e-01, -6.0810e-02, -1.7685e+00,\n",
      "         -1.9505e-01,  4.4522e-02, -7.7847e-01,  1.8373e-01, -4.2723e-01,\n",
      "          6.8433e-01,  1.8931e+00,  1.2958e+00, -1.4696e-01,  1.3200e+00,\n",
      "          9.3177e-01,  3.3332e-01,  2.1568e+00, -7.1687e-01, -1.1772e+00,\n",
      "         -1.9325e+00, -7.6486e-01, -2.6507e-01, -7.7857e-01, -6.9343e-01,\n",
      "          2.2706e-01,  5.1547e-01,  3.9443e-01, -3.7325e-01, -4.0761e-01,\n",
      "         -9.1326e-01, -3.5168e-02,  1.2849e+00,  6.5735e-01, -1.6801e-01,\n",
      "          1.6162e+00,  8.9050e-01,  3.6242e-02,  9.6751e-01, -9.8209e-01,\n",
      "          5.0387e-01,  3.8770e-01, -1.5614e+00, -1.2928e+00, -2.7578e-01,\n",
      "         -6.3201e-01,  5.4922e-01,  2.3006e+00, -5.5675e-01,  1.4047e-01,\n",
      "          7.5635e-01, -4.3703e-01,  2.0555e+00, -1.6409e-01, -1.2175e+00,\n",
      "          6.2897e-01, -2.8455e-01, -2.1147e+00,  8.2583e-02,  1.1642e+00,\n",
      "         -3.9219e-01, -6.1668e-01, -1.5773e+00, -2.3558e-01, -1.0693e-01,\n",
      "         -1.3725e-01, -9.8625e-01, -1.1483e+00, -2.4339e+00,  2.3262e+00,\n",
      "         -6.3012e-01,  3.3978e-01,  1.0251e+00,  1.0785e+00,  1.0984e+00,\n",
      "         -1.6422e+00, -3.1177e-01, -1.1199e+00, -4.0803e-01,  6.1449e-02,\n",
      "          6.6913e-01,  6.4227e-01, -8.6553e-01,  8.4356e-01, -5.9576e-01,\n",
      "         -1.6027e+00,  2.1559e+00,  1.3992e+00,  1.0424e+00,  8.1753e-01,\n",
      "         -1.3924e+00,  4.2415e-02,  5.9470e-01,  1.1188e+00,  7.6874e-01,\n",
      "          2.5236e-02, -7.4991e-01, -3.3371e-01, -2.6022e-01, -1.9046e-01,\n",
      "          1.6419e-01, -6.6656e-01,  6.4348e-01, -6.8847e-01, -1.8166e+00,\n",
      "         -8.2808e-01,  1.4999e+00, -3.0021e-01,  2.6201e-01,  5.7743e-01,\n",
      "          5.0189e-01,  2.9656e-01, -1.3998e-01,  1.1742e+00, -2.1197e-01,\n",
      "         -1.5735e+00, -8.0576e-01, -4.7355e-01,  7.9945e-01,  7.5415e-01,\n",
      "         -2.4142e+00, -2.7362e-01, -4.4965e-01,  8.3738e-01,  8.0803e-01,\n",
      "          5.6955e-01, -1.0598e+00, -9.0323e-01, -1.7879e-01, -2.9770e-01,\n",
      "         -1.3184e-01,  7.2976e-01, -1.5855e-01, -1.1510e+00,  1.2391e+00,\n",
      "          1.5541e+00,  8.8062e-01,  7.6686e-01, -5.5865e-03, -1.4754e-01,\n",
      "         -3.4117e-01,  1.7616e+00,  7.7173e-01, -6.9774e-01,  4.7177e-01,\n",
      "          1.8783e-04, -7.4474e-01, -7.4792e-01, -1.1922e+00,  2.9991e+00,\n",
      "          2.1098e+00,  1.1097e+00, -1.2545e+00, -4.7533e-01, -1.7896e+00,\n",
      "         -4.7337e-01,  1.3221e+00, -7.0161e-01,  8.1644e-01, -4.4125e-01,\n",
      "         -2.6048e-01, -1.2940e+00,  1.6462e+00,  1.5369e+00,  4.3164e-01,\n",
      "          6.2311e-01,  1.1588e+00,  4.3139e-01,  7.5497e-01,  1.6531e+00,\n",
      "         -1.1056e+00,  1.3262e+00,  1.8819e+00, -1.6696e+00,  9.2327e-01,\n",
      "         -5.8630e-01,  9.3174e-01, -2.8759e-01, -8.8680e-02,  8.5561e-01,\n",
      "         -1.4229e+00,  1.3117e+00,  1.0028e+00, -1.6551e+00,  1.3388e+00,\n",
      "         -3.9453e-01,  2.3358e+00, -2.1843e-01, -2.3757e-01, -1.7918e+00,\n",
      "         -3.7609e-01,  2.2267e+00, -3.0143e-01,  9.0373e-01, -6.8855e-01,\n",
      "         -5.8911e-01, -1.2256e+00,  4.1564e-01,  1.9874e-01,  4.6316e-01,\n",
      "         -1.6634e+00, -2.3040e-02,  2.1195e+00, -2.8008e-01,  1.2349e+00,\n",
      "         -9.1521e-01,  5.7178e-01, -7.9190e-01,  5.6033e-01, -2.7718e-01,\n",
      "         -5.6751e-01, -1.3767e-02,  2.9173e-01,  4.9143e-01, -8.6559e-01,\n",
      "          4.1157e-01, -6.9679e-01,  4.7739e-01, -1.0425e+00, -1.7913e+00,\n",
      "         -7.2769e-01, -4.0399e-01, -6.9903e-01,  9.1828e-01, -2.6960e-01,\n",
      "         -1.4346e+00,  5.5915e-01, -7.7760e-01, -7.0029e-02, -3.7309e-01,\n",
      "         -1.0299e+00, -1.2881e+00,  6.8472e-01,  1.5059e+00,  1.8781e-01,\n",
      "         -1.3002e-01,  2.1011e-01,  9.2440e-01,  5.0797e-01, -6.0533e-01,\n",
      "          6.2779e-01,  7.2916e-01,  1.2578e-01, -5.4384e-01, -7.9188e-01,\n",
      "         -2.6114e-01, -8.1619e-01, -6.4851e-02,  2.1852e+00,  1.8706e-01,\n",
      "          6.7416e-01, -1.1518e-01, -1.1267e+00, -1.6734e+00,  1.2010e+00,\n",
      "          4.2912e-01,  2.1213e+00, -6.9895e-01, -1.2445e+00,  1.1165e+00,\n",
      "         -2.9241e-01, -8.0608e-01,  4.2713e-01,  6.6258e-02,  6.3823e-01,\n",
      "          7.8867e-01,  2.7125e-01, -7.3013e-01, -1.3009e+00,  1.4833e+00,\n",
      "         -6.2761e-01, -6.6229e-02, -6.0385e-01,  6.4602e-01,  6.1152e-01,\n",
      "          2.4229e-01,  1.0934e+00, -3.9857e-01, -3.6342e-01, -7.9264e-01,\n",
      "          2.0798e-01, -1.1510e+00, -4.3223e-01,  2.2619e+00,  8.7961e-01,\n",
      "          1.4214e-01, -7.4028e-01, -7.5569e-01, -3.7549e-01, -3.7794e-01,\n",
      "          2.7407e-02, -2.9683e-01,  2.6364e-01, -1.4762e+00,  3.1741e-01,\n",
      "          7.6410e-02,  7.0462e-01,  6.8519e-01,  1.6031e+00, -8.1123e-01,\n",
      "          6.8642e-01,  2.3693e-01, -5.6208e-01, -3.6517e-01, -9.7314e-01,\n",
      "         -5.5781e-01,  1.2865e+00,  8.6510e-01, -9.4199e-01,  2.0579e+00,\n",
      "          6.1480e-01, -9.4047e-01, -1.7059e-01,  2.4544e-01, -2.2648e-01,\n",
      "         -7.0253e-02,  1.0437e+00, -2.8622e-01,  1.5356e+00, -1.4315e+00,\n",
      "          2.8749e-01,  3.8716e-02, -1.6373e-01,  2.9594e-01,  5.2879e-01,\n",
      "         -3.8585e-01, -2.5261e-01, -2.1800e+00,  1.8307e-01,  7.1622e-01,\n",
      "          1.4475e-01, -8.5452e-01,  7.6892e-01,  5.2730e-01, -1.7634e-01,\n",
      "          5.5318e-01,  6.9407e-01, -9.1132e-01, -1.1685e+00,  1.0968e+00,\n",
      "          1.0020e-01,  1.2513e+00,  1.2118e+00,  1.8171e-01,  1.7112e+00,\n",
      "          5.1184e-01, -3.1435e-01, -5.0720e-01,  1.0588e+00,  3.7146e-01,\n",
      "          2.0554e+00, -5.2561e-01,  1.2774e+00, -5.7148e-01, -1.0275e+00,\n",
      "         -6.3158e-01,  1.0429e+00,  2.1167e-01,  1.1623e+00,  1.7066e-01,\n",
      "          8.6301e-01, -8.3188e-01,  4.4052e-01, -5.1852e-01, -7.4396e-01,\n",
      "          4.7125e-01,  5.5531e-01, -6.0173e-01, -2.2043e-01, -2.0730e+00,\n",
      "         -5.2143e-01, -1.8649e-01,  1.1637e-01, -5.0375e-01,  8.6954e-01,\n",
      "         -2.5793e-01, -2.5188e-01, -5.0003e-01, -3.9354e-01, -1.2606e+00,\n",
      "         -2.6937e-01, -2.5695e+00, -2.0760e+00, -1.1794e+00,  8.6878e-01,\n",
      "         -1.6844e+00,  7.9970e-01, -7.3301e-01,  5.5079e-01, -1.6659e+00,\n",
      "          3.8875e-01, -7.0238e-01,  7.3582e-02,  8.4943e-01, -6.0564e-01,\n",
      "         -1.8256e+00, -1.2122e+00,  2.8668e-02, -9.7261e-01, -1.0323e+00,\n",
      "          8.4378e-01,  1.3540e+00,  2.5321e-01, -4.5511e-02, -9.1284e-01,\n",
      "          3.5237e-01, -3.0490e+00,  5.5908e-01,  6.3055e-01,  7.5979e-01,\n",
      "         -1.0678e+00, -1.2938e+00,  5.5381e-01,  2.5433e-01,  5.2166e-01,\n",
      "         -7.2392e-01,  1.0452e+00, -8.3914e-01, -7.7934e-01,  1.0056e+00,\n",
      "         -1.4795e+00, -8.4761e-01, -7.3652e-02, -9.2485e-01,  9.1397e-02,\n",
      "          5.7705e-01, -1.1246e+00, -4.9372e-01,  6.2159e-01,  3.2048e-01,\n",
      "         -6.4329e-01,  1.3538e+00,  8.2542e-01,  8.2952e-01,  9.4919e-01,\n",
      "         -2.7107e-01,  1.2941e+00,  1.5542e+00, -1.3386e+00,  7.6982e-01,\n",
      "          1.8494e-01, -4.0957e-02, -8.9616e-01,  2.6252e-01,  3.8936e-01,\n",
      "          8.6802e-01,  7.1594e-01,  2.4082e-01,  4.0441e-01, -1.3613e+00,\n",
      "          1.3528e+00,  1.1237e+00, -7.7422e-01,  1.0375e+00, -1.1390e+00,\n",
      "         -6.0957e-01, -3.1289e-01,  2.2288e-01, -4.8560e-02, -2.6133e-02,\n",
      "          3.6700e-02, -4.0848e-01,  1.1228e+00, -1.1022e+00,  2.8578e-01,\n",
      "         -7.1096e-01, -1.5200e-01,  1.0417e-01,  1.2416e+00, -1.0287e+00,\n",
      "          2.8487e-01,  7.4487e-01, -9.4220e-01, -2.6885e-01, -5.2068e-01,\n",
      "         -1.1659e+00, -8.8580e-01,  3.4480e-02,  1.0600e+00, -5.1562e-01,\n",
      "          6.9059e-01,  1.1453e+00, -4.8965e-01,  1.2993e+00,  4.2672e-01,\n",
      "         -1.3145e+00, -1.3814e+00,  1.0463e-01,  2.2882e+00, -1.8624e-02,\n",
      "          9.0786e-01,  6.9977e-01,  1.0626e+00, -2.9519e+00,  9.7521e-01,\n",
      "          1.1066e+00,  1.6091e-01, -7.5761e-01, -1.1583e-01,  4.7557e-01,\n",
      "          1.4144e+00, -2.7720e-01,  6.5007e-01,  4.0683e-01, -1.7069e+00,\n",
      "          1.5019e+00, -1.7355e-01,  4.8911e-01,  7.3630e-01,  1.8305e-01,\n",
      "          9.7845e-01,  2.8889e+00, -9.5798e-01,  1.7878e+00,  4.8501e-01,\n",
      "         -1.9880e+00,  7.8004e-01, -9.9871e-01,  5.2943e-02, -9.3514e-01,\n",
      "         -3.0923e-01,  5.3849e-01, -3.2743e-01, -8.3867e-01, -1.0933e+00,\n",
      "          3.1167e-02,  4.1939e-01,  2.2149e-01,  3.9372e-01, -1.9058e+00,\n",
      "         -9.7314e-01, -1.6330e+00, -1.0672e+00, -1.1483e+00, -6.1888e-01,\n",
      "         -3.7749e-01, -4.9516e-01,  8.1274e-01,  1.0450e+00, -2.4804e-01,\n",
      "          7.1142e-01,  7.6154e-01, -1.3184e+00,  1.2565e+00, -1.1805e+00,\n",
      "          5.7937e-01, -1.9932e-02, -5.0971e-01, -2.6081e+00,  8.2089e-01,\n",
      "          1.6487e-01, -1.0275e+00,  2.3874e+00,  1.1482e+00, -1.1953e+00,\n",
      "          1.9157e+00, -8.2969e-01, -1.3694e+00,  1.0513e+00,  1.4558e+00,\n",
      "          3.3837e-01, -4.4018e-01,  9.1967e-01, -1.8599e+00,  1.1783e-01,\n",
      "          1.6908e+00,  1.4779e+00,  5.5916e-01, -5.7468e-01,  4.0181e-01,\n",
      "         -8.6833e-01,  2.7163e-01,  3.5625e-01,  4.9322e-01, -3.1395e-01,\n",
      "         -1.3669e+00,  1.3571e-01, -7.8946e-01, -1.4575e-01, -1.6063e+00,\n",
      "          1.8869e-01,  1.1007e+00, -1.0174e+00,  6.7318e-02,  8.0347e-01,\n",
      "         -8.8695e-01,  1.1938e+00,  1.5194e-01,  1.5010e-01, -1.3316e+00,\n",
      "         -9.5710e-01, -1.2816e+00,  7.5231e-01, -1.2390e+00, -8.8793e-01,\n",
      "          1.2295e+00,  4.1196e-01, -1.3147e+00,  7.8617e-01,  7.6342e-01,\n",
      "          5.0315e-01,  1.6559e+00, -1.6999e+00,  4.8862e-01, -1.9475e-01,\n",
      "         -1.1263e+00, -5.2418e-01,  7.5752e-02, -1.8652e+00, -6.2643e-01,\n",
      "          2.3238e-01, -6.5911e-01, -2.2172e-01,  4.4219e-01,  1.9641e-01,\n",
      "          1.0389e+00, -2.3707e+00, -1.1563e+00,  3.5104e-01, -8.8802e-01,\n",
      "         -6.1713e-01,  2.4062e-01,  4.7942e-02,  6.2058e-01, -3.4116e-01,\n",
      "          1.4692e+00, -7.0810e-01,  8.7133e-01,  5.3982e-01, -1.5995e-01,\n",
      "         -2.0143e-01,  9.7201e-01, -3.4818e-01, -8.0676e-02,  1.0522e+00,\n",
      "         -3.0436e-02,  1.8412e+00, -6.8346e-02, -1.4562e+00,  3.8111e-01,\n",
      "         -6.7209e-01,  1.2451e-01,  1.6365e+00, -1.2720e-01, -1.0757e-01,\n",
      "          2.0806e-01, -6.9745e-01, -5.4672e-01,  5.9927e-01,  7.0853e-01,\n",
      "         -5.0146e-01, -5.2836e-01, -1.9307e-01,  3.2156e-01, -3.0327e+00,\n",
      "         -2.4379e-02, -7.5883e-01,  3.1512e-01, -1.0765e+00, -4.8727e-01,\n",
      "          1.0954e+00, -1.2035e+00,  1.4680e-01, -1.9329e+00,  3.8191e-01,\n",
      "         -8.3196e-01, -1.5584e+00, -2.2654e-01,  1.2264e+00,  7.4026e-01,\n",
      "         -1.7122e+00,  6.2393e-02,  1.3521e+00, -2.2741e+00, -6.2192e-01,\n",
      "         -1.2522e+00,  3.7003e-01,  4.7727e-01,  5.4230e-03,  5.4474e-01,\n",
      "         -2.6456e-01,  6.5033e-01,  1.0163e+00, -1.0945e+00, -9.2496e-01,\n",
      "         -4.6012e-01, -2.1345e+00, -1.5876e+00, -3.6525e-01, -6.2241e-02,\n",
      "         -1.3196e-02,  1.2203e+00,  3.2695e+00,  2.7894e-01, -1.3118e+00,\n",
      "          6.5042e-01,  1.4822e+00, -3.6790e-01,  1.1217e+00,  2.3830e-01,\n",
      "         -8.8077e-01,  7.3810e-02,  9.1528e-02,  7.2343e-01, -3.7635e-01,\n",
      "         -7.6965e-01, -8.7949e-02, -6.2255e-01,  2.0665e-01, -4.4832e-01,\n",
      "          8.2602e-01,  1.1960e+00, -2.1093e+00, -2.3046e-01, -2.6715e+00,\n",
      "         -2.4867e-01, -2.2828e+00,  8.1881e-01, -8.0621e-01,  6.1841e-01,\n",
      "          4.5330e-01, -8.9835e-01, -5.7326e-01, -1.2246e+00, -6.5286e-01,\n",
      "         -2.9203e-01, -3.4494e-01, -1.1148e+00,  2.5511e-01,  3.0939e-01,\n",
      "         -2.8970e-01,  7.7387e-01, -8.3463e-01,  2.1829e-01, -1.9567e+00,\n",
      "          3.4880e-02, -2.9554e-01,  7.1535e-01, -8.1859e-01, -1.5571e-01,\n",
      "         -9.4058e-01,  1.7883e+00, -8.5414e-01, -1.4376e+00, -7.1885e-01,\n",
      "         -2.8805e-01, -9.7360e-01,  4.8657e-01,  1.6111e+00, -5.8095e-01,\n",
      "         -7.4971e-01,  7.1472e-01,  1.1506e+00, -1.0249e+00, -2.1299e-01,\n",
      "         -6.5368e-01,  8.3486e-01,  1.7877e+00]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-2.5123e-01, -1.3800e+00, -4.7343e-01,  1.4895e+00,  1.2388e+00,\n",
      "         1.7518e-01,  4.3627e-01, -7.5286e-01,  7.7418e-02, -9.5624e-01,\n",
      "        -4.7313e-02, -2.6422e-01,  3.0973e-01, -1.0369e+00,  8.6545e-01,\n",
      "         9.2528e-01, -8.5859e-01, -4.3299e-01,  1.2726e-01, -5.2897e-01,\n",
      "        -2.2277e-01,  8.3013e-01, -9.6092e-01, -8.3783e-01,  1.2289e+00,\n",
      "         8.1784e-01, -4.1163e-02,  1.5107e+00,  1.0423e-01,  5.1291e-01,\n",
      "         9.0537e-01,  1.0138e+00,  1.7055e-01, -7.4367e-01,  2.6764e-01,\n",
      "        -1.3949e-01, -2.1838e-01, -7.2254e-01, -2.4854e-01,  1.0335e+00,\n",
      "         4.0653e-01, -1.0451e+00,  1.1313e+00,  5.0629e-01,  6.8816e-02,\n",
      "         2.4503e-01,  7.1488e-02,  3.0323e-01, -1.6889e+00, -1.3515e+00,\n",
      "        -5.6291e-01, -1.5243e+00, -2.6190e-02,  5.6904e-01,  2.7338e-01,\n",
      "        -1.8250e+00, -1.0221e-01,  1.5102e+00,  1.0537e+00, -1.7259e-01,\n",
      "        -9.0708e-01, -1.6736e+00,  1.0138e+00,  1.3520e+00,  3.7943e-01,\n",
      "        -1.0227e+00, -1.1232e+00, -1.2228e+00, -5.9753e-01, -2.2852e-01,\n",
      "         4.5718e-01,  3.8447e-04,  8.0357e-01, -1.5868e+00,  4.5482e-01,\n",
      "        -8.2068e-01, -1.1440e+00, -1.0657e+00, -1.1650e+00,  1.2067e+00,\n",
      "        -4.5769e-01,  7.4766e-01,  4.6946e-01,  8.9847e-01, -1.4892e-01,\n",
      "        -6.5114e-01, -9.7562e-01, -6.6161e-01, -1.1521e+00,  3.4993e-01,\n",
      "        -7.8629e-01,  2.8891e+00, -2.5406e+00,  8.3740e-01, -4.2075e-01,\n",
      "        -6.5094e-03,  4.3010e-01,  1.1962e+00, -1.1895e-02,  1.5303e+00,\n",
      "        -3.3803e+00, -8.2291e-01,  3.8793e-01, -1.3882e+00, -1.9655e+00,\n",
      "         8.3467e-01,  3.7989e-01, -7.5390e-02,  1.2036e+00, -6.1229e-01,\n",
      "        -1.4181e+00,  9.9172e-01, -1.5220e+00, -6.1671e-01,  1.3531e+00,\n",
      "        -3.3211e-01, -3.2591e-01, -8.6304e-01,  5.4710e-01, -4.4071e-01,\n",
      "         8.1700e-02,  9.2950e-04,  1.0740e+00, -2.0782e-01,  1.1336e+00,\n",
      "         2.6968e-01,  8.0040e-01,  4.9682e-01, -3.6789e-01, -8.5980e-01,\n",
      "        -6.1998e-02,  1.0009e+00,  6.1547e-01,  8.3111e-01,  1.3180e+00,\n",
      "         1.3477e+00, -1.1874e+00,  1.2225e+00,  8.6577e-01,  4.4039e-01,\n",
      "        -9.5960e-01,  6.7921e-01,  2.4091e-01,  8.2279e-01,  7.9251e-03,\n",
      "         3.7143e-01, -2.0267e+00,  6.6717e-01,  6.5647e-01,  7.7277e-01,\n",
      "         6.1262e-01,  1.0142e+00,  2.3187e-01, -4.9769e-01, -1.2551e-01,\n",
      "         7.1945e-01,  3.0700e-01, -6.8488e-01,  1.0735e+00,  8.4773e-01,\n",
      "         4.8634e-01, -3.0052e-01, -1.2109e+00,  1.0221e+00, -2.6348e-01,\n",
      "        -1.2127e-01, -1.5244e+00, -4.5991e-01, -1.4562e+00, -1.2570e+00,\n",
      "         1.7503e+00, -6.8562e-01,  1.0005e+00,  2.6424e-01, -8.9744e-01,\n",
      "         2.5541e-01,  6.0619e-01, -1.7036e-01, -6.7428e-01,  3.1419e-01,\n",
      "        -1.9055e-01, -1.6434e-01,  6.0890e-01,  3.4664e-01,  1.1642e+00,\n",
      "        -3.1831e-02,  3.0466e-01, -1.0466e-01,  2.7982e-01, -5.5259e-01,\n",
      "        -3.4604e-01,  1.8325e+00,  2.6336e-01,  1.6848e+00, -4.2523e-02,\n",
      "         7.8475e-01, -1.6552e-01, -1.1500e+00, -7.6897e-01,  1.5263e+00,\n",
      "        -1.1163e+00, -1.3698e+00, -1.4242e-02, -2.6218e-01,  3.8559e-02,\n",
      "         1.9599e+00,  1.6791e+00,  1.0012e+00,  1.1607e+00, -1.1652e+00,\n",
      "        -1.8108e+00,  1.1220e+00, -6.7585e-01, -3.0625e-01, -1.0629e+00,\n",
      "        -1.8702e+00,  2.9486e-01,  9.3799e-01,  1.4361e+00, -8.0363e-01,\n",
      "        -8.5324e-01, -1.1815e-01, -8.3002e-01, -5.1348e-01,  6.5100e-02,\n",
      "         2.2124e-01, -1.7662e+00,  7.5315e-01,  6.8916e-01,  1.8278e+00,\n",
      "         6.5229e-01,  9.5207e-01, -2.0148e+00,  9.7852e-02, -1.6453e+00,\n",
      "         9.4977e-01, -1.5165e+00,  4.4378e-01, -1.3569e+00, -4.3038e-01,\n",
      "         2.6308e+00,  7.1380e-01, -9.6566e-01, -2.8014e-02, -2.0346e-01,\n",
      "        -6.6602e-01, -1.5479e-01, -2.3069e-01, -2.2924e+00, -3.6005e-01,\n",
      "         1.7610e+00,  7.1804e-01,  1.4528e-01, -3.8017e-01, -1.8186e+00,\n",
      "         1.2048e+00,  2.3850e-01, -8.8123e-01, -5.5633e-01,  1.4714e-01,\n",
      "         4.4584e-01, -1.3085e+00,  1.4255e+00,  1.1650e+00, -3.2979e-01,\n",
      "         2.7374e+00,  7.0533e-01, -5.1474e-01, -1.0407e+00, -7.3090e-01,\n",
      "        -8.9104e-01, -1.8159e-01, -4.9962e-01, -1.1151e+00, -2.0916e-02,\n",
      "         4.1388e-01,  1.8028e+00, -1.8977e+00, -1.2639e+00,  3.7863e-02,\n",
      "         1.5213e+00,  5.2473e-01, -3.3450e-01, -3.1500e-01, -9.7226e-01,\n",
      "        -3.3316e-01, -3.8988e-01,  1.1758e+00,  9.9082e-01, -1.2621e+00,\n",
      "         5.7869e-01, -1.2379e+00, -1.0676e-02, -8.0248e-01, -6.5136e-01,\n",
      "         1.1959e+00,  1.2623e+00, -1.3994e+00, -1.5959e+00,  4.3627e-02,\n",
      "        -2.7774e+00, -1.2417e-01, -1.7838e-03,  1.8213e-01,  1.5908e+00,\n",
      "        -5.0868e-02,  6.6021e-01,  1.2199e+00, -1.0032e+00, -2.1162e+00,\n",
      "        -9.4388e-01,  1.1533e+00,  2.1744e-01, -1.3981e+00, -4.6244e-01,\n",
      "         2.8425e-01, -1.8355e+00,  3.9394e-01,  5.9228e-02, -1.7839e+00,\n",
      "        -1.5341e+00, -8.3640e-01,  8.9694e-01, -1.6854e+00, -7.8385e-02,\n",
      "         7.1612e-01,  2.2940e-02,  2.0919e-01,  2.4287e-01, -1.6764e+00,\n",
      "        -2.7878e-01, -1.7475e-01, -2.9480e-01, -3.0856e-01, -1.7798e+00,\n",
      "        -9.4764e-02,  2.7153e-01,  1.1725e+00, -3.5052e-01, -7.6816e-01,\n",
      "         7.0127e-01,  3.7592e-01, -2.1538e+00, -1.6090e+00, -2.4774e-01,\n",
      "        -1.2682e+00, -8.7765e-01, -6.7274e-01, -6.0410e-01, -1.2331e+00,\n",
      "        -3.0042e-01,  5.1654e-01,  1.2616e+00, -1.0193e+00, -1.5380e+00,\n",
      "        -5.9090e-01,  5.5091e-01, -1.1348e+00,  1.0373e+00,  9.1123e-02,\n",
      "        -2.0363e-01,  3.2774e-01, -1.2707e-01,  2.3321e-01, -1.6679e-01,\n",
      "        -1.1670e+00, -7.4265e-01, -4.7575e-02, -8.5443e-01,  2.2166e-01,\n",
      "        -1.4631e-01, -1.7353e+00,  2.6903e-01,  1.6023e-01,  6.7719e-01,\n",
      "        -2.2439e-01,  2.6438e-01, -1.4636e+00, -7.3219e-01,  2.2204e-01,\n",
      "         7.8358e-01,  3.8155e-01, -4.4020e-01, -1.1691e+00, -2.3114e+00,\n",
      "         2.8595e+00, -4.9482e-01,  7.1404e-01, -3.2816e-01,  9.7962e-01,\n",
      "        -2.4362e-01, -1.5437e+00, -1.7890e+00,  5.2350e-01, -1.3142e-01,\n",
      "        -2.3911e+00,  1.7217e+00, -2.6341e+00,  1.2582e+00, -2.1269e+00,\n",
      "        -7.6320e-01,  2.9921e-01,  4.2502e-01, -9.1756e-01, -4.1690e-01,\n",
      "        -2.3912e-01,  1.0890e+00,  3.7396e-01,  1.1658e-01,  1.2861e+00,\n",
      "        -7.3610e-01,  2.0948e+00, -3.2340e-01,  1.1455e+00,  1.3741e+00,\n",
      "        -9.0534e-01,  8.5793e-01, -2.2318e-01,  1.1122e+00,  6.9691e-01,\n",
      "        -2.4122e-01, -1.4924e+00,  2.1287e-02, -2.3263e-01,  1.3307e+00,\n",
      "         6.0073e-01,  9.0132e-01,  3.3473e-02,  3.0340e-01, -1.0658e+00,\n",
      "        -1.7284e+00,  4.6251e-01, -4.2330e-01,  7.4098e-01, -1.0129e+00,\n",
      "         1.9932e-01,  1.2903e+00,  5.5976e-01,  7.6279e-01,  1.7088e+00,\n",
      "         1.2073e-01,  2.3618e+00, -7.4015e-01, -1.7125e+00,  3.2177e-01,\n",
      "        -1.7023e+00, -8.6612e-01, -3.8345e-01, -1.3972e+00, -8.6544e-01,\n",
      "         1.4479e+00, -1.5673e-01, -1.3507e+00,  3.1106e-01, -5.3462e-01,\n",
      "         7.1970e-01,  2.4405e-01, -4.0161e-01, -1.0260e+00, -1.8922e-02,\n",
      "        -3.1911e-01, -6.3973e-01,  9.8259e-01,  2.4000e+00, -5.4717e-01,\n",
      "        -1.3366e-01, -3.3934e-02,  2.7882e-01, -1.2063e+00, -8.6988e-01,\n",
      "        -1.3667e+00,  1.5717e-03, -1.1738e-01, -2.6063e-01, -2.5855e-01,\n",
      "        -3.3939e-01,  6.8232e-02, -1.2281e+00,  1.5431e+00, -1.6102e+00,\n",
      "        -6.2689e-01,  8.1194e-01, -1.0662e+00, -7.5461e-01,  7.0062e-01,\n",
      "        -7.8239e-01,  2.5521e-01, -2.8116e-01, -1.9188e+00, -5.4532e-01,\n",
      "        -4.3813e-01, -1.6853e+00,  1.8921e+00,  3.2930e-01, -7.3997e-01,\n",
      "         1.2624e+00,  1.0328e+00, -3.3149e-01,  1.5277e+00, -1.2135e+00,\n",
      "        -1.7344e+00, -1.1582e+00,  7.5884e-01, -6.6258e-01,  8.6097e-01,\n",
      "         1.0328e+00, -7.1497e-01,  5.1048e-01, -1.4993e-01,  5.3529e-01,\n",
      "        -1.4030e+00,  1.3235e+00, -2.4118e+00,  8.0907e-02, -1.6947e+00,\n",
      "         2.8127e-01,  1.0727e+00,  9.4061e-01, -1.1862e+00, -8.0362e-02,\n",
      "         1.1921e+00, -4.2459e-01,  8.1968e-02, -4.8431e-01, -1.0893e-01,\n",
      "         5.9717e-01, -6.5488e-01, -6.2715e-02, -1.3811e+00, -5.4428e-01,\n",
      "         9.3493e-01, -2.9194e-01, -3.0271e-01,  1.4071e+00, -1.1016e+00,\n",
      "         2.8888e-01,  2.7623e-01,  2.2635e-01, -1.5539e+00, -8.8511e-01,\n",
      "         2.2418e+00, -1.9684e+00,  2.9835e-01,  8.5899e-01, -5.2152e-01,\n",
      "         1.7990e+00,  7.1699e-01, -6.3717e-01, -1.1774e+00, -1.0948e+00,\n",
      "         4.6163e-01,  1.7642e-01, -8.1683e-01, -3.3163e-01,  6.7643e-01,\n",
      "         7.7208e-01,  3.8417e-01, -3.9312e-01,  1.1903e+00,  8.0827e-01,\n",
      "         3.2874e-01, -7.0140e-01, -2.3277e+00,  1.3766e+00,  6.2258e-01,\n",
      "        -2.2043e-01,  6.5580e-01,  8.7055e-03, -4.2044e-01,  1.1808e-01,\n",
      "        -6.2526e-01, -1.1595e+00, -1.9007e+00, -3.9841e-02,  9.3693e-01,\n",
      "        -4.7098e-01,  1.5653e+00, -2.6084e+00,  3.0894e-01,  1.2710e+00,\n",
      "        -1.0817e+00, -1.1604e-01,  1.3498e+00,  1.5686e+00,  4.9746e-01,\n",
      "        -9.5059e-01, -1.8064e+00, -1.1996e-01,  4.5589e-01,  4.8778e-01,\n",
      "        -1.9485e-02, -1.8794e-01, -1.4926e-01, -2.2766e-01, -1.7381e-02,\n",
      "         8.8352e-01,  3.7620e-01, -9.5194e-01, -3.2286e+00,  5.9696e-01,\n",
      "        -3.8267e-01,  3.1474e-01, -2.0150e+00,  1.0116e+00, -1.7836e+00,\n",
      "         1.1654e+00, -4.5846e-01,  1.9533e-01,  4.7073e-01, -8.9169e-01,\n",
      "        -1.5525e+00, -8.8105e-01,  7.6892e-01, -1.6965e+00, -4.3336e-01,\n",
      "         2.8131e-01, -1.2352e+00, -6.0389e-03, -1.4160e-01, -5.1977e-01,\n",
      "        -3.2225e-01, -8.7067e-01, -2.1530e+00, -1.2456e+00, -3.9350e-01,\n",
      "        -9.6663e-01,  8.4897e-01,  1.2885e+00, -3.7622e-01, -1.0833e+00,\n",
      "        -8.1281e-03,  8.5874e-01, -1.3968e-01, -6.1777e-01, -1.7604e+00,\n",
      "         8.8747e-01,  2.1490e-01, -1.8698e+00, -1.1970e-01,  1.4864e+00,\n",
      "        -2.7825e-01,  1.2267e+00, -7.0936e-01,  8.9829e-01, -2.8539e-01,\n",
      "         1.3119e-01,  1.8628e+00, -5.8883e-01, -3.6472e-01, -4.2674e-01,\n",
      "         8.1312e-01,  1.0352e+00, -3.2643e-01, -7.7788e-01, -1.7042e+00,\n",
      "         1.1953e-02,  1.5109e+00, -2.6527e-01, -7.5489e-02, -2.9884e-01,\n",
      "         1.7427e+00,  1.4471e-01, -8.0687e-01,  5.8211e-01, -1.4177e+00,\n",
      "        -2.7831e-01, -1.2149e+00,  1.2045e+00, -1.2615e-01,  1.5155e+00,\n",
      "        -1.6112e+00,  1.5362e+00,  7.0634e-01, -6.2468e-01, -5.2802e-01,\n",
      "        -5.1575e-01,  2.7738e-01,  1.7723e+00,  7.2000e-01,  1.4213e+00,\n",
      "        -2.3153e+00,  7.5717e-01,  6.8327e-01,  1.4865e+00, -2.2380e-01,\n",
      "        -2.0468e-01, -1.2736e+00, -2.0676e-01, -3.7304e-01, -8.9019e-01,\n",
      "         1.3797e+00, -2.1903e-01, -9.8020e-01, -2.6893e-02,  2.8206e-01,\n",
      "         8.4501e-01,  1.6529e+00, -1.1920e+00, -2.3512e+00,  5.7163e-01,\n",
      "        -8.5528e-01, -1.4919e-01,  8.9936e-01, -5.3385e-01,  1.7128e-01,\n",
      "         9.8400e-01,  1.7604e-01, -1.8488e-01, -9.3252e-01, -8.5933e-01,\n",
      "         8.9318e-01, -1.3226e+00,  6.8374e-01,  1.4620e+00, -1.2816e+00,\n",
      "         1.5303e+00,  8.1317e-01, -1.6062e+00,  2.2278e-01, -1.6265e+00,\n",
      "        -3.8002e-01,  1.4623e+00,  1.1926e+00,  1.3431e+00,  1.2399e+00,\n",
      "        -5.6962e-01, -1.2173e+00,  5.6539e-01, -1.3971e-01,  4.4281e-01,\n",
      "        -9.2706e-01, -4.4768e-01,  7.1613e-01, -1.0115e+00, -3.1706e-01,\n",
      "        -5.8217e-01, -7.9853e-01, -1.1560e+00, -2.7827e-01,  1.5302e-01,\n",
      "        -5.5858e-01,  7.2003e-01,  5.2976e-01,  7.2943e-01,  1.1750e+00,\n",
      "         7.1467e-01,  3.5288e-01, -1.1016e+00,  2.2969e+00,  1.5530e+00,\n",
      "        -1.2089e-02, -1.0706e+00,  2.1842e-01, -1.0681e+00, -2.9241e-01,\n",
      "        -5.5556e-01, -6.6446e-01,  2.3408e+00,  3.6792e-01,  1.2063e+00,\n",
      "        -4.2363e-01,  1.4799e+00,  2.9863e-01,  9.4630e-01, -1.5268e-01,\n",
      "         1.2619e+00, -7.6149e-01, -1.7208e-01], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([ 1.7262e+00,  1.2505e+00,  4.9811e-01,  1.5653e+00, -6.0056e-01,\n",
      "         8.0729e-01, -3.9429e-01, -1.2371e+00,  4.4861e-01,  5.3796e-01,\n",
      "         1.2052e+00,  1.0682e+00,  1.4230e+00,  2.3213e-01,  8.0851e-02,\n",
      "         1.7415e-01, -1.2116e+00, -1.0914e+00, -7.2005e-01,  4.6805e-01,\n",
      "         5.7978e-01, -4.4128e-01,  1.7636e+00, -4.9754e-01,  7.6154e-01,\n",
      "         1.8171e+00,  2.8426e-01, -9.5967e-01, -1.5505e-01,  9.4651e-01,\n",
      "        -6.6262e-01, -2.8523e-01,  4.3439e-02,  2.4013e+00, -4.1730e-01,\n",
      "         2.0757e+00,  1.2517e+00, -8.7718e-02, -7.9227e-01,  6.8793e-01,\n",
      "         6.9743e-02, -9.3229e-01, -1.6422e-02,  1.8021e+00,  8.3489e-01,\n",
      "        -4.2542e-01,  1.8826e+00, -2.2307e+00,  1.2764e+00, -2.3127e+00,\n",
      "        -7.3812e-01,  4.5989e-01, -5.4836e-01, -9.9867e-01, -1.4954e+00,\n",
      "         8.8222e-01, -4.8680e-01,  4.4512e-01, -2.1897e-02, -2.7146e-01,\n",
      "        -1.8042e+00, -2.9794e-01,  4.1847e-01,  8.6181e-01, -1.4805e+00,\n",
      "         1.3259e+00,  3.8787e-02,  6.8895e-01,  7.9022e-01, -4.1950e-01,\n",
      "         1.4894e-01, -1.4037e-01,  1.1937e+00,  4.8640e-01, -2.1417e-01,\n",
      "         6.4220e-04, -2.2582e-01, -4.9818e-01,  3.3715e-01,  5.4975e-01,\n",
      "         1.0321e+00, -3.2062e-01, -1.4107e+00,  1.6165e-01, -1.4259e+00,\n",
      "        -1.3668e+00, -4.5188e-01, -8.0888e-01, -8.5543e-01, -6.5682e-01,\n",
      "         1.9371e+00,  1.3938e+00,  1.7061e+00,  9.6284e-01,  4.3946e-02,\n",
      "        -7.6088e-01, -8.9706e-01, -7.6784e-01,  9.9726e-01,  1.1056e+00,\n",
      "        -1.4147e+00, -2.6370e-01, -7.8807e-01, -1.1182e+00, -7.0214e-01,\n",
      "         2.5232e-01,  6.6134e-01, -4.1784e-01, -1.0676e+00, -7.9071e-01,\n",
      "        -1.2343e+00,  3.5529e-01,  7.8475e-01,  2.3192e-01,  1.0367e+00,\n",
      "        -4.6513e-01,  1.1577e+00,  8.8637e-01,  1.5070e-01,  4.1081e-01,\n",
      "        -3.9427e-01,  1.0286e+00,  7.0168e-01,  5.4695e-01, -1.1633e-01,\n",
      "         5.2832e-01, -2.9100e-01, -1.9119e+00, -1.6545e-02,  2.6972e-01,\n",
      "         1.3080e-01, -2.3125e-01,  1.3145e+00,  1.4344e+00,  9.5809e-01,\n",
      "        -1.0413e-01, -2.0510e+00, -3.0146e-01,  1.7212e+00, -3.6925e-01,\n",
      "         1.0684e+00,  5.5040e-01,  7.0700e-02,  3.1697e-01,  2.0024e-01,\n",
      "         9.7729e-01, -1.2682e+00,  1.3388e+00, -9.8946e-01,  1.1315e+00,\n",
      "         3.2634e-01, -6.0164e-01, -1.1131e+00,  7.9564e-02,  1.1104e+00,\n",
      "        -8.8160e-01,  4.5906e-01,  2.0834e+00, -4.0279e-02, -3.0471e-01,\n",
      "         4.5728e-01, -1.5060e+00, -1.1093e-02,  7.0206e-02,  1.3678e+00,\n",
      "        -1.1128e+00, -5.0278e-01, -2.4783e+00,  7.6480e-01, -1.8845e+00,\n",
      "        -8.5263e-01, -5.3298e-01,  1.6693e+00,  7.2412e-01,  1.4295e+00,\n",
      "        -8.5862e-01,  1.2124e+00,  2.2533e+00, -1.1790e+00, -1.4623e+00,\n",
      "        -1.6989e+00, -7.9731e-02, -1.3369e+00,  1.2163e+00,  1.9793e-01,\n",
      "         1.5175e+00, -1.5101e+00, -1.2071e+00, -3.6004e-01,  5.7459e-01,\n",
      "        -3.5218e-03,  8.7811e-01, -1.0676e+00, -6.0614e-01, -3.3193e-02,\n",
      "         5.6231e-01, -1.9215e-01,  4.4216e-01,  2.0170e-01,  1.0966e-01,\n",
      "         1.6526e+00, -1.2198e-01, -6.3403e-01,  4.8349e-01,  1.8032e-01,\n",
      "         1.0323e+00,  9.7959e-01,  8.4440e-01, -1.1782e+00,  1.3525e-01,\n",
      "        -1.0342e+00,  8.9078e-02, -4.7227e-01,  6.1032e-01,  1.1674e+00,\n",
      "         1.2304e-01, -4.8408e-01, -3.7444e-01, -7.9611e-02, -1.0656e+00,\n",
      "         1.4112e+00,  5.8601e-01,  2.2848e-01, -4.2433e-01,  1.1214e+00,\n",
      "         1.6477e+00,  6.3809e-01, -3.4692e-01, -5.2366e-01, -3.4510e-02,\n",
      "        -2.4809e-01,  2.8182e-01,  1.1938e+00,  2.3830e-01, -3.7460e-01,\n",
      "        -3.0868e-01,  1.0779e+00,  7.6177e-01, -1.0460e+00,  2.0045e-01,\n",
      "        -1.7027e-02,  1.2252e+00, -7.4875e-01,  9.5987e-01,  7.0739e-01,\n",
      "         9.4029e-01, -1.0765e+00,  4.4560e-01,  9.3662e-01,  7.1799e-01,\n",
      "         2.5009e-01, -9.3039e-01,  4.3493e-02,  4.8119e-01, -1.0676e+00,\n",
      "        -4.3508e-01, -1.1120e-01,  3.2098e-02, -5.3594e-01,  3.2401e-01,\n",
      "         1.9778e+00,  8.3768e-01,  6.2546e-01,  5.6417e-01, -2.5355e+00,\n",
      "        -1.6375e-01, -1.1373e+00,  4.4987e-03,  2.2691e+00,  4.1725e-01,\n",
      "         9.6591e-01, -1.0919e+00, -1.2356e+00, -1.4759e+00,  1.0644e-01,\n",
      "         1.9593e+00, -6.1419e-01, -6.7922e-01,  1.8883e+00, -2.6819e-01,\n",
      "        -8.5443e-02, -3.5616e-01,  1.2565e+00,  9.5130e-01, -3.7869e-01,\n",
      "        -4.8489e-01, -5.0873e-01, -6.4862e-01, -1.5932e+00,  6.2414e-01,\n",
      "         1.9854e+00, -8.3744e-01,  1.8922e+00,  7.5005e-01, -6.0219e-03,\n",
      "        -6.2104e-02,  9.0225e-02,  2.1988e-01,  7.6734e-02, -5.8195e-01,\n",
      "         8.3694e-01, -9.8171e-01,  1.3828e+00,  8.6585e-01,  3.8016e-01,\n",
      "         8.7985e-01, -1.9834e+00,  1.0380e+00,  3.0892e-01, -1.4001e+00,\n",
      "         9.8107e-02,  4.5947e-01,  1.1684e+00, -1.1331e+00,  2.9776e-01,\n",
      "        -1.4795e+00,  3.8555e-01, -7.2305e-02,  7.1899e-01,  1.2450e+00,\n",
      "        -1.4265e+00, -1.0502e-01,  5.7314e-02, -8.7159e-01,  3.2031e-01,\n",
      "         7.3602e-01,  1.3601e+00,  1.6547e+00,  9.2455e-01,  8.9337e-02,\n",
      "         5.1139e-01, -2.9663e-01,  1.0944e+00,  1.6590e-01, -2.2141e+00,\n",
      "        -3.9729e-01, -9.1374e-02,  1.7169e-01,  1.0870e+00, -1.4122e+00,\n",
      "        -2.9160e-02, -4.9586e-01, -8.8127e-01,  1.4906e+00,  1.0366e+00,\n",
      "         7.7996e-02, -5.2772e-01,  1.1783e+00,  3.0900e-01,  3.2959e-01,\n",
      "        -3.4513e+00, -9.7627e-01, -4.9668e-01, -1.1513e+00,  2.4941e+00,\n",
      "         2.1436e+00,  5.1997e-01,  5.2500e-01, -1.4907e+00, -1.7504e-01,\n",
      "        -1.2206e+00,  5.1152e-01, -6.4935e-01, -4.8021e-01, -6.1315e-01,\n",
      "        -3.3012e-01, -1.7759e+00,  2.2341e-01, -1.4347e+00,  8.5208e-01,\n",
      "        -2.0731e-01, -1.4208e+00,  1.1192e+00, -2.2002e+00,  2.7020e-01,\n",
      "         2.2339e-01,  1.5119e+00, -5.9205e-01, -1.0447e+00,  1.4362e+00,\n",
      "        -8.3912e-01,  1.9242e-01, -9.6448e-01, -3.4301e-01,  1.0755e-01,\n",
      "        -1.4968e+00,  9.7613e-01, -5.8941e-01, -2.6704e-01,  2.5512e+00,\n",
      "        -9.1701e-01, -1.2229e+00,  1.0496e+00, -9.2403e-02,  5.1084e-01,\n",
      "        -7.8129e-01,  9.4881e-02, -3.8144e-01,  9.0286e-01, -1.1208e+00,\n",
      "         3.1602e+00, -2.3898e-01, -1.4031e-01,  1.2889e+00,  5.9963e-01,\n",
      "         7.5962e-02, -4.1732e-01,  4.5976e-01, -1.2409e+00, -8.6132e-01,\n",
      "        -1.2723e+00, -5.6392e-01,  1.8256e+00,  8.5845e-01,  4.8432e-01,\n",
      "         1.2632e+00,  5.1035e-01,  1.4017e+00,  7.3690e-01, -5.1991e-01,\n",
      "         1.6257e+00, -8.5820e-02, -8.2182e-01, -5.0365e-01, -1.0782e+00,\n",
      "         1.0847e+00,  2.1961e+00,  6.4850e-01, -1.3860e+00, -8.9481e-01,\n",
      "         6.8127e-01,  4.8778e-01, -1.3077e+00,  1.4034e-01,  7.1663e-01,\n",
      "        -2.3878e-01,  5.6408e-01, -2.7777e-01, -1.3484e-02,  1.8356e+00,\n",
      "         3.9099e-01, -6.0574e-01, -1.1768e+00, -3.5206e-01, -2.9027e-01,\n",
      "        -2.1037e-01, -2.6773e-01, -6.8272e-01, -1.2126e-01,  7.0282e-01,\n",
      "        -6.6720e-01,  3.3678e-01, -8.2397e-01, -5.8982e-01, -1.7569e+00,\n",
      "        -2.4350e-01, -1.1306e+00, -8.2106e-01, -9.0511e-01, -2.0637e+00,\n",
      "        -2.3765e+00, -3.3599e-01,  9.3589e-01,  2.0355e-01,  8.9595e-01,\n",
      "         1.0208e+00,  8.3215e-01, -7.1457e-01, -1.9986e-01, -9.2819e-01,\n",
      "        -1.3470e+00,  6.7589e-01, -6.1833e-01, -1.6318e+00, -9.3038e-01,\n",
      "        -2.4433e-01,  5.3261e-01, -2.0267e-02,  1.5253e+00, -2.3252e-03,\n",
      "        -2.0767e-01,  8.7382e-01, -1.1748e+00,  1.6039e+00,  2.2094e+00,\n",
      "        -1.0751e+00,  8.3008e-01, -1.5569e+00, -1.0952e-01,  8.7069e-01,\n",
      "        -8.1586e-02, -1.6090e-01, -1.0609e+00, -7.0995e-02, -1.4867e+00,\n",
      "        -1.7175e+00, -7.1328e-01,  1.1534e-01,  5.4701e-01,  1.0212e+00,\n",
      "         5.3079e-01, -5.3607e-02,  1.2069e-01,  1.0313e+00,  8.0229e-01,\n",
      "         1.4710e-01,  1.6441e+00,  2.3800e+00, -1.3452e-01,  7.5391e-01,\n",
      "        -1.2712e+00,  1.2103e+00,  7.7490e-01,  5.2585e-01, -3.8780e-01,\n",
      "        -2.0379e-01,  1.1631e+00, -7.3947e-01, -9.6066e-03, -3.5028e-01,\n",
      "        -1.6218e-01, -7.2437e-01, -9.2201e-01,  3.1495e-01, -3.4924e-01,\n",
      "        -1.2998e+00,  1.0983e+00,  7.8731e-01,  5.7245e-02,  8.7795e-01,\n",
      "        -1.2558e-02,  6.0266e-01, -9.6055e-01,  5.8953e-02,  1.2047e+00,\n",
      "        -1.2666e+00,  1.1900e+00, -1.5945e-01,  6.6071e-01,  8.8141e-01,\n",
      "         2.2556e-01,  1.6791e+00,  5.3159e-01,  2.1242e+00, -7.6692e-02,\n",
      "         1.0074e+00, -1.2509e+00,  1.9360e-03, -3.0270e-01, -2.8049e-02,\n",
      "        -7.1331e-01,  5.7099e-01,  4.5428e-01,  9.3924e-01, -5.7269e-01,\n",
      "        -3.8371e-02, -1.6727e+00, -3.7537e-01,  8.0766e-01,  1.6952e+00,\n",
      "         6.8832e-01, -1.9159e+00,  1.0204e+00,  5.0466e-01,  1.0116e+00,\n",
      "        -1.3265e+00,  1.9310e-01, -3.9474e-01, -2.3768e+00,  9.5192e-01,\n",
      "        -7.7341e-01,  1.6063e-01,  2.1102e-01,  6.7568e-01, -2.5838e-01,\n",
      "        -1.0590e+00,  1.0655e+00,  1.9253e+00, -1.0531e+00,  8.6803e-01,\n",
      "         3.5307e-02, -2.9634e-01, -1.2185e+00,  1.3918e+00, -4.6622e-01,\n",
      "         7.2889e-01, -1.7376e-01, -7.1965e-02, -1.3162e+00, -9.7179e-01,\n",
      "         1.7531e-01, -1.0071e+00, -2.6059e+00,  7.1370e-01, -1.1791e+00,\n",
      "         7.8057e-02, -1.7423e-02,  1.5143e+00, -1.8836e-01,  1.7923e+00,\n",
      "         1.6711e-01, -8.8834e-01,  1.0711e+00,  8.2209e-01, -3.9787e-01,\n",
      "        -1.8378e+00, -1.0070e+00,  1.4369e+00,  1.0574e+00, -6.4973e-01,\n",
      "        -8.2241e-01,  2.8282e-02,  1.8816e-01,  7.8606e-02, -1.7747e+00,\n",
      "         3.0223e-01,  9.7591e-01, -2.2337e-01,  9.8229e-01,  3.3454e-01,\n",
      "        -7.5847e-01, -2.1378e-01,  1.0769e+00, -1.2017e+00,  8.8406e-01,\n",
      "        -2.1525e+00,  1.1729e+00, -5.4028e-01, -3.4249e-01, -1.1073e+00,\n",
      "        -1.4994e+00, -1.0936e+00, -2.2808e+00, -1.5791e+00,  2.7851e-01,\n",
      "         9.9834e-02,  2.8511e-01, -6.6922e-01, -1.8547e+00, -6.9824e-01,\n",
      "         6.9262e-01, -1.1964e-02,  5.1537e-01, -3.0979e-01, -2.6041e-02,\n",
      "         8.8845e-01, -5.4818e-01,  1.8112e-01, -1.3316e-01, -1.8393e+00,\n",
      "         1.5357e+00,  2.2083e+00, -2.1710e-01, -5.7033e-02,  7.7799e-01,\n",
      "        -5.4374e-01, -5.5125e-01, -1.0179e+00, -4.2785e-01, -1.4031e+00,\n",
      "         9.6640e-02, -2.3415e-01, -2.2791e+00, -1.1465e-01, -1.0619e+00,\n",
      "        -7.2938e-01, -4.8392e-01, -5.3572e-01,  2.8128e-01,  1.0937e+00,\n",
      "         7.5021e-01,  7.8202e-01, -7.1368e-01, -1.6122e+00, -2.9853e-01,\n",
      "         8.2530e-01, -7.3171e-02,  3.5381e-01, -1.0375e+00, -9.4624e-01,\n",
      "         8.2152e-01,  6.1578e-01, -1.1971e+00, -3.4914e-01, -8.6481e-01,\n",
      "         1.1496e+00,  1.1367e+00,  1.2976e+00, -8.0757e-01, -3.8355e-01,\n",
      "        -4.0250e-01,  9.9927e-01,  2.4087e-01, -6.6587e-01,  4.6383e-01,\n",
      "         3.9178e+00,  9.3638e-01,  2.4697e-01, -1.4065e-01,  6.1828e-02,\n",
      "        -2.6888e-01,  3.9005e-01, -6.5218e-01, -3.7942e-02,  3.6662e-01,\n",
      "         2.0368e+00, -1.4587e-01, -8.6397e-01, -1.5678e+00,  6.3487e-01,\n",
      "         8.8388e-01, -6.4405e-01, -2.2450e+00,  8.3624e-01, -6.3033e-01,\n",
      "        -1.2961e+00, -2.4982e-02,  3.6447e-01, -4.9910e-01, -5.3900e-01,\n",
      "         6.4992e-01, -1.4438e+00,  3.4363e-01,  6.0485e-01,  6.9444e-02,\n",
      "        -8.9969e-01, -1.8619e+00, -3.0149e-02, -1.0343e-01,  9.4776e-02,\n",
      "        -3.9698e-01,  4.2774e-01,  6.5539e-01,  1.5575e-01,  2.4890e-01,\n",
      "        -4.7763e-01,  7.2982e-01,  6.6099e-02,  6.1213e-01, -2.0085e-01,\n",
      "        -1.2757e+00, -3.6357e-01, -9.7847e-01,  7.6149e-01, -1.2569e+00,\n",
      "         3.9466e-01,  1.5269e+00,  1.4590e+00,  2.3770e+00, -7.8534e-01,\n",
      "         2.9559e-01,  3.9047e-01,  1.7651e+00, -1.1708e+00,  5.4245e-01,\n",
      "         1.7307e+00, -5.5884e-01, -4.2325e-01, -1.5570e+00, -1.6572e+00,\n",
      "         1.9560e-01,  1.1142e+00, -3.1746e-01,  5.2081e-01, -1.6217e-01,\n",
      "        -9.3781e-01,  5.2247e-01, -1.1535e+00], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0138, -0.0221,  0.0226,  ..., -0.0042,  0.0214,  0.0123],\n",
      "        [ 0.0149,  0.0174,  0.0108,  ..., -0.0214,  0.0018, -0.0066],\n",
      "        [-0.0017,  0.0181,  0.0173,  ...,  0.0119, -0.0115,  0.0129],\n",
      "        ...,\n",
      "        [-0.0225,  0.0074,  0.0058,  ...,  0.0174, -0.0193,  0.0161],\n",
      "        [ 0.0243, -0.0217, -0.0065,  ...,  0.0121,  0.0019, -0.0181],\n",
      "        [ 0.0056,  0.0145,  0.0068,  ..., -0.0193,  0.0121,  0.0069]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-6.1555e-05,  4.6174e-03, -1.1860e-02,  1.3748e-02, -1.7582e-02,\n",
      "         5.4140e-03,  3.3762e-03, -2.3225e-02, -3.2451e-03,  1.4075e-02,\n",
      "         1.9490e-02, -2.1404e-02, -2.0953e-02, -1.1593e-02,  1.7295e-02,\n",
      "         8.7412e-03, -2.9133e-03,  1.0346e-02, -7.3144e-03, -7.7503e-04,\n",
      "        -1.4836e-03, -2.2846e-02, -1.6553e-02, -4.3867e-04,  5.8542e-03,\n",
      "         2.3226e-02, -2.5381e-02, -1.4959e-03,  1.1433e-02,  2.5170e-02,\n",
      "         1.1709e-02,  9.8935e-03, -1.0338e-02, -2.3632e-02,  1.2170e-03,\n",
      "         1.3877e-02, -1.2697e-02, -9.8018e-03,  3.8147e-03, -4.1076e-04,\n",
      "         1.6395e-02,  6.1150e-03, -1.5848e-02,  1.0781e-03, -5.4909e-03,\n",
      "        -7.9390e-03, -6.8822e-03,  1.0496e-02,  1.9457e-02,  2.1679e-02,\n",
      "        -3.3253e-03,  6.6099e-05,  2.4942e-02, -1.4528e-02, -2.9849e-03,\n",
      "         2.4845e-02, -2.3354e-02, -1.4213e-02, -1.9919e-02, -2.1871e-02,\n",
      "        -4.7612e-04, -1.8712e-02,  1.2705e-02,  1.2321e-02,  1.6033e-02,\n",
      "         2.0308e-02, -2.0765e-02, -1.6447e-02,  6.8642e-03, -6.9826e-03,\n",
      "         1.0081e-02, -1.4812e-02, -1.9615e-02,  1.5339e-02,  1.4598e-02,\n",
      "        -8.2424e-03,  1.8312e-02, -1.5592e-02,  1.3422e-02,  1.7006e-02,\n",
      "         2.0934e-02, -5.0250e-03,  6.2336e-03,  1.3924e-02,  1.0877e-02,\n",
      "        -1.5977e-02,  5.2753e-03, -4.4413e-03, -8.9128e-03,  8.2836e-03,\n",
      "         1.7131e-02, -2.0294e-02,  6.9564e-03,  6.4659e-03, -1.7863e-02,\n",
      "        -8.0229e-03,  1.1947e-02, -6.3506e-03,  1.1878e-03, -2.6728e-03,\n",
      "         1.5121e-03, -1.8967e-02,  2.2294e-02,  2.0513e-02,  2.5425e-02,\n",
      "        -2.4914e-02,  1.0986e-02,  1.9996e-02,  1.6780e-02,  1.0749e-02,\n",
      "        -1.4211e-03, -8.9370e-04, -1.8704e-02, -1.8723e-02,  1.9784e-02,\n",
      "         4.5109e-03, -2.1846e-02, -1.1056e-02,  2.3633e-02,  2.0268e-02,\n",
      "         5.3543e-03,  1.9046e-02,  1.9479e-02,  1.5364e-02,  2.3143e-02,\n",
      "         2.0860e-02,  1.8131e-02, -2.1266e-02, -7.3540e-03,  8.8370e-03,\n",
      "         1.7585e-02, -2.2491e-02,  1.1302e-02,  7.2808e-03,  2.1961e-02,\n",
      "        -5.2169e-03,  2.7844e-03,  2.4415e-02,  1.9927e-02, -8.4091e-03,\n",
      "        -2.0316e-02,  2.4368e-03,  9.0833e-03, -2.2789e-02,  2.4202e-02,\n",
      "        -1.4109e-02,  1.9825e-02,  1.7563e-02, -2.3902e-02, -7.0193e-04,\n",
      "        -2.5575e-03, -6.9035e-03,  9.8719e-04, -1.5878e-02, -1.9232e-02,\n",
      "         1.9262e-02,  1.7351e-02,  7.8868e-03, -1.4747e-02,  2.2743e-02,\n",
      "         1.3651e-02,  5.1874e-03, -2.3001e-02, -2.2805e-02,  1.5219e-02,\n",
      "         2.0565e-03,  2.1153e-03, -1.5271e-02,  7.6628e-03, -2.2596e-02,\n",
      "         1.5471e-02, -1.4984e-02, -2.1355e-02,  6.4839e-03,  1.8279e-02,\n",
      "         1.1311e-02,  1.3138e-02,  1.0955e-02, -2.6523e-03,  2.1921e-02,\n",
      "        -2.2783e-03, -2.4627e-02,  1.6386e-03,  5.1797e-03, -1.5199e-02,\n",
      "        -2.1252e-02,  4.6791e-03,  1.5845e-02,  1.9185e-02,  2.9999e-03,\n",
      "        -1.6431e-02, -1.0264e-02, -6.2633e-03,  4.1046e-03,  6.4141e-03,\n",
      "        -1.5846e-02, -2.4876e-02, -1.5386e-02, -2.3990e-02, -2.2922e-02,\n",
      "         9.1018e-03,  1.9646e-02,  9.3138e-03, -4.3951e-03, -1.5439e-02,\n",
      "         1.1863e-02, -2.3960e-02,  3.1942e-04, -4.7279e-03,  1.9477e-02,\n",
      "         1.9554e-03, -5.6441e-03, -1.0486e-02,  1.8784e-03,  3.6368e-03,\n",
      "        -1.3390e-03,  2.4612e-02, -1.7062e-02, -1.2195e-03, -2.3546e-02,\n",
      "         1.4421e-02,  9.9184e-03,  1.3010e-03, -2.1703e-02,  4.6156e-03,\n",
      "         2.3348e-02, -3.1746e-04, -2.4446e-02, -7.0325e-03, -3.3932e-03,\n",
      "        -6.2365e-03, -4.0397e-03, -1.8293e-02,  1.1146e-03,  8.3709e-03,\n",
      "        -1.4170e-02, -7.2135e-03,  5.3819e-03, -1.1955e-02, -1.5066e-02,\n",
      "         3.9517e-03,  3.8991e-03,  1.9751e-02,  2.3940e-02,  1.7060e-02,\n",
      "         2.4804e-02,  4.1151e-03,  4.4448e-03, -2.3617e-02,  3.5697e-03,\n",
      "         5.0639e-03,  3.3202e-03,  1.5557e-02,  1.2628e-02,  1.3986e-02,\n",
      "        -1.1093e-02, -1.8934e-02,  1.6124e-02,  9.9051e-03, -9.0472e-03,\n",
      "         1.4308e-02,  1.8123e-03,  1.0584e-02, -1.7541e-03, -8.2679e-05,\n",
      "         1.9165e-02, -2.3600e-03, -7.6998e-03,  9.1160e-03,  1.3757e-02,\n",
      "         5.9971e-03, -2.4021e-02, -1.6368e-02, -6.5302e-04,  1.4370e-02,\n",
      "         1.0939e-03, -7.8568e-04, -2.4978e-02,  2.1133e-02, -8.4706e-03,\n",
      "        -2.1253e-02,  1.7971e-02, -1.4218e-02, -1.8622e-02, -1.0203e-02,\n",
      "        -2.1372e-02,  2.0765e-02,  8.4506e-03, -7.8321e-03, -1.5267e-02,\n",
      "         2.7775e-04,  8.9790e-03, -4.1873e-03, -1.5481e-02,  5.9275e-03,\n",
      "         3.8208e-03,  2.3153e-02, -1.0365e-02,  2.4032e-02,  1.6294e-03,\n",
      "        -7.9518e-03,  2.7333e-04, -2.4406e-02,  2.5135e-02, -2.5331e-02,\n",
      "        -6.0970e-03, -1.5176e-02,  9.4524e-04, -2.0415e-02, -2.3574e-02,\n",
      "         4.1209e-03, -1.0460e-02, -7.1748e-03,  5.5602e-03,  4.3011e-03,\n",
      "         1.9327e-02,  1.7172e-02,  1.0398e-02, -1.8179e-02,  1.7788e-02,\n",
      "         5.9895e-04,  1.4795e-02, -1.6711e-02, -1.8152e-02,  4.8810e-03,\n",
      "         2.3474e-02,  2.3529e-02, -8.3111e-03,  4.8815e-04, -4.0718e-03,\n",
      "        -5.6287e-03,  1.6673e-02,  1.6571e-02, -7.9571e-03, -2.4917e-02,\n",
      "        -9.3707e-03,  2.4714e-02,  1.9168e-02, -1.5131e-02, -3.0551e-03,\n",
      "        -8.0918e-03,  1.7093e-02, -1.1372e-02,  6.9357e-03, -1.5638e-02,\n",
      "         1.0792e-02, -1.6645e-02,  2.1710e-02,  1.7161e-03, -1.5697e-02,\n",
      "        -5.4832e-03, -1.2506e-02,  1.4811e-04, -1.3026e-03,  9.6961e-03,\n",
      "        -1.2987e-02,  1.3670e-02, -2.4832e-02, -1.3011e-02,  2.4076e-02,\n",
      "         1.5717e-02, -4.5634e-03, -3.6719e-03, -2.2860e-02, -2.1312e-02,\n",
      "         3.5695e-03, -4.6831e-03,  8.9750e-03, -2.0168e-02,  2.0910e-02,\n",
      "         8.2212e-03, -1.0984e-03,  9.4096e-03, -1.3168e-02,  1.9091e-02,\n",
      "        -1.8877e-02, -6.6924e-03, -2.4114e-02,  5.1198e-03,  2.3145e-02,\n",
      "        -1.2395e-03, -1.9031e-02,  1.5792e-02,  1.4385e-02,  2.9363e-03,\n",
      "        -1.9772e-02, -9.2005e-03, -8.2085e-03,  1.3632e-02, -2.3518e-02,\n",
      "        -1.7386e-02, -8.7812e-03, -8.8209e-03,  1.4383e-02,  5.9969e-03,\n",
      "        -9.2548e-03, -1.8947e-02, -2.0337e-02,  1.8652e-02, -2.4612e-03,\n",
      "        -1.3854e-02,  7.6917e-03,  2.3851e-02, -2.1382e-02, -1.5274e-02,\n",
      "         2.0574e-02,  1.9958e-02, -1.3837e-02,  1.8975e-02, -2.4808e-02,\n",
      "        -3.3259e-03, -1.1078e-02, -1.3896e-02, -2.1089e-02,  2.4854e-02,\n",
      "         1.4877e-02,  5.7411e-03,  8.7014e-03,  9.3043e-03,  1.4421e-02,\n",
      "         1.0439e-02, -1.0669e-02, -9.8454e-03,  1.2647e-02,  2.2644e-02,\n",
      "        -2.1130e-02,  1.0297e-02, -1.5300e-02, -2.4325e-02,  1.3340e-02,\n",
      "         1.1525e-02,  1.2613e-02, -9.3234e-03,  1.5962e-02, -1.8763e-02,\n",
      "        -7.2018e-03, -1.7330e-03,  2.8302e-03,  3.6133e-03,  5.5021e-03,\n",
      "         4.7491e-03,  1.7522e-02,  1.0248e-02,  4.5148e-03,  1.8400e-02,\n",
      "         2.3977e-02,  1.9771e-02, -1.4442e-02,  2.1858e-02, -1.9603e-02,\n",
      "        -1.7168e-02, -1.9762e-02,  3.4101e-03, -4.9150e-03, -1.7479e-03,\n",
      "        -7.5098e-03, -1.7343e-02,  6.4087e-03, -1.6414e-02, -1.1516e-02,\n",
      "         2.3777e-02, -1.7088e-02, -1.0631e-02, -1.2576e-02,  6.8082e-03,\n",
      "        -3.4225e-03, -2.1540e-02,  2.1402e-02,  1.8069e-02, -1.3782e-02,\n",
      "        -1.1047e-02, -1.4599e-03,  1.2247e-02, -8.4435e-04,  1.0002e-02,\n",
      "         1.0595e-02,  1.9715e-02,  7.4357e-03, -1.1020e-02,  2.5699e-03,\n",
      "        -7.6942e-03, -4.8235e-03,  1.0428e-02, -1.1840e-02,  1.8884e-02,\n",
      "        -6.4924e-03, -1.1510e-02,  1.1342e-02,  2.6751e-03, -2.7805e-03,\n",
      "        -2.3529e-02, -1.4151e-02,  8.5276e-03, -2.1425e-02,  2.3739e-02,\n",
      "         1.9756e-02,  1.4583e-03,  2.3732e-02, -1.2607e-02, -1.2913e-02,\n",
      "        -3.5764e-03, -1.9046e-02,  1.4794e-02,  2.4539e-02,  1.7737e-02,\n",
      "        -1.3709e-02, -9.8921e-03,  2.3329e-03,  2.3023e-02, -2.2389e-03,\n",
      "        -1.9065e-03, -3.9299e-03, -1.4314e-02, -3.4360e-03, -1.7036e-02,\n",
      "         6.1576e-03, -1.2241e-02, -3.7712e-03,  6.5666e-03, -9.7362e-03,\n",
      "        -6.3496e-03,  1.1190e-02, -7.1455e-03,  1.8132e-02,  1.5689e-02,\n",
      "        -1.0385e-02, -1.6944e-02,  2.3269e-02, -1.6204e-02, -1.7803e-02,\n",
      "        -2.2310e-02, -1.3488e-02, -1.7672e-03, -1.4419e-02, -2.3934e-02,\n",
      "         1.5829e-02, -5.3236e-03, -7.4347e-03,  1.1586e-02,  6.0084e-03,\n",
      "        -2.1920e-02, -1.9077e-02, -7.4969e-03,  1.6121e-02,  1.0410e-02,\n",
      "         2.3975e-02, -1.4826e-02, -1.6612e-02, -2.3349e-02, -2.4417e-02,\n",
      "        -1.9848e-02, -1.8326e-02,  5.6297e-03,  1.1453e-02, -1.2627e-02,\n",
      "        -1.5074e-02,  1.7818e-02, -3.1591e-03,  7.5618e-03, -1.1850e-02,\n",
      "        -4.1838e-03,  6.3021e-03, -6.1442e-03,  2.3457e-02,  1.2740e-03,\n",
      "         2.2981e-02, -1.0855e-02, -4.3092e-03,  1.1810e-02, -8.3259e-03,\n",
      "        -3.9146e-03, -1.2573e-02,  1.5834e-02,  9.4528e-03, -1.8372e-02,\n",
      "        -2.1199e-02, -1.5331e-02,  1.7068e-02,  9.4079e-03,  1.3009e-02,\n",
      "         2.3666e-02, -1.4001e-02,  1.2869e-02,  2.4390e-02, -1.5795e-02,\n",
      "         5.9329e-03,  1.8077e-02, -1.0555e-02,  2.1662e-02, -2.0120e-02,\n",
      "        -2.1609e-02,  1.9042e-02, -2.3395e-02,  2.4381e-02,  2.2200e-02,\n",
      "        -2.0249e-02,  1.7748e-02, -1.3036e-02,  1.6992e-02, -1.9883e-02,\n",
      "         1.1590e-02, -4.8189e-03,  1.2870e-02, -6.4524e-03, -4.9454e-03,\n",
      "         1.0837e-02,  7.3324e-03, -2.2503e-02, -9.9758e-03, -8.7804e-03,\n",
      "        -1.0357e-02,  1.3342e-02, -1.3120e-02,  3.7234e-03,  4.2344e-03,\n",
      "         2.1331e-02, -1.2730e-02, -1.2791e-02, -7.0736e-03, -7.7362e-03,\n",
      "        -5.5163e-03, -1.8410e-02,  3.3145e-03,  2.8616e-03,  6.9308e-03,\n",
      "        -9.4160e-03,  1.4625e-02,  1.5531e-02, -1.0811e-02,  6.0196e-03,\n",
      "        -1.7629e-02,  6.9330e-03, -1.0994e-02,  7.1032e-03,  1.5168e-02,\n",
      "         2.1181e-03,  1.5921e-02,  1.1848e-02, -1.8158e-03, -1.0594e-02,\n",
      "        -1.1517e-02, -8.7976e-03, -2.1316e-02, -2.2400e-02, -5.4086e-03,\n",
      "         2.5225e-02, -2.1749e-02, -2.2903e-02, -1.8272e-02,  1.3550e-02,\n",
      "         9.8268e-03, -7.9960e-03, -1.7298e-02, -9.1196e-03,  1.8158e-03,\n",
      "         3.8025e-04, -1.1207e-02,  1.6647e-02,  4.7232e-03,  2.3647e-02,\n",
      "         7.4565e-03,  1.3600e-02,  1.8818e-02,  2.3631e-02, -1.7379e-04,\n",
      "        -3.0325e-03,  5.1445e-03, -1.0038e-03, -4.0806e-03, -5.5836e-03,\n",
      "        -1.9364e-02,  1.8778e-02, -2.2265e-02,  2.3607e-02, -2.1830e-02,\n",
      "        -4.1851e-03, -1.5655e-02, -1.8311e-02, -6.2364e-03,  2.4089e-02,\n",
      "         5.7090e-03,  1.4981e-02,  2.0540e-02,  1.7615e-02, -1.6622e-02,\n",
      "        -9.8137e-03,  2.1210e-02, -1.1946e-02,  1.3002e-02, -5.9081e-03,\n",
      "         1.3448e-02, -1.0701e-03,  2.5743e-03, -2.5253e-03, -1.1943e-02,\n",
      "         1.1693e-02,  2.1095e-03, -3.3514e-03, -2.3937e-02,  7.1421e-03,\n",
      "         1.3626e-02, -2.0203e-02,  2.1651e-02, -2.4563e-02,  1.7961e-02,\n",
      "         6.0605e-03, -1.1484e-02,  1.8270e-02,  3.2907e-03,  5.4453e-04,\n",
      "         1.5216e-02,  5.3017e-04, -1.4808e-02, -8.1873e-03, -1.6841e-02,\n",
      "        -5.6128e-03, -1.1403e-02, -8.3521e-03,  1.4237e-02, -1.4347e-02,\n",
      "        -2.3334e-02,  7.6320e-03,  1.2688e-02,  1.0352e-02,  9.0593e-03,\n",
      "        -1.9514e-03, -1.1718e-02, -1.6592e-02, -2.3819e-02, -1.7259e-02,\n",
      "        -1.9109e-02, -2.1837e-02, -2.0592e-02,  1.1522e-02,  4.6929e-03,\n",
      "        -1.1583e-02,  2.5253e-02, -2.0703e-02,  2.5299e-02, -1.6790e-02,\n",
      "         2.3664e-02, -2.2991e-02,  2.3133e-02,  9.0896e-03, -2.3486e-02,\n",
      "        -3.9345e-03,  1.9670e-02,  3.6238e-04,  1.6976e-02, -2.2438e-02,\n",
      "         1.5468e-03,  1.3040e-02,  2.3645e-02, -1.5264e-03,  1.0176e-02,\n",
      "         2.5001e-02, -3.9051e-03,  1.7355e-02,  1.7069e-02, -1.1041e-02,\n",
      "        -3.0521e-03,  2.3229e-02,  2.1068e-02,  1.5370e-02, -1.3401e-02,\n",
      "         2.3085e-02,  1.5857e-02,  7.6602e-04], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-2.1802e-01],\n",
      "        [-2.1021e-01],\n",
      "        [ 2.3838e-01],\n",
      "        [-9.3008e-01],\n",
      "        [-8.9063e-01],\n",
      "        [ 1.6496e-01],\n",
      "        [ 3.7483e-02],\n",
      "        [-3.9625e-01],\n",
      "        [-4.3148e-01],\n",
      "        [ 7.9325e-01],\n",
      "        [-3.3081e-01],\n",
      "        [ 2.3754e-01],\n",
      "        [ 1.3914e-01],\n",
      "        [-1.7132e-01],\n",
      "        [-9.5669e-01],\n",
      "        [-9.2892e-01],\n",
      "        [-3.2155e-01],\n",
      "        [-9.7787e-02],\n",
      "        [-3.8742e-01],\n",
      "        [-6.7730e-01],\n",
      "        [-6.9914e-01],\n",
      "        [-4.3513e-01],\n",
      "        [ 9.9046e-01],\n",
      "        [ 4.1362e-01],\n",
      "        [-4.7568e-01],\n",
      "        [ 2.1435e-01],\n",
      "        [ 9.3136e-01],\n",
      "        [-3.6225e-01],\n",
      "        [-7.5142e-01],\n",
      "        [-8.6068e-01],\n",
      "        [ 1.6883e-01],\n",
      "        [-1.8514e-01],\n",
      "        [ 2.1396e-01],\n",
      "        [ 1.6313e-01],\n",
      "        [-7.3284e-01],\n",
      "        [ 3.3963e-01],\n",
      "        [-7.6516e-01],\n",
      "        [ 5.4806e-01],\n",
      "        [-2.7621e-01],\n",
      "        [ 8.8942e-01],\n",
      "        [-9.8071e-01],\n",
      "        [ 2.8214e-01],\n",
      "        [ 2.4520e-01],\n",
      "        [ 5.9527e-02],\n",
      "        [ 4.3131e-01],\n",
      "        [-8.9818e-01],\n",
      "        [-8.8977e-01],\n",
      "        [ 1.4477e-01],\n",
      "        [-2.3513e-01],\n",
      "        [-4.8921e-01],\n",
      "        [ 3.4474e-01],\n",
      "        [ 9.4648e-01],\n",
      "        [ 8.5216e-01],\n",
      "        [-5.7148e-01],\n",
      "        [ 8.1035e-01],\n",
      "        [ 4.3262e-01],\n",
      "        [ 1.0050e-01],\n",
      "        [ 6.4723e-01],\n",
      "        [ 2.6324e-02],\n",
      "        [-6.0533e-01],\n",
      "        [-3.8663e-01],\n",
      "        [ 6.5021e-01],\n",
      "        [-4.3979e-01],\n",
      "        [-7.9546e-01],\n",
      "        [ 4.9198e-01],\n",
      "        [ 6.6617e-01],\n",
      "        [-9.2655e-01],\n",
      "        [ 5.8217e-01],\n",
      "        [-8.4991e-02],\n",
      "        [ 7.3177e-01],\n",
      "        [ 9.5655e-02],\n",
      "        [ 8.8995e-01],\n",
      "        [ 8.1646e-01],\n",
      "        [ 7.9564e-01],\n",
      "        [-9.1360e-01],\n",
      "        [ 8.7482e-01],\n",
      "        [-4.0350e-01],\n",
      "        [-9.4184e-01],\n",
      "        [ 2.3739e-01],\n",
      "        [-7.0972e-01],\n",
      "        [ 8.5177e-02],\n",
      "        [ 1.9937e-01],\n",
      "        [-5.7736e-01],\n",
      "        [-3.6966e-01],\n",
      "        [-2.1833e-01],\n",
      "        [-1.5247e-01],\n",
      "        [-8.7095e-01],\n",
      "        [ 4.3569e-01],\n",
      "        [-2.2716e-01],\n",
      "        [ 1.1951e-01],\n",
      "        [-3.0727e-01],\n",
      "        [-6.9677e-02],\n",
      "        [-3.1635e-01],\n",
      "        [-6.1783e-01],\n",
      "        [-7.4541e-01],\n",
      "        [-7.9407e-01],\n",
      "        [ 6.1923e-01],\n",
      "        [ 3.6550e-01],\n",
      "        [-5.2699e-01],\n",
      "        [ 7.2312e-01],\n",
      "        [ 5.2384e-01],\n",
      "        [ 6.3225e-01],\n",
      "        [-8.2528e-01],\n",
      "        [ 5.5151e-01],\n",
      "        [ 8.6904e-01],\n",
      "        [-4.1312e-01],\n",
      "        [ 2.2619e-01],\n",
      "        [-3.2611e-01],\n",
      "        [-9.8962e-01],\n",
      "        [ 7.6775e-01],\n",
      "        [ 6.9082e-01],\n",
      "        [-2.0889e-01],\n",
      "        [ 1.9994e-01],\n",
      "        [ 3.2121e-01],\n",
      "        [-6.0619e-01],\n",
      "        [ 3.9897e-01],\n",
      "        [ 4.3332e-01],\n",
      "        [ 8.5187e-01],\n",
      "        [-9.1125e-01],\n",
      "        [ 1.9968e-01],\n",
      "        [-1.3328e-01],\n",
      "        [ 4.0966e-01],\n",
      "        [ 4.1395e-01],\n",
      "        [ 6.2852e-01],\n",
      "        [ 6.5900e-01],\n",
      "        [-3.9332e-01],\n",
      "        [-3.8520e-01],\n",
      "        [ 3.2187e-01],\n",
      "        [ 1.9122e-01],\n",
      "        [ 5.4639e-02],\n",
      "        [-9.1450e-02],\n",
      "        [-6.1182e-01],\n",
      "        [-2.1601e-01],\n",
      "        [-4.4018e-02],\n",
      "        [-4.7207e-01],\n",
      "        [ 7.0338e-02],\n",
      "        [ 7.7488e-01],\n",
      "        [ 7.3345e-01],\n",
      "        [-8.8175e-01],\n",
      "        [ 9.7704e-01],\n",
      "        [-5.6088e-01],\n",
      "        [-9.2493e-02],\n",
      "        [ 2.2004e-01],\n",
      "        [ 2.1464e-01],\n",
      "        [ 8.0596e-01],\n",
      "        [-1.6315e-01],\n",
      "        [-3.1864e-02],\n",
      "        [-4.8539e-01],\n",
      "        [ 6.2790e-01],\n",
      "        [ 9.9301e-02],\n",
      "        [-9.7056e-01],\n",
      "        [-2.6246e-01],\n",
      "        [ 2.2377e-01],\n",
      "        [ 7.1158e-01],\n",
      "        [-5.7352e-01],\n",
      "        [ 7.6847e-01],\n",
      "        [-5.5332e-01],\n",
      "        [-7.2841e-01],\n",
      "        [-2.5852e-01],\n",
      "        [ 7.7481e-01],\n",
      "        [-2.0676e-01],\n",
      "        [-6.1853e-01],\n",
      "        [ 8.0209e-01],\n",
      "        [-9.0051e-01],\n",
      "        [-2.3460e-01],\n",
      "        [-1.4684e-01],\n",
      "        [ 8.0735e-01],\n",
      "        [-4.3518e-01],\n",
      "        [ 9.8129e-01],\n",
      "        [-7.0816e-01],\n",
      "        [-2.0076e-01],\n",
      "        [-5.0770e-01],\n",
      "        [ 1.3648e-01],\n",
      "        [-4.1114e-01],\n",
      "        [-5.9820e-01],\n",
      "        [ 8.3747e-01],\n",
      "        [-2.0590e-01],\n",
      "        [-8.1142e-01],\n",
      "        [ 8.7377e-01],\n",
      "        [-2.0323e-01],\n",
      "        [ 2.7945e-01],\n",
      "        [-9.7672e-01],\n",
      "        [-5.3585e-01],\n",
      "        [ 1.4674e-01],\n",
      "        [-2.0260e-02],\n",
      "        [-9.9362e-02],\n",
      "        [ 9.2905e-01],\n",
      "        [-7.1154e-01],\n",
      "        [-4.1957e-01],\n",
      "        [ 8.6974e-01],\n",
      "        [ 6.3804e-01],\n",
      "        [ 6.3912e-01],\n",
      "        [ 9.4297e-01],\n",
      "        [ 3.0077e-01],\n",
      "        [ 4.2900e-01],\n",
      "        [-5.2957e-01],\n",
      "        [-8.6185e-01],\n",
      "        [ 2.7123e-01],\n",
      "        [ 6.4267e-01],\n",
      "        [-5.0895e-01],\n",
      "        [ 2.9489e-01],\n",
      "        [ 5.3216e-01],\n",
      "        [-9.1948e-01],\n",
      "        [-9.7426e-01],\n",
      "        [-5.3094e-01],\n",
      "        [-4.5897e-01],\n",
      "        [-7.9451e-01],\n",
      "        [-5.6467e-01],\n",
      "        [-3.2914e-02],\n",
      "        [-3.9990e-01],\n",
      "        [ 8.5053e-01],\n",
      "        [ 9.3560e-01],\n",
      "        [ 4.6723e-01],\n",
      "        [ 4.8400e-01],\n",
      "        [-4.6327e-02],\n",
      "        [ 2.6965e-02],\n",
      "        [ 1.8709e-01],\n",
      "        [-9.7753e-01],\n",
      "        [ 8.7243e-01],\n",
      "        [-8.5625e-01],\n",
      "        [ 6.2264e-02],\n",
      "        [ 5.1622e-01],\n",
      "        [ 5.4682e-01],\n",
      "        [ 9.3704e-01],\n",
      "        [ 5.5989e-01],\n",
      "        [-2.6462e-01],\n",
      "        [-6.0998e-02],\n",
      "        [-3.4204e-01],\n",
      "        [-2.2910e-01],\n",
      "        [-9.7505e-01],\n",
      "        [-6.3532e-01],\n",
      "        [-5.1695e-01],\n",
      "        [-9.5591e-01],\n",
      "        [ 9.5232e-01],\n",
      "        [-4.2144e-01],\n",
      "        [-8.7282e-01],\n",
      "        [-8.0624e-01],\n",
      "        [ 7.9488e-01],\n",
      "        [ 8.8402e-01],\n",
      "        [-6.3749e-01],\n",
      "        [-9.5484e-01],\n",
      "        [ 2.4475e-01],\n",
      "        [ 6.1293e-01],\n",
      "        [ 2.7737e-01],\n",
      "        [ 4.0504e-02],\n",
      "        [-9.5656e-01],\n",
      "        [ 5.1942e-01],\n",
      "        [-7.4954e-01],\n",
      "        [-7.8061e-01],\n",
      "        [ 3.6334e-01],\n",
      "        [-2.1559e-01],\n",
      "        [-8.1405e-01],\n",
      "        [-1.7710e-01],\n",
      "        [ 5.9774e-01],\n",
      "        [-6.9219e-01],\n",
      "        [-7.3036e-01],\n",
      "        [ 9.5516e-01],\n",
      "        [-4.2921e-01],\n",
      "        [-3.1051e-01],\n",
      "        [-6.1438e-01],\n",
      "        [-3.2732e-01],\n",
      "        [-4.3965e-01],\n",
      "        [-1.4685e-01],\n",
      "        [-3.2220e-01],\n",
      "        [ 8.0449e-01],\n",
      "        [-3.8241e-01],\n",
      "        [ 1.7467e-01],\n",
      "        [ 4.7015e-01],\n",
      "        [ 2.2901e-01],\n",
      "        [ 1.2672e-01],\n",
      "        [-4.9525e-01],\n",
      "        [ 6.0454e-01],\n",
      "        [-7.4815e-01],\n",
      "        [ 1.1005e-01],\n",
      "        [-8.1632e-01],\n",
      "        [-8.5825e-01],\n",
      "        [ 3.7308e-01],\n",
      "        [-5.4816e-01],\n",
      "        [-4.2298e-01],\n",
      "        [ 5.4505e-01],\n",
      "        [-8.5696e-01],\n",
      "        [ 4.6416e-01],\n",
      "        [ 2.2316e-01],\n",
      "        [ 4.1262e-01],\n",
      "        [ 3.0057e-01],\n",
      "        [-7.3867e-01],\n",
      "        [-8.2128e-01],\n",
      "        [ 3.0387e-01],\n",
      "        [-7.5212e-01],\n",
      "        [-6.0540e-01],\n",
      "        [-9.7289e-01],\n",
      "        [-5.2639e-01],\n",
      "        [ 5.3907e-01],\n",
      "        [-7.6570e-01],\n",
      "        [-7.2420e-01],\n",
      "        [ 4.4485e-01],\n",
      "        [ 2.5954e-01],\n",
      "        [-1.8244e-01],\n",
      "        [ 2.2711e-01],\n",
      "        [ 7.8211e-01],\n",
      "        [ 6.3179e-01],\n",
      "        [-9.3973e-01],\n",
      "        [ 2.7724e-01],\n",
      "        [-7.1846e-01],\n",
      "        [ 4.3823e-01],\n",
      "        [-1.7467e-01],\n",
      "        [-9.9977e-01],\n",
      "        [-5.3954e-01],\n",
      "        [-1.7368e-01],\n",
      "        [-9.0901e-01],\n",
      "        [ 6.4731e-02],\n",
      "        [-9.9773e-02],\n",
      "        [-8.0905e-01],\n",
      "        [ 7.4055e-01],\n",
      "        [-3.0357e-01],\n",
      "        [-6.4082e-01],\n",
      "        [-7.4579e-01],\n",
      "        [ 2.6322e-01],\n",
      "        [-1.7393e-02],\n",
      "        [ 6.3889e-01],\n",
      "        [-4.0472e-01],\n",
      "        [-2.3681e-01],\n",
      "        [ 9.3042e-01],\n",
      "        [-7.4040e-01],\n",
      "        [-6.8650e-01],\n",
      "        [ 6.9187e-02],\n",
      "        [-6.6691e-01],\n",
      "        [-4.4418e-01],\n",
      "        [-2.4966e-02],\n",
      "        [-9.5246e-02],\n",
      "        [ 1.9371e-02],\n",
      "        [ 5.3356e-01],\n",
      "        [ 5.8622e-01],\n",
      "        [-2.2481e-01],\n",
      "        [-5.6780e-01],\n",
      "        [-9.6281e-01],\n",
      "        [-6.7839e-02],\n",
      "        [ 7.2533e-01],\n",
      "        [ 7.4285e-01],\n",
      "        [-6.6561e-01],\n",
      "        [ 2.8175e-01],\n",
      "        [ 9.2675e-01],\n",
      "        [ 1.1436e-01],\n",
      "        [ 5.2486e-01],\n",
      "        [-3.3040e-01],\n",
      "        [ 3.9764e-01],\n",
      "        [-7.4856e-01],\n",
      "        [ 4.4510e-01],\n",
      "        [ 3.7946e-01],\n",
      "        [ 7.4488e-01],\n",
      "        [-5.9823e-01],\n",
      "        [ 5.3443e-01],\n",
      "        [ 4.4635e-01],\n",
      "        [ 3.4094e-01],\n",
      "        [-8.2212e-02],\n",
      "        [ 9.5145e-01],\n",
      "        [-7.5636e-01],\n",
      "        [-7.2861e-01],\n",
      "        [ 8.0696e-01],\n",
      "        [-5.0586e-01],\n",
      "        [ 8.3517e-01],\n",
      "        [-1.5109e-01],\n",
      "        [ 4.5940e-01],\n",
      "        [-2.9661e-02],\n",
      "        [-6.1255e-01],\n",
      "        [ 5.4515e-01],\n",
      "        [-4.7361e-01],\n",
      "        [-4.5864e-01],\n",
      "        [ 1.0664e-01],\n",
      "        [-9.2671e-01],\n",
      "        [ 9.0002e-01],\n",
      "        [-5.1239e-01],\n",
      "        [-3.7262e-03],\n",
      "        [ 3.1893e-01],\n",
      "        [-4.6990e-01],\n",
      "        [-3.1189e-01],\n",
      "        [ 6.2954e-01],\n",
      "        [ 5.2301e-01],\n",
      "        [ 5.3028e-01],\n",
      "        [-9.9122e-01],\n",
      "        [-9.2204e-01],\n",
      "        [ 8.5078e-01],\n",
      "        [-3.5693e-01],\n",
      "        [-9.6452e-01],\n",
      "        [ 5.7267e-01],\n",
      "        [ 1.2780e-01],\n",
      "        [-6.5568e-01],\n",
      "        [ 2.2947e-01],\n",
      "        [ 9.8961e-01],\n",
      "        [-7.2476e-01],\n",
      "        [-7.0151e-01],\n",
      "        [ 5.4279e-01],\n",
      "        [ 8.7554e-01],\n",
      "        [ 2.8465e-01],\n",
      "        [-1.1707e-01],\n",
      "        [-1.2704e-01],\n",
      "        [-6.5960e-01],\n",
      "        [-8.4691e-01],\n",
      "        [ 2.0381e-01],\n",
      "        [ 1.3297e-01],\n",
      "        [ 3.8985e-01],\n",
      "        [ 9.1614e-01],\n",
      "        [-8.7490e-01],\n",
      "        [ 7.1297e-01],\n",
      "        [-2.2902e-02],\n",
      "        [-8.6057e-01],\n",
      "        [-7.0490e-01],\n",
      "        [-6.8002e-01],\n",
      "        [ 3.3028e-01],\n",
      "        [ 4.7723e-01],\n",
      "        [-7.3839e-01],\n",
      "        [-6.2883e-01],\n",
      "        [-1.8399e-01],\n",
      "        [-8.5367e-01],\n",
      "        [-3.4717e-01],\n",
      "        [ 6.9989e-01],\n",
      "        [-8.9916e-01],\n",
      "        [ 8.1810e-01],\n",
      "        [ 8.8985e-01],\n",
      "        [ 9.2003e-01],\n",
      "        [ 7.1649e-01],\n",
      "        [-6.9195e-01],\n",
      "        [ 8.2227e-01],\n",
      "        [-2.4729e-01],\n",
      "        [-8.6751e-01],\n",
      "        [-1.5655e-01],\n",
      "        [-2.0585e-01],\n",
      "        [ 5.8945e-01],\n",
      "        [-8.1670e-01],\n",
      "        [-4.0679e-01],\n",
      "        [ 3.5498e-02],\n",
      "        [ 9.1475e-01],\n",
      "        [-8.4596e-01],\n",
      "        [-4.1221e-01],\n",
      "        [-9.7943e-01],\n",
      "        [ 4.1986e-01],\n",
      "        [ 1.1466e-01],\n",
      "        [-5.4473e-01],\n",
      "        [ 8.3375e-01],\n",
      "        [ 9.5846e-01],\n",
      "        [-8.4322e-01],\n",
      "        [-1.8325e-01],\n",
      "        [ 7.4798e-01],\n",
      "        [-9.1424e-01],\n",
      "        [-3.0403e-01],\n",
      "        [-4.9517e-01],\n",
      "        [ 3.3933e-01],\n",
      "        [-1.8802e-01],\n",
      "        [ 8.7185e-01],\n",
      "        [-9.8512e-02],\n",
      "        [ 5.3099e-01],\n",
      "        [-1.8444e-02],\n",
      "        [ 6.8286e-01],\n",
      "        [-4.4757e-01],\n",
      "        [-6.2704e-02],\n",
      "        [ 2.6674e-01],\n",
      "        [-6.9793e-01],\n",
      "        [-6.4784e-02],\n",
      "        [ 5.8884e-01],\n",
      "        [ 4.9119e-01],\n",
      "        [ 2.7380e-01],\n",
      "        [ 1.7234e-01],\n",
      "        [ 6.0090e-01],\n",
      "        [ 5.5250e-01],\n",
      "        [ 5.0894e-01],\n",
      "        [-5.0130e-01],\n",
      "        [ 9.4570e-01],\n",
      "        [-6.4914e-01],\n",
      "        [ 9.0130e-01],\n",
      "        [-7.8182e-01],\n",
      "        [ 6.3463e-01],\n",
      "        [ 7.6630e-01],\n",
      "        [-1.8335e-01],\n",
      "        [-5.2724e-02],\n",
      "        [-3.2449e-01],\n",
      "        [-9.5388e-01],\n",
      "        [-5.1302e-01],\n",
      "        [-1.1322e-01],\n",
      "        [-2.6091e-01],\n",
      "        [ 9.0298e-02],\n",
      "        [ 9.7246e-01],\n",
      "        [-8.7621e-02],\n",
      "        [-5.4318e-01],\n",
      "        [-5.0163e-01],\n",
      "        [-9.3001e-01],\n",
      "        [ 1.3036e-01],\n",
      "        [-4.2342e-01],\n",
      "        [ 3.0966e-01],\n",
      "        [-3.6114e-01],\n",
      "        [ 1.0278e-01],\n",
      "        [ 9.0727e-01],\n",
      "        [ 9.8592e-01],\n",
      "        [-4.4823e-01],\n",
      "        [ 1.0281e-01],\n",
      "        [-6.4450e-01],\n",
      "        [-9.2981e-01],\n",
      "        [-8.4958e-01],\n",
      "        [-4.5004e-01],\n",
      "        [-7.2695e-01],\n",
      "        [ 7.3692e-01],\n",
      "        [ 4.9579e-01],\n",
      "        [ 5.8558e-01],\n",
      "        [ 1.7279e-01],\n",
      "        [-3.4386e-01],\n",
      "        [ 4.6556e-01],\n",
      "        [-2.1989e-01],\n",
      "        [ 5.3668e-01],\n",
      "        [-8.6188e-01],\n",
      "        [ 7.3130e-01],\n",
      "        [ 5.9766e-03],\n",
      "        [-4.4785e-01],\n",
      "        [-5.8398e-01],\n",
      "        [-2.5650e-01],\n",
      "        [ 4.0538e-01],\n",
      "        [-9.3094e-01],\n",
      "        [ 5.8108e-01],\n",
      "        [ 2.5437e-01],\n",
      "        [ 1.4874e-01],\n",
      "        [ 3.1983e-01],\n",
      "        [-9.0517e-01],\n",
      "        [ 2.6296e-01],\n",
      "        [ 2.9451e-01],\n",
      "        [ 8.7697e-01],\n",
      "        [ 3.5763e-01],\n",
      "        [ 8.3142e-01],\n",
      "        [-7.8096e-01],\n",
      "        [-7.1624e-01],\n",
      "        [-5.8413e-01],\n",
      "        [ 5.2916e-01],\n",
      "        [ 7.7101e-01],\n",
      "        [-7.5184e-01],\n",
      "        [-4.0840e-01],\n",
      "        [-4.7491e-01],\n",
      "        [-8.0627e-01],\n",
      "        [-4.3471e-01],\n",
      "        [-2.6504e-01],\n",
      "        [ 8.2150e-01],\n",
      "        [-6.1905e-01],\n",
      "        [-6.5601e-01],\n",
      "        [ 3.0606e-01],\n",
      "        [-3.5092e-01],\n",
      "        [-7.4948e-01],\n",
      "        [ 9.7719e-01],\n",
      "        [-3.4213e-01],\n",
      "        [-6.1295e-01],\n",
      "        [ 7.4784e-01],\n",
      "        [-6.5625e-01],\n",
      "        [ 7.2285e-01],\n",
      "        [-6.8035e-01],\n",
      "        [ 8.2249e-01],\n",
      "        [-8.2299e-01],\n",
      "        [-3.5020e-01],\n",
      "        [-3.1685e-02],\n",
      "        [-3.1003e-01],\n",
      "        [ 2.7240e-01],\n",
      "        [ 5.6755e-01],\n",
      "        [-9.8198e-01],\n",
      "        [-9.5844e-01],\n",
      "        [ 9.3183e-01],\n",
      "        [ 6.2121e-01],\n",
      "        [ 4.9936e-01],\n",
      "        [ 2.7595e-01],\n",
      "        [ 9.7117e-01],\n",
      "        [-4.8013e-01],\n",
      "        [-4.8932e-01],\n",
      "        [-9.5329e-01],\n",
      "        [ 4.7643e-01],\n",
      "        [ 3.2949e-01],\n",
      "        [ 1.6406e-01],\n",
      "        [ 5.4129e-01],\n",
      "        [ 8.5151e-01],\n",
      "        [-5.5086e-01],\n",
      "        [ 9.4890e-01],\n",
      "        [-2.1805e-01],\n",
      "        [ 9.9768e-01],\n",
      "        [ 2.2100e-01],\n",
      "        [-6.9484e-01],\n",
      "        [-7.0547e-02],\n",
      "        [-6.5223e-01],\n",
      "        [ 6.5621e-01],\n",
      "        [ 1.5058e-02],\n",
      "        [-1.0998e-01],\n",
      "        [ 6.5417e-01],\n",
      "        [-3.3126e-01],\n",
      "        [-4.3227e-01],\n",
      "        [-9.4080e-01],\n",
      "        [-3.4546e-01],\n",
      "        [ 2.0053e-01],\n",
      "        [ 3.4697e-01],\n",
      "        [ 2.2212e-01],\n",
      "        [ 4.0651e-01],\n",
      "        [-1.0251e-02],\n",
      "        [-7.4529e-01],\n",
      "        [ 2.6870e-01],\n",
      "        [ 6.7866e-01],\n",
      "        [ 3.2196e-01],\n",
      "        [-4.6444e-01],\n",
      "        [-8.7441e-01],\n",
      "        [ 4.0920e-01],\n",
      "        [-9.2483e-01],\n",
      "        [ 1.0606e-01],\n",
      "        [-4.4706e-01],\n",
      "        [ 6.8110e-02],\n",
      "        [ 7.4201e-01],\n",
      "        [ 4.1435e-01],\n",
      "        [-7.5817e-05],\n",
      "        [-9.9302e-01],\n",
      "        [-4.7696e-01],\n",
      "        [-4.9212e-01],\n",
      "        [-2.5519e-01],\n",
      "        [ 6.7207e-01],\n",
      "        [ 6.2928e-01],\n",
      "        [-6.6681e-01],\n",
      "        [-4.2563e-01],\n",
      "        [ 3.1252e-02],\n",
      "        [ 1.0627e-01],\n",
      "        [ 9.4508e-01],\n",
      "        [-6.8753e-01],\n",
      "        [-4.7379e-01],\n",
      "        [ 9.8807e-01],\n",
      "        [ 4.7297e-01],\n",
      "        [ 9.4574e-02],\n",
      "        [-3.2658e-01],\n",
      "        [-7.5531e-04],\n",
      "        [ 1.5444e-01],\n",
      "        [ 4.0981e-01],\n",
      "        [ 2.5218e-01],\n",
      "        [-7.8350e-01],\n",
      "        [-8.2163e-01],\n",
      "        [ 8.2004e-01],\n",
      "        [-3.9054e-02],\n",
      "        [-1.6331e-01],\n",
      "        [ 2.3154e-01],\n",
      "        [ 9.8310e-01],\n",
      "        [-7.7343e-01],\n",
      "        [ 9.0650e-01],\n",
      "        [-7.2491e-01],\n",
      "        [ 5.3182e-01],\n",
      "        [-7.1448e-01],\n",
      "        [-1.9339e-01],\n",
      "        [-3.0270e-01],\n",
      "        [ 7.2924e-02],\n",
      "        [ 6.1227e-01],\n",
      "        [-7.9046e-01],\n",
      "        [-1.1553e-01],\n",
      "        [-7.5419e-01],\n",
      "        [ 6.8965e-01],\n",
      "        [-5.3408e-01],\n",
      "        [ 9.1614e-01],\n",
      "        [-6.0656e-01],\n",
      "        [ 2.8344e-01],\n",
      "        [ 4.4265e-02],\n",
      "        [-4.8708e-02],\n",
      "        [ 1.8596e-01],\n",
      "        [-4.9956e-01],\n",
      "        [-9.9737e-01],\n",
      "        [-6.8762e-02],\n",
      "        [-7.5533e-03],\n",
      "        [ 9.2025e-01],\n",
      "        [-8.4803e-01],\n",
      "        [ 9.0739e-01],\n",
      "        [-6.1916e-01],\n",
      "        [-9.8973e-01],\n",
      "        [ 2.5580e-01],\n",
      "        [-5.5723e-01],\n",
      "        [-6.0381e-01],\n",
      "        [-5.3508e-01],\n",
      "        [-3.7840e-01],\n",
      "        [-9.3578e-02],\n",
      "        [-2.4893e-01],\n",
      "        [ 4.8330e-01],\n",
      "        [ 7.6345e-01],\n",
      "        [-5.4664e-01],\n",
      "        [ 2.7421e-01],\n",
      "        [-5.2061e-02],\n",
      "        [-2.9925e-01],\n",
      "        [-8.4068e-01],\n",
      "        [ 8.7908e-01],\n",
      "        [-7.8712e-01],\n",
      "        [ 9.8690e-01],\n",
      "        [ 5.3272e-02],\n",
      "        [-8.6350e-01],\n",
      "        [ 3.2572e-02],\n",
      "        [ 6.5079e-01],\n",
      "        [-7.0537e-01],\n",
      "        [-4.0891e-01],\n",
      "        [ 6.5887e-01],\n",
      "        [-3.5144e-01],\n",
      "        [-6.6788e-01],\n",
      "        [-5.3130e-01],\n",
      "        [ 1.4843e-02],\n",
      "        [-2.7470e-01],\n",
      "        [ 6.6899e-01],\n",
      "        [ 4.4933e-01],\n",
      "        [ 9.9927e-01],\n",
      "        [-3.5989e-01],\n",
      "        [-8.0809e-02],\n",
      "        [ 2.7879e-01],\n",
      "        [ 7.2485e-01],\n",
      "        [ 8.7941e-01],\n",
      "        [-7.4067e-01],\n",
      "        [-6.2704e-01],\n",
      "        [ 7.9039e-01],\n",
      "        [-2.7941e-01],\n",
      "        [ 7.5582e-01],\n",
      "        [-9.8801e-01],\n",
      "        [-5.7303e-01],\n",
      "        [-3.4817e-01],\n",
      "        [ 1.4268e-01],\n",
      "        [ 2.3580e-01],\n",
      "        [ 3.3094e-01],\n",
      "        [-8.9819e-01],\n",
      "        [ 1.6790e-01],\n",
      "        [ 9.5512e-01],\n",
      "        [ 8.8928e-01],\n",
      "        [ 6.3309e-01],\n",
      "        [-2.9577e-01],\n",
      "        [ 1.7190e-01],\n",
      "        [-9.8456e-01],\n",
      "        [ 8.5540e-01],\n",
      "        [-4.6298e-01],\n",
      "        [-6.3907e-01],\n",
      "        [ 8.2968e-01],\n",
      "        [ 2.4601e-01],\n",
      "        [-6.7467e-01],\n",
      "        [-8.1895e-01],\n",
      "        [-7.9988e-01],\n",
      "        [-2.1849e-01],\n",
      "        [ 8.0186e-01],\n",
      "        [ 8.9287e-01],\n",
      "        [-3.3599e-01],\n",
      "        [-6.9223e-01],\n",
      "        [-4.1777e-02],\n",
      "        [-8.2039e-01],\n",
      "        [-4.9392e-01],\n",
      "        [-4.5460e-01],\n",
      "        [ 8.3982e-01],\n",
      "        [ 2.4228e-01],\n",
      "        [-5.0314e-01],\n",
      "        [-9.4634e-01],\n",
      "        [ 7.5493e-02],\n",
      "        [-5.1145e-02],\n",
      "        [-2.0821e-01],\n",
      "        [-6.4073e-01],\n",
      "        [-9.6161e-01],\n",
      "        [-2.6526e-02],\n",
      "        [ 6.0366e-02],\n",
      "        [-4.3847e-01],\n",
      "        [ 2.8932e-01],\n",
      "        [ 4.7819e-01],\n",
      "        [ 9.0701e-01],\n",
      "        [-9.5860e-01],\n",
      "        [-9.1743e-02],\n",
      "        [-4.6999e-01],\n",
      "        [ 7.9896e-01],\n",
      "        [ 3.2039e-01],\n",
      "        [ 7.2705e-01],\n",
      "        [-7.0018e-01],\n",
      "        [ 3.3187e-01],\n",
      "        [-8.7002e-01],\n",
      "        [ 2.1250e-01],\n",
      "        [ 4.0207e-01],\n",
      "        [-3.7095e-01],\n",
      "        [ 3.9738e-01],\n",
      "        [-5.8990e-01],\n",
      "        [-6.8233e-01],\n",
      "        [-2.5791e-01],\n",
      "        [-7.7378e-01]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([ 3.6288e-01, -3.9177e-01, -1.8402e-02, -5.8816e-01,  3.4628e-01,\n",
      "         2.5228e-01, -1.6248e-01,  2.0617e-01, -1.9075e-01,  5.2431e-01,\n",
      "        -3.1155e-01,  6.2712e-01, -6.9188e-01, -7.0272e-01,  3.3992e-01,\n",
      "        -4.2135e-01,  9.3301e-01, -5.3128e-01,  1.0881e-01,  9.0764e-01,\n",
      "        -4.5207e-01,  9.3008e-01,  5.6154e-01, -7.2690e-01,  4.3580e-01,\n",
      "        -8.7600e-01, -7.7552e-01, -8.1912e-01, -2.7850e-01,  5.6541e-01,\n",
      "         2.5797e-01,  6.8557e-01, -8.5712e-01, -1.1866e-01, -3.9811e-01,\n",
      "        -6.8785e-01,  9.6452e-01,  8.9973e-03, -4.4782e-01,  5.8267e-01,\n",
      "        -3.5839e-01,  1.3286e-01, -7.3133e-01, -1.7226e-01,  7.8814e-01,\n",
      "         7.9309e-01, -2.0102e-02,  3.4577e-01,  1.6046e-01,  7.2362e-01,\n",
      "         1.7781e-01,  4.2453e-01,  1.8338e-01,  6.0694e-01, -6.6952e-01,\n",
      "        -1.8442e-01,  8.6930e-01,  5.0088e-01, -8.4739e-01, -2.4354e-01,\n",
      "         4.2371e-01,  7.8367e-01, -3.6316e-01,  8.9547e-01, -4.1515e-01,\n",
      "         8.4902e-01, -2.0342e-01,  7.8249e-02,  6.0293e-01, -6.0663e-01,\n",
      "         2.3577e-01, -8.9280e-01, -3.6751e-01, -2.2790e-01, -1.9552e-01,\n",
      "         3.7544e-01,  4.3087e-02,  9.0436e-01,  1.3475e-01,  1.1299e-01,\n",
      "        -5.5062e-02, -4.3915e-01,  4.5870e-01,  5.5710e-01, -1.6551e-01,\n",
      "         5.6396e-01, -6.2050e-02, -3.6481e-01,  4.3585e-01, -7.5835e-01,\n",
      "         7.8159e-01, -7.8102e-01,  7.5050e-01, -1.9047e-01, -6.6794e-01,\n",
      "        -7.0711e-01,  7.3040e-01, -1.6599e-01,  9.9961e-01, -7.1126e-01,\n",
      "        -4.0441e-01, -4.9414e-01,  6.4035e-01, -1.5753e-01, -7.1744e-01,\n",
      "         3.7743e-01,  1.5191e-01, -5.1009e-01, -8.5155e-01, -7.7377e-01,\n",
      "         2.3736e-01, -2.0133e-01,  5.2590e-01, -5.4738e-02,  8.6771e-01,\n",
      "         8.6583e-01, -5.2989e-01, -2.1541e-01,  9.0285e-01,  3.6775e-01,\n",
      "         3.3028e-01,  2.8349e-01,  4.9146e-01,  6.9262e-02,  4.6005e-02,\n",
      "        -6.5546e-01,  3.1187e-01, -9.9774e-01, -3.1246e-01, -5.5785e-01,\n",
      "         6.6454e-01, -8.0547e-01, -7.4047e-01,  6.2996e-01, -1.9631e-01,\n",
      "        -2.4149e-01, -8.2085e-01, -2.3280e-01,  6.3384e-03,  5.6853e-01,\n",
      "         5.4213e-01, -8.1780e-01,  3.6929e-01,  7.4864e-01,  6.8809e-01,\n",
      "         6.5412e-01, -1.1149e-01, -2.7852e-01,  8.4229e-01,  7.1221e-02,\n",
      "        -9.9989e-01, -4.3525e-01, -1.1740e-01, -8.0406e-01, -9.0217e-01,\n",
      "        -5.7104e-02,  2.0673e-01,  3.4857e-01,  4.0119e-01,  1.5290e-01,\n",
      "         7.7444e-01, -1.1221e-01, -6.7063e-01,  4.1772e-01,  9.8143e-01,\n",
      "         8.6353e-01,  1.9651e-01, -4.2538e-01, -9.7532e-01,  6.5864e-01,\n",
      "        -4.0431e-01,  4.2012e-01,  3.8227e-01, -9.7576e-02, -4.9932e-01,\n",
      "         2.0174e-03,  3.4773e-01,  7.2711e-01, -1.2839e-01,  6.7398e-01,\n",
      "         7.7297e-01, -2.2782e-01, -8.9312e-01, -9.4164e-01,  6.4872e-01,\n",
      "         9.7854e-01, -9.4954e-01, -4.7880e-01,  7.1616e-01,  2.3449e-01,\n",
      "        -3.1157e-01,  9.9528e-01,  4.2419e-01,  6.0540e-01, -6.7535e-01,\n",
      "         9.2944e-01,  4.6939e-01,  2.5814e-01,  3.8055e-01,  9.2204e-01,\n",
      "         1.7883e-01, -7.1512e-02, -5.3465e-01,  6.3747e-01, -6.3499e-01,\n",
      "        -8.1496e-01, -5.6157e-01, -7.4075e-01, -4.8238e-01,  8.0132e-01,\n",
      "         3.5542e-01, -4.0843e-02, -5.4218e-01, -9.7906e-01, -9.3862e-01,\n",
      "        -4.1711e-01, -8.0730e-01,  5.7837e-01,  6.9727e-01, -3.0754e-01,\n",
      "        -7.4951e-01,  4.9013e-01,  5.6394e-02, -9.3071e-01,  8.7059e-01,\n",
      "         6.3150e-01,  5.0648e-03,  2.4704e-01,  9.1569e-01, -2.3621e-01,\n",
      "         5.1726e-01,  8.0660e-01, -5.4539e-01, -4.3674e-01, -6.2242e-01,\n",
      "         5.3029e-01, -8.1103e-01,  7.5595e-02, -9.7180e-01, -1.5493e-01,\n",
      "        -6.5730e-01, -2.9729e-01, -7.9662e-01, -1.8333e-01, -6.7416e-01,\n",
      "        -9.1311e-01, -3.7151e-01, -3.2282e-01,  6.7650e-01, -3.3436e-01,\n",
      "        -3.3718e-01,  9.6951e-01,  5.2615e-01,  2.7825e-01,  2.1635e-01,\n",
      "         9.9248e-01, -1.3702e-01,  1.0325e-01, -5.3651e-01, -1.6498e-01,\n",
      "        -5.9912e-01, -8.2014e-01,  1.4346e-01, -5.1262e-01,  9.5940e-01,\n",
      "        -7.5283e-01,  3.2664e-01,  3.5715e-01, -3.8216e-01, -7.7278e-01,\n",
      "        -9.8347e-01,  9.3658e-01,  8.3733e-01, -3.3920e-02,  3.7647e-01,\n",
      "         4.3310e-01,  4.3140e-01, -9.6095e-01,  9.9815e-02, -9.5660e-01,\n",
      "         2.0820e-01, -1.9331e-01,  4.0420e-01,  4.4285e-01, -6.1633e-01,\n",
      "         3.7115e-01, -7.2574e-01,  6.1189e-01, -8.0323e-02,  6.6067e-01,\n",
      "        -8.7044e-01, -4.5443e-01, -8.8070e-01,  6.7796e-01,  2.0911e-01,\n",
      "        -3.7927e-01, -3.7014e-01, -9.1914e-01, -7.2086e-01, -3.0601e-01,\n",
      "        -9.8607e-01,  2.7667e-01,  6.7101e-01,  5.7436e-01, -4.6336e-01,\n",
      "         3.5454e-01, -5.5897e-01,  5.7287e-01,  5.4248e-01,  5.6317e-01,\n",
      "         9.1913e-01,  2.9104e-01,  9.1854e-01,  2.0284e-01, -8.9470e-01,\n",
      "         9.6896e-01, -5.1740e-01, -2.8134e-01, -5.5157e-01, -6.1935e-01,\n",
      "        -5.0624e-01,  6.8237e-02, -5.4085e-01, -8.2163e-01, -6.0967e-01,\n",
      "        -5.5117e-01,  1.9126e-01,  5.2165e-01,  7.6434e-01, -3.9217e-01,\n",
      "        -3.2147e-01, -5.4168e-01,  9.9516e-01,  2.7919e-01,  1.1460e-01,\n",
      "        -6.4201e-03, -1.2820e-01,  9.5973e-03, -1.1604e-02,  5.5875e-01,\n",
      "        -2.2783e-01,  2.3407e-01,  2.0783e-01, -2.5157e-02,  2.4367e-01,\n",
      "         8.0296e-01, -5.2441e-01,  4.4127e-01,  7.1172e-02, -8.1994e-01,\n",
      "        -6.8851e-01,  8.0675e-02, -5.6587e-01,  9.4015e-02,  4.4719e-01,\n",
      "        -4.2298e-01,  9.7618e-01,  5.2279e-01, -1.3932e-01, -7.5383e-02,\n",
      "        -6.4796e-01, -7.4851e-01,  6.8374e-01, -4.3806e-01, -6.8703e-02,\n",
      "         7.8563e-01,  6.9053e-02,  1.9139e-01,  4.3917e-01,  5.2026e-01,\n",
      "        -7.9007e-01, -6.6007e-01,  6.4302e-01, -1.4820e-01, -9.6075e-01,\n",
      "         9.7546e-01,  5.1650e-01, -9.0889e-02,  3.1151e-01,  1.5773e-01,\n",
      "         7.3676e-02,  5.0133e-01, -8.1493e-01, -9.4914e-01,  4.3926e-01,\n",
      "        -1.0454e-01, -9.1219e-01,  8.0469e-01, -7.0488e-01, -8.9128e-01,\n",
      "         5.8698e-01,  2.0963e-01,  4.6315e-01, -6.9191e-01, -9.3689e-01,\n",
      "        -8.9095e-01,  6.3571e-02, -5.1759e-01,  9.5945e-02, -1.9079e-01,\n",
      "         5.3513e-02,  6.6385e-01, -4.6587e-01,  5.2680e-01, -5.3116e-01,\n",
      "         2.6439e-01, -2.9795e-01,  1.3176e-01,  1.6304e-01,  5.2618e-01,\n",
      "        -1.8700e-01, -1.9503e-01,  1.9213e-01, -8.5121e-01,  5.4361e-01,\n",
      "        -5.6161e-01,  7.4892e-01, -9.8211e-01,  7.2072e-01, -7.9099e-01,\n",
      "         5.6900e-01,  7.0196e-01,  3.0973e-01,  5.0537e-01, -9.6663e-01,\n",
      "        -5.4910e-01, -9.3135e-01,  3.9692e-01, -1.8248e-02,  7.1081e-01,\n",
      "        -8.2018e-01,  8.4947e-01, -5.5939e-01,  5.9821e-01,  2.2942e-01,\n",
      "        -3.5482e-01,  6.5208e-01, -9.2812e-01, -9.5905e-01, -7.3914e-01,\n",
      "         5.2264e-01, -5.8342e-01, -9.7409e-01, -1.9676e-02,  7.1061e-01,\n",
      "        -9.5661e-01, -8.0088e-01,  1.5951e-01,  7.5277e-02,  8.4636e-01,\n",
      "        -6.7546e-01,  3.8330e-01, -1.4672e-01, -5.9932e-01,  9.0147e-01,\n",
      "         3.7785e-01, -2.1498e-01,  5.3725e-01,  7.3377e-01,  3.3161e-01,\n",
      "        -8.8984e-01, -2.1038e-01, -1.3257e-02,  3.3892e-01,  8.1200e-02,\n",
      "         1.9882e-02, -8.1142e-01, -7.7189e-01,  6.9990e-01,  8.9168e-02,\n",
      "         2.4659e-01, -3.5976e-01,  7.1881e-01, -1.8893e-01,  5.1290e-01,\n",
      "         4.9213e-01,  9.1020e-01,  6.5855e-02,  7.0328e-02, -8.4369e-01,\n",
      "        -1.9527e-01, -4.4194e-01,  9.7236e-01,  5.1942e-01, -8.4541e-01,\n",
      "        -2.7944e-01, -5.0163e-01, -1.5941e-01, -4.8511e-01,  8.7286e-01,\n",
      "        -7.2523e-01,  2.7202e-02, -5.6496e-01, -9.6249e-02, -3.1169e-01,\n",
      "        -7.9947e-01, -4.4427e-01, -6.8710e-01, -7.7539e-01,  2.5344e-01,\n",
      "        -5.1496e-01,  3.2438e-01, -1.6004e-01, -8.5500e-01,  3.7980e-02,\n",
      "         4.4218e-01,  3.4833e-01,  1.7840e-02, -5.8652e-01, -3.8400e-01,\n",
      "         4.4794e-01, -6.6398e-01, -9.9820e-01,  5.2697e-02, -8.7461e-01,\n",
      "        -9.1186e-01,  7.9793e-01,  3.5256e-01,  1.0260e-01, -7.2073e-01,\n",
      "         3.6636e-01,  2.7526e-01, -2.8861e-01, -8.7440e-01,  6.7777e-02,\n",
      "         6.6298e-01, -7.5376e-01, -3.6086e-01, -1.4859e-01, -6.2669e-01,\n",
      "         2.4212e-01, -4.6325e-01, -4.3365e-01, -2.8532e-01,  3.4355e-01,\n",
      "         7.8219e-01, -4.4445e-02,  4.6798e-01,  8.3839e-01,  6.5539e-01,\n",
      "         6.3276e-01,  6.8630e-01,  4.0529e-01, -4.0076e-01,  9.6953e-04,\n",
      "         2.6502e-01,  8.5845e-01, -9.0761e-02,  6.5668e-01,  6.4031e-01,\n",
      "        -2.9620e-01,  4.9586e-01, -9.6479e-01, -1.3566e-01,  1.9346e-01,\n",
      "         6.3530e-01,  9.7521e-02,  4.5925e-01, -4.7506e-01,  5.3075e-01,\n",
      "        -9.5244e-01, -8.7748e-01, -8.9104e-01, -8.6450e-01,  6.6571e-01,\n",
      "         2.1577e-01,  6.5701e-02,  5.1317e-01,  1.8818e-01, -5.2747e-02,\n",
      "         1.6358e-01,  6.1048e-01, -2.8159e-01, -7.7419e-01, -4.9055e-01,\n",
      "         3.2279e-01,  6.3475e-02, -6.0780e-01,  6.2141e-01, -9.8664e-01,\n",
      "         7.1402e-01, -6.4472e-01, -5.6095e-02,  3.7667e-01, -1.4428e-01,\n",
      "        -7.9354e-01,  7.8135e-01, -2.5457e-01, -6.0046e-01, -2.1360e-02,\n",
      "        -7.8890e-01, -7.4745e-01, -9.2011e-01, -5.0725e-01, -3.1151e-02,\n",
      "        -1.7858e-01,  3.3710e-02,  3.7265e-01, -7.6530e-01,  2.3580e-01,\n",
      "         1.6574e-01,  4.4177e-01,  4.1304e-01,  2.0692e-01, -9.5941e-01,\n",
      "         2.6242e-01,  5.0955e-01,  9.8666e-01,  1.6755e-01, -7.9289e-01,\n",
      "         7.0060e-01, -1.1816e-02, -5.1859e-01, -6.0517e-01, -1.8166e-01,\n",
      "        -2.1061e-01,  1.9964e-01, -5.9861e-01,  5.1547e-01, -6.7247e-01,\n",
      "        -1.0044e-01,  4.1564e-01,  3.2794e-01, -8.9076e-01, -2.5595e-02,\n",
      "         9.0167e-01, -4.3806e-01,  5.4427e-01, -5.3687e-01, -8.8750e-01,\n",
      "         3.9391e-01,  7.8563e-01,  6.5800e-01,  7.6750e-01,  7.4250e-01,\n",
      "        -2.6686e-01, -4.1266e-01,  6.7095e-01,  2.5882e-01, -8.1136e-01,\n",
      "        -4.0095e-01, -3.0622e-01,  4.2117e-01, -4.5492e-01,  4.8997e-01,\n",
      "        -3.2170e-01, -3.8033e-01, -2.1316e-01, -6.1841e-02,  6.0613e-01,\n",
      "         2.1764e-01, -9.8744e-01,  6.8351e-01,  2.2237e-01, -9.7305e-01,\n",
      "         5.3613e-01,  3.8673e-01, -6.9152e-01, -3.4590e-02,  3.0751e-01,\n",
      "        -3.7575e-01,  6.5277e-01,  7.0665e-01, -2.4365e-01, -7.4545e-01,\n",
      "        -1.3518e-02, -9.8907e-01, -6.3291e-01,  2.2742e-01, -8.2621e-01,\n",
      "         6.1873e-02,  5.3335e-01, -3.1073e-01, -5.5318e-01, -3.1670e-01,\n",
      "        -6.9904e-01, -9.8536e-01, -3.6359e-01,  7.5230e-01,  5.4696e-01,\n",
      "        -9.7127e-02,  4.6561e-01, -3.4976e-01,  3.8958e-01, -9.3127e-02,\n",
      "         5.3424e-01, -6.4355e-01,  3.5920e-01,  7.1885e-01,  4.6564e-01,\n",
      "         3.1795e-01,  2.0184e-01,  4.1919e-01,  3.5500e-01, -2.9172e-01,\n",
      "         6.4009e-01,  5.3719e-02,  1.0783e-01, -1.4053e-01, -9.2202e-01,\n",
      "        -7.1939e-01, -9.1736e-01, -8.9016e-01, -5.9633e-01,  9.0162e-01,\n",
      "         5.2597e-01,  9.4554e-01,  2.3732e-01, -3.6043e-01, -6.0500e-01,\n",
      "        -5.1711e-01,  2.9711e-01, -4.9309e-01, -1.2036e-01, -4.5583e-01,\n",
      "        -3.7470e-01, -7.6586e-01,  9.2502e-01, -8.6456e-01,  9.1725e-01,\n",
      "        -2.2943e-01,  2.5947e-01,  1.1719e-01, -2.1602e-02,  7.0947e-01,\n",
      "        -2.0381e-01,  1.0507e-01, -3.8374e-01, -6.6105e-01,  1.4920e-01,\n",
      "        -6.3108e-01, -8.6434e-02,  2.9293e-01,  7.2791e-01, -4.0914e-01,\n",
      "         5.7896e-01,  5.0677e-01,  8.6744e-01,  2.4059e-01, -6.8837e-02,\n",
      "         6.1204e-02,  1.9386e-01, -2.4196e-01,  5.4853e-01, -6.5775e-02,\n",
      "        -3.1086e-01, -5.2111e-01, -2.8386e-01,  3.6960e-01, -1.0736e-01,\n",
      "         8.9989e-01,  9.5390e-01, -8.7248e-01,  6.7564e-01,  5.0522e-01,\n",
      "        -6.4348e-01, -1.8284e-01,  5.8063e-01, -1.2557e-01, -4.7047e-01,\n",
      "         8.8233e-01, -8.1280e-01,  3.8244e-01,  7.5944e-01, -6.8602e-01,\n",
      "        -1.7020e-01,  2.3618e-02,  3.3012e-01], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0304,  0.0019, -0.0309,  ..., -0.0154,  0.0141, -0.0161],\n",
      "        [ 0.0015,  0.0196,  0.0318,  ..., -0.0272, -0.0260, -0.0018],\n",
      "        [ 0.0243,  0.0140, -0.0147,  ..., -0.0030, -0.0304,  0.0251],\n",
      "        ...,\n",
      "        [ 0.0279, -0.0094, -0.0348,  ...,  0.0070, -0.0006,  0.0221],\n",
      "        [ 0.0110,  0.0247, -0.0196,  ...,  0.0235,  0.0091,  0.0224],\n",
      "        [-0.0103,  0.0196, -0.0230,  ...,  0.0248,  0.0092,  0.0117]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-1.9880e-02, -9.5092e-03, -2.9251e-02,  2.5831e-02,  2.6972e-02,\n",
      "        -2.0938e-02, -2.8501e-02, -1.2971e-02, -7.3850e-03,  3.3170e-02,\n",
      "        -2.3106e-02,  1.8736e-02,  1.4224e-03,  4.0583e-03,  3.5046e-02,\n",
      "        -3.5327e-02, -2.0185e-02, -1.6393e-02,  6.2867e-03,  2.1446e-02,\n",
      "         9.5883e-03,  4.0676e-03, -1.2381e-02,  2.0106e-02,  4.0708e-03,\n",
      "        -2.3762e-02,  2.8444e-02,  9.2460e-03,  1.7154e-02,  3.1467e-02,\n",
      "         2.4124e-02, -1.0443e-02, -7.7718e-03,  1.2752e-02, -7.0894e-03,\n",
      "        -2.4830e-02, -3.4544e-02,  3.4051e-03,  9.8579e-03,  9.9202e-03,\n",
      "         3.1424e-02, -3.5020e-02, -2.6483e-02, -2.6879e-02,  3.4484e-02,\n",
      "         2.5612e-02,  1.8504e-02, -2.2865e-02, -2.0594e-02,  3.3830e-03,\n",
      "         2.5531e-02, -7.7313e-03, -1.3703e-02,  4.9689e-03,  1.1621e-02,\n",
      "        -2.5815e-02,  1.0031e-02, -5.6010e-03,  1.7769e-02,  5.2681e-03,\n",
      "         1.6247e-02,  1.8196e-02, -2.3787e-02, -3.2264e-03, -1.8513e-02,\n",
      "         3.5486e-02, -3.0182e-02,  9.8585e-03, -3.5405e-02,  2.3081e-02,\n",
      "        -1.3787e-02, -1.1154e-02,  1.7952e-02,  1.6609e-02, -2.8176e-02,\n",
      "        -1.1334e-02, -6.3446e-03, -3.1303e-02,  1.6219e-02, -2.6147e-02,\n",
      "        -3.4217e-02, -2.7188e-02, -1.1083e-02,  2.5319e-02, -1.1793e-02,\n",
      "        -3.5345e-02, -3.5032e-02,  3.4606e-02, -6.4640e-03, -1.3594e-02,\n",
      "        -2.9775e-02, -3.7126e-03, -2.4089e-02, -1.1613e-02, -2.2195e-02,\n",
      "        -2.0898e-02, -3.2492e-02, -2.5904e-02, -2.0489e-02,  1.2098e-02,\n",
      "        -1.9906e-02,  9.7360e-03,  1.4957e-02,  2.2551e-02,  3.1916e-02,\n",
      "         2.5384e-02,  2.9964e-02, -2.9391e-02,  1.7895e-02, -1.8427e-02,\n",
      "        -3.2242e-02,  3.2254e-02, -1.4075e-02,  3.4046e-02, -1.4924e-02,\n",
      "         3.2601e-03, -5.0607e-03, -3.5996e-02, -8.3834e-03, -5.5637e-03,\n",
      "         3.4052e-02, -1.5673e-02,  8.4163e-03, -2.6187e-02, -3.2035e-02,\n",
      "        -3.4764e-02,  2.8513e-02, -1.7926e-02,  1.4932e-02, -7.3184e-03,\n",
      "         3.2033e-02,  1.9177e-02,  1.1574e-02,  2.5937e-02, -3.5916e-02,\n",
      "         2.6161e-02, -1.9835e-02, -2.6182e-02,  1.1992e-02,  1.5011e-02,\n",
      "        -1.8104e-02,  2.5663e-03,  3.5086e-02, -2.6153e-02,  2.0856e-02,\n",
      "         6.8021e-03, -1.0042e-03, -2.7307e-02, -3.5901e-03,  3.0038e-02,\n",
      "        -4.5209e-03, -1.9328e-02, -5.1599e-03,  3.1473e-02,  2.2646e-02,\n",
      "         1.1790e-02,  1.2839e-02, -3.4319e-02, -1.2314e-02,  3.6500e-03,\n",
      "         3.3990e-02, -1.6163e-02,  2.2661e-02, -1.4727e-02,  1.6787e-02,\n",
      "         2.9661e-03, -1.4661e-02, -3.3359e-02,  2.1141e-02, -2.3326e-02,\n",
      "         3.2925e-02, -1.5937e-02,  2.1356e-02,  3.3397e-02, -2.2063e-02,\n",
      "        -6.6866e-03, -2.7798e-03, -1.0558e-02,  2.9259e-02, -2.2239e-03,\n",
      "         1.5283e-02, -2.8826e-02,  2.6957e-03, -2.5138e-02, -1.5254e-02,\n",
      "        -2.1845e-02, -9.8386e-03, -2.1525e-02, -8.6041e-03,  2.8290e-02,\n",
      "         2.2600e-02,  4.7828e-03,  8.0328e-03,  1.3752e-02,  2.4864e-02,\n",
      "         2.1814e-02, -3.0011e-02,  2.3274e-02, -2.3880e-02,  2.2496e-02,\n",
      "        -2.1926e-02, -3.4633e-03, -1.7151e-02,  1.8739e-02, -3.4200e-02,\n",
      "         3.9576e-04, -1.0082e-02, -2.7937e-02, -2.6623e-02,  8.7996e-03,\n",
      "        -3.0201e-05, -2.1915e-02,  1.7729e-02,  2.7562e-02,  2.4526e-02,\n",
      "         1.3691e-02, -1.2359e-02,  4.8026e-03, -2.7350e-02,  4.5325e-03,\n",
      "         8.5426e-03,  1.2807e-02,  8.4872e-03,  2.8926e-02,  2.6657e-02,\n",
      "         3.5660e-02,  6.6108e-03,  1.0305e-02, -2.0900e-02, -3.9116e-03,\n",
      "         3.3120e-02, -1.2169e-02,  1.7732e-02, -4.3269e-03, -8.4522e-03,\n",
      "        -2.7887e-02, -2.8606e-02, -2.5219e-02,  3.1979e-03, -3.4483e-02,\n",
      "        -1.9759e-02, -7.4168e-03,  2.3855e-02, -2.5652e-02, -3.3822e-03,\n",
      "         3.0412e-02, -7.3660e-04, -7.3616e-03, -1.0298e-02,  3.2447e-02,\n",
      "         3.2808e-02,  6.6683e-03,  1.4231e-02,  1.7850e-02, -4.4186e-03,\n",
      "        -7.7949e-03,  1.6194e-02, -1.3479e-03,  3.2812e-02,  5.8094e-03,\n",
      "         9.5402e-03, -3.4516e-02, -3.2197e-05,  1.8442e-02, -1.4960e-02,\n",
      "         4.4119e-03,  1.6972e-02,  2.1309e-02,  1.7797e-02,  7.0206e-03,\n",
      "        -1.4071e-02,  6.4872e-03, -3.4138e-02,  2.9748e-02,  3.5799e-02,\n",
      "         2.7364e-02, -1.1144e-02, -1.1749e-02, -1.6593e-02,  5.6657e-04,\n",
      "        -4.7500e-03, -3.1551e-02,  9.2902e-03,  1.7329e-02, -7.5261e-03,\n",
      "        -6.2649e-03, -9.2659e-03, -4.2807e-03,  2.5824e-03, -1.9530e-02,\n",
      "         1.3176e-02, -9.1493e-04,  1.1177e-02,  8.9838e-03, -3.4342e-02,\n",
      "        -1.8050e-02,  1.7412e-03, -9.1137e-03, -2.2125e-02,  3.4468e-02,\n",
      "         2.0543e-02,  3.1523e-02, -3.5931e-02, -9.5764e-03, -2.3642e-02,\n",
      "         2.3691e-02,  3.4620e-02, -9.9970e-04, -4.2578e-03,  3.0146e-02,\n",
      "        -1.0751e-02,  3.0825e-02,  8.2343e-04,  3.1521e-02,  1.9654e-03,\n",
      "        -5.5815e-03, -1.4332e-02,  3.0655e-02,  3.1760e-02,  3.1664e-02,\n",
      "        -1.7551e-02, -8.6390e-04, -3.3357e-02, -8.6700e-03,  2.8245e-02,\n",
      "        -2.3678e-02,  1.7150e-03, -2.9936e-02, -1.1306e-02,  9.6852e-03,\n",
      "         9.8219e-03, -1.8586e-02,  1.2424e-02, -3.1352e-02, -1.2617e-02,\n",
      "         1.6459e-02,  3.5278e-02, -6.1834e-03,  3.3820e-02,  1.7682e-02,\n",
      "         3.4884e-02, -1.8365e-02,  2.2786e-02, -2.7891e-02, -2.2920e-02,\n",
      "        -2.1715e-03, -2.6382e-02,  1.0488e-02, -2.6518e-02,  1.4993e-02,\n",
      "         1.9925e-02, -2.1031e-02,  1.3247e-02, -1.1532e-02,  8.8049e-03,\n",
      "         1.0748e-02, -1.6440e-02,  2.6578e-02,  1.2923e-02, -2.3843e-02,\n",
      "         2.7416e-02, -9.9490e-03, -3.4559e-02, -1.9872e-02,  1.6109e-03,\n",
      "         3.5711e-02,  8.9295e-03, -2.5648e-02,  3.5329e-02, -2.3170e-02,\n",
      "        -2.8404e-02, -2.2668e-02, -1.8557e-02,  1.7479e-02,  3.3039e-02,\n",
      "        -8.2029e-03, -6.7141e-03,  2.4592e-02,  8.7963e-03,  1.3466e-02,\n",
      "         2.2335e-02, -2.4965e-02,  3.2504e-02,  2.7047e-02,  1.0368e-02,\n",
      "        -2.8779e-02, -3.9547e-03,  3.0410e-03,  2.5092e-02, -9.1684e-03,\n",
      "        -2.1212e-02, -3.0497e-02,  5.4035e-03, -1.3261e-02, -3.1212e-02,\n",
      "        -3.6047e-05,  1.0405e-02, -3.0251e-02,  8.2970e-04,  3.3143e-03,\n",
      "         2.9845e-02,  2.1922e-02,  6.6129e-03, -7.8142e-04, -3.6884e-03,\n",
      "        -3.3687e-02,  1.5157e-03, -2.2517e-02,  8.7320e-03,  2.4646e-02,\n",
      "        -5.6482e-03, -1.6802e-02, -3.5584e-02,  1.0497e-02, -1.9022e-02,\n",
      "         2.4579e-02,  1.2928e-02, -7.0116e-03,  2.0898e-02, -8.8798e-03,\n",
      "        -1.8662e-02, -2.5852e-02,  3.3286e-02, -2.8513e-02, -3.3285e-02,\n",
      "         1.6030e-02, -1.2592e-02,  3.2950e-02, -3.1107e-02, -1.0693e-02,\n",
      "        -2.3354e-02,  2.9446e-02, -6.2658e-03,  3.3733e-02,  3.4217e-02,\n",
      "        -3.5777e-02,  1.4480e-02,  2.2650e-02, -2.1440e-02,  1.5081e-02,\n",
      "        -3.4195e-02, -1.0245e-02,  3.2300e-03,  8.4796e-03,  2.3249e-03,\n",
      "        -3.7787e-03,  2.9935e-02,  2.3969e-02, -1.3362e-02,  8.5997e-03,\n",
      "        -3.3556e-03, -2.9129e-02, -2.4049e-02, -1.4442e-02,  1.0030e-02,\n",
      "        -2.0988e-02, -2.2499e-02,  2.7179e-02,  2.6057e-02,  1.2871e-02,\n",
      "        -3.3555e-02,  2.3923e-03,  2.4855e-02,  6.9945e-03,  2.5090e-02,\n",
      "         1.6409e-03,  4.4434e-03, -2.9292e-02,  2.4486e-02,  1.4712e-02,\n",
      "        -3.5356e-02,  3.0106e-02, -3.3220e-02, -1.1000e-02,  1.2322e-02,\n",
      "        -2.9903e-02, -2.8081e-02, -1.5834e-02, -1.0965e-02, -1.9184e-02,\n",
      "         2.6260e-02,  1.8397e-02, -2.3118e-02,  1.7737e-02, -1.7020e-02,\n",
      "        -1.7506e-02,  4.1268e-03, -2.6483e-02,  1.5891e-02,  1.8067e-02,\n",
      "         3.3265e-02,  3.5809e-02, -1.4263e-02,  3.4968e-02,  8.9747e-03,\n",
      "         1.1143e-02, -3.3886e-02, -4.6504e-03,  3.9184e-03, -2.0679e-02,\n",
      "         2.2851e-04, -3.3906e-03, -4.9101e-03,  3.0932e-02,  3.8650e-03,\n",
      "        -7.0364e-03,  1.9186e-02,  2.2980e-02, -2.7544e-02,  3.3365e-02,\n",
      "        -2.5589e-02, -1.1079e-02,  2.0971e-03,  1.1237e-03,  1.2634e-02,\n",
      "        -7.6176e-03,  2.9577e-02, -2.1876e-02, -1.4830e-02,  2.0906e-02,\n",
      "         9.5470e-03, -8.9324e-03,  1.2090e-02,  1.1735e-02, -2.6732e-02,\n",
      "         7.1285e-03,  1.0969e-02,  1.4322e-02,  2.4893e-02,  3.0222e-02,\n",
      "        -7.0545e-03,  1.5262e-02, -3.5137e-02,  4.2103e-03,  1.0974e-02,\n",
      "        -1.5964e-03, -2.2652e-02,  2.2857e-02, -3.7502e-04,  4.0860e-03,\n",
      "         6.4655e-03, -1.7145e-02,  1.9040e-02, -1.7106e-02, -2.9482e-02,\n",
      "         3.3665e-02,  1.2088e-02, -1.4134e-02, -1.8975e-02,  3.5020e-04,\n",
      "         8.1026e-03, -3.0373e-02,  1.6447e-02,  4.8742e-03,  4.0105e-03,\n",
      "        -5.1931e-03, -1.9105e-02,  2.6047e-02,  1.9063e-02,  1.4836e-03,\n",
      "         3.4222e-02,  3.3417e-02,  3.0864e-02, -1.7626e-02,  1.2431e-02,\n",
      "        -2.0872e-02,  3.2863e-03, -2.4959e-02, -9.9710e-03, -2.8707e-02,\n",
      "        -2.1542e-02,  1.7140e-02,  6.3322e-03, -3.0563e-02,  3.1144e-02,\n",
      "         3.4314e-03,  3.1505e-02, -3.0969e-02, -2.4689e-02,  1.4773e-02,\n",
      "         3.8677e-03, -3.3478e-02, -2.8866e-02, -3.1356e-02,  3.5155e-02,\n",
      "         3.5506e-02,  3.4713e-02, -1.9308e-02, -3.0361e-02, -3.1138e-02,\n",
      "         2.5742e-02,  8.7882e-03,  3.5840e-02, -3.3188e-02, -3.2083e-02,\n",
      "         2.9613e-02,  3.3337e-02,  3.4892e-02, -3.2497e-02, -3.1095e-02,\n",
      "         1.1898e-02, -9.2687e-03,  8.7839e-03, -1.3860e-02,  2.0794e-02,\n",
      "         2.0774e-02, -3.5744e-02, -1.3901e-02,  2.4763e-02, -3.0826e-02,\n",
      "         5.7432e-03,  3.0029e-02,  2.7289e-02,  3.2967e-02,  1.9264e-02,\n",
      "         2.7096e-02,  2.7729e-02, -1.6501e-02, -7.1818e-03, -4.9936e-03,\n",
      "         4.2861e-03,  2.6913e-02, -3.3735e-02, -3.8897e-03, -1.4921e-02,\n",
      "         2.3880e-02,  2.5655e-02, -2.7921e-02, -2.0721e-02,  3.2778e-02,\n",
      "         2.8229e-02, -2.1868e-02, -8.8670e-03, -6.7014e-03, -1.2485e-02,\n",
      "         2.0861e-02,  1.7070e-02,  6.2552e-03, -2.1606e-02,  2.6712e-02,\n",
      "        -2.2577e-02, -3.1896e-02,  7.2643e-03,  2.6850e-02, -3.1495e-02,\n",
      "        -2.3043e-02,  4.2422e-03, -2.5158e-04, -2.3703e-02, -5.2310e-03,\n",
      "        -3.7230e-03,  1.8825e-02,  2.1815e-02,  2.4177e-04, -2.5996e-02,\n",
      "        -3.1364e-02, -2.2419e-02, -1.5844e-02, -1.3092e-02,  2.5039e-02,\n",
      "         1.3712e-02,  1.5000e-02, -3.8355e-03,  1.9732e-02, -2.4305e-02,\n",
      "         7.6808e-03, -2.4818e-03, -9.8328e-03, -1.9473e-02,  2.4026e-02,\n",
      "        -1.9092e-02,  9.4606e-03,  1.2430e-02, -2.8683e-02, -3.4230e-02,\n",
      "         3.3131e-02,  2.1714e-02, -1.0207e-02, -5.0710e-03, -5.7424e-03,\n",
      "         2.3946e-02, -2.2814e-03,  1.0529e-02,  3.1346e-02,  1.2213e-02,\n",
      "         3.2756e-02,  7.2193e-03,  5.0403e-03,  7.9945e-03,  2.7333e-02,\n",
      "         3.2864e-02, -2.7391e-03, -1.9648e-02, -6.9074e-03, -1.7104e-02,\n",
      "        -3.2579e-02,  2.2676e-02,  9.9336e-03,  2.9731e-02, -9.1513e-03,\n",
      "         1.1063e-02, -2.5627e-02, -2.7334e-02,  3.1793e-02,  2.2722e-03,\n",
      "        -8.6660e-03,  1.3114e-02,  5.2692e-03, -1.7262e-02, -4.0000e-03,\n",
      "        -2.0073e-02, -2.6451e-02,  1.5768e-02, -2.7343e-03, -1.9835e-02,\n",
      "         1.3768e-02,  3.4942e-02, -2.1369e-02,  1.8137e-02, -2.1832e-03,\n",
      "         2.4859e-03,  2.4771e-02,  2.9219e-02, -1.0911e-02, -3.0460e-02,\n",
      "         1.9987e-02, -3.2244e-02,  2.6653e-03,  9.1355e-03, -8.7767e-03,\n",
      "        -3.2061e-02, -3.4828e-03,  3.2403e-02,  2.3157e-02, -1.3402e-02,\n",
      "         3.4384e-02, -1.7571e-02,  2.4589e-02,  1.5795e-02, -1.9020e-02,\n",
      "        -3.4407e-02, -1.8915e-02,  1.5831e-02,  7.7162e-04, -1.5760e-02,\n",
      "         3.4291e-02,  2.9127e-02, -3.4708e-02, -1.3995e-02, -3.4619e-02,\n",
      "         3.4225e-02,  3.1408e-02, -9.7745e-04, -8.3197e-04, -2.4961e-02,\n",
      "        -2.6438e-02,  2.2122e-02,  1.9288e-02, -2.7166e-02,  3.9128e-03,\n",
      "        -2.5117e-02, -1.7101e-02,  2.6222e-02, -1.4586e-02,  2.1475e-02,\n",
      "         3.1639e-02,  2.3300e-02,  9.8531e-03], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.7126, -1.5891,  0.2943,  ..., -1.9952,  1.7104, -1.2701],\n",
      "        [ 0.3613, -0.1219,  1.7634,  ..., -0.2975,  0.9921, -0.4134],\n",
      "        [ 1.2652,  0.8067, -1.0855,  ..., -0.1556,  0.5439,  1.3181],\n",
      "        ...,\n",
      "        [ 0.5178, -0.2480, -1.0158,  ...,  0.4158,  1.8847,  0.9066],\n",
      "        [-0.2289, -0.4976,  2.0739,  ...,  2.1776, -0.1725,  2.4237],\n",
      "        [-1.8412, -0.5930, -0.1576,  ...,  0.3241,  0.7576,  1.0622]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.3796, -0.5644, -1.0253,  ...,  0.7936,  0.4886, -1.1543],\n",
      "        [ 1.3468,  0.1509, -0.3485,  ...,  1.0306,  0.3322, -1.8955],\n",
      "        [-0.9754, -0.4013,  1.2310,  ...,  0.3586,  0.4198, -1.3077]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 1.2935,  0.1045,  0.0675,  ..., -0.8503, -0.3183, -1.2471],\n",
      "        [-0.0513,  1.1872,  0.5620,  ..., -2.7456, -0.0569, -0.7011],\n",
      "        [-0.8071, -1.2234, -0.0562,  ...,  2.0724,  1.9121,  0.5078],\n",
      "        ...,\n",
      "        [ 0.6437, -2.3342, -0.7541,  ...,  0.4649,  0.3997, -0.8420],\n",
      "        [-1.3219,  0.2381,  0.6692,  ..., -0.6620,  0.1292, -0.5457],\n",
      "        [ 0.1462,  1.8205, -0.2577,  ..., -0.1015, -1.1704, -2.0238]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-1.6774, -0.8126,  0.1243,  ..., -0.0843, -1.0687, -0.4635],\n",
      "        [ 0.2918,  1.1216,  2.9099,  ...,  2.0004,  0.5280,  0.5286]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.6969, -0.1608,  1.2393,  ...,  0.8447,  0.7064, -2.2724],\n",
      "        [-1.1889,  2.2895, -0.3938,  ..., -0.8304, -0.4417,  0.3208],\n",
      "        [ 0.6137,  1.3701, -0.6039,  ...,  0.8141, -0.7532,  0.9864],\n",
      "        [ 1.9513, -0.1160,  2.2897,  ...,  1.4837, -1.0475,  0.1098]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0159,  0.0125,  0.0044,  ...,  0.0062,  0.0045,  0.0170],\n",
      "        [-0.0088,  0.0066, -0.0069,  ..., -0.0002, -0.0093,  0.0111],\n",
      "        [-0.0580, -0.0155, -0.0228,  ..., -0.0092,  0.0237,  0.0417],\n",
      "        ...,\n",
      "        [-0.0036, -0.0008, -0.0171,  ...,  0.0086,  0.0188, -0.0198],\n",
      "        [ 0.0241,  0.0057,  0.0106,  ...,  0.0064, -0.0277,  0.0042],\n",
      "        [-0.0031, -0.0041,  0.0020,  ...,  0.0216,  0.0003, -0.0069]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0129,  0.0103,  0.0096,  ...,  0.0074, -0.0178,  0.0289],\n",
      "        [-0.0048,  0.0065, -0.0177,  ..., -0.0096, -0.0139, -0.0318]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-2.6646e-03, -1.2401e-02, -1.4125e-02,  ...,  6.7393e-03,\n",
      "          2.9344e-02,  1.8372e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-3.7868e-04, -1.6126e-02, -1.0849e-02,  ..., -1.3496e-03,\n",
      "         -5.7271e-03, -2.1627e-02],\n",
      "        ...,\n",
      "        [ 1.4803e-02,  2.4659e-02,  8.1294e-05,  ..., -5.1707e-02,\n",
      "          3.8575e-02,  3.1198e-02],\n",
      "        [-1.7266e-02,  1.0165e-02, -4.4671e-02,  ...,  9.0311e-03,\n",
      "          6.2338e-02, -1.9787e-02],\n",
      "        [ 5.0787e-02, -1.8836e-02,  5.8175e-03,  ..., -3.2377e-02,\n",
      "          1.4445e-02, -1.1845e-03]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0002, -0.0301, -0.0058,  ...,  0.0063, -0.0096,  0.0029],\n",
      "        [-0.0137,  0.0180,  0.0097,  ..., -0.0099, -0.0137,  0.0076],\n",
      "        [ 0.0043,  0.0113, -0.0041,  ..., -0.0073, -0.0184,  0.0076],\n",
      "        ...,\n",
      "        [-0.0125,  0.0174, -0.0058,  ..., -0.0192,  0.0132, -0.0064],\n",
      "        [-0.0204, -0.0052,  0.0118,  ..., -0.0323, -0.0129,  0.0148],\n",
      "        [ 0.0135,  0.0102,  0.0165,  ...,  0.0072,  0.0076, -0.0161]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0200,  0.0005,  0.0475,  ..., -0.0097, -0.0227, -0.0116],\n",
      "        [ 0.0009, -0.0140,  0.0135,  ..., -0.0610, -0.0043,  0.0192],\n",
      "        [ 0.0297,  0.0363, -0.0112,  ..., -0.0042, -0.0126, -0.0206],\n",
      "        ...,\n",
      "        [ 0.0679, -0.0267, -0.0139,  ...,  0.0031,  0.0109, -0.0222],\n",
      "        [-0.0148, -0.0004, -0.0092,  ..., -0.0310, -0.0087, -0.0037],\n",
      "        [-0.0181, -0.0063, -0.0056,  ..., -0.0173, -0.0198,  0.0074]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0214, -0.0087, -0.0152,  ..., -0.0127, -0.0184, -0.0343],\n",
      "        [-0.0109, -0.0087,  0.0254,  ...,  0.0319,  0.0065, -0.0388],\n",
      "        [ 0.0075,  0.0212, -0.0063,  ..., -0.0039,  0.0048,  0.0209],\n",
      "        ...,\n",
      "        [ 0.0244, -0.0111,  0.0258,  ...,  0.0011, -0.0115,  0.0093],\n",
      "        [ 0.0033,  0.0327,  0.0204,  ...,  0.0213, -0.0032, -0.0153],\n",
      "        [ 0.0081, -0.0129, -0.0232,  ..., -0.0141,  0.0372,  0.0237]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0067,  0.0145,  0.0202,  ...,  0.0032, -0.0031,  0.0042],\n",
      "        [-0.0056, -0.0155, -0.0128,  ...,  0.0037, -0.0387,  0.0044],\n",
      "        [ 0.0128,  0.0129,  0.0333,  ..., -0.0061,  0.0300,  0.0060],\n",
      "        ...,\n",
      "        [-0.0009,  0.0122, -0.0041,  ...,  0.0499,  0.0021, -0.0072],\n",
      "        [ 0.0074,  0.0028,  0.0044,  ..., -0.0163,  0.0046, -0.0038],\n",
      "        [ 0.0132, -0.0176, -0.0151,  ...,  0.0408,  0.0227,  0.0335]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-1.1918e-04,  5.2762e-02, -1.1849e-02,  ...,  4.2684e-03,\n",
      "          8.7798e-03,  1.9799e-02],\n",
      "        [-8.2788e-03, -5.9913e-03, -3.3684e-03,  ...,  1.3837e-02,\n",
      "          4.9121e-02, -1.9109e-02],\n",
      "        [ 1.0636e-02,  7.9075e-03,  3.6708e-04,  ..., -2.6124e-02,\n",
      "         -1.6715e-02, -1.1770e-02],\n",
      "        ...,\n",
      "        [ 5.4881e-03, -1.7476e-02,  3.0758e-02,  ..., -3.0855e-03,\n",
      "          2.2551e-02,  1.4641e-02],\n",
      "        [-2.0788e-03, -1.0972e-02, -1.1184e-02,  ...,  3.2036e-02,\n",
      "         -7.9974e-03,  2.3088e-02],\n",
      "        [ 2.8019e-02, -6.4430e-03, -3.9725e-03,  ...,  2.5331e-03,\n",
      "          1.9523e-02, -9.8420e-05]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0130,  0.0017, -0.0048,  ..., -0.0020, -0.0083,  0.0330],\n",
      "        [ 0.0142, -0.0265,  0.0356,  ...,  0.0060, -0.0200,  0.0264],\n",
      "        [ 0.0188,  0.0036,  0.0035,  ...,  0.0061, -0.0222, -0.0066],\n",
      "        ...,\n",
      "        [ 0.0220,  0.0020, -0.0043,  ..., -0.0217, -0.0285, -0.0177],\n",
      "        [-0.0119,  0.0027,  0.0039,  ..., -0.0088,  0.0091, -0.0325],\n",
      "        [-0.0129,  0.0062, -0.0138,  ...,  0.0013, -0.0315,  0.0173]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0125,  0.0033, -0.0041,  ...,  0.0030,  0.0257, -0.0090],\n",
      "        [ 0.0225,  0.0049, -0.0046,  ..., -0.0046, -0.0139, -0.0143],\n",
      "        [ 0.0074, -0.0315, -0.0066,  ..., -0.0026,  0.0096, -0.0165],\n",
      "        ...,\n",
      "        [-0.0094, -0.0175, -0.0404,  ..., -0.0483, -0.0040,  0.0121],\n",
      "        [ 0.0176, -0.0165, -0.0140,  ...,  0.0164, -0.0054,  0.0072],\n",
      "        [-0.0069, -0.0188, -0.0234,  ..., -0.0126,  0.0204,  0.0032]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 8.5521e-03,  1.8202e-02,  2.1154e-03,  ...,  1.3740e-02,\n",
      "          1.5936e-02, -1.9684e-02],\n",
      "        [-1.6502e-02, -4.1108e-03,  9.0243e-03,  ..., -3.1370e-02,\n",
      "          1.2761e-02,  9.8637e-03],\n",
      "        [-1.6609e-02, -2.4881e-03, -2.0921e-02,  ...,  3.6786e-02,\n",
      "         -1.1809e-02,  1.3086e-02],\n",
      "        ...,\n",
      "        [-1.7858e-02,  3.2186e-02,  3.3721e-02,  ..., -5.2236e-03,\n",
      "          8.1076e-05,  3.1574e-03],\n",
      "        [-2.7774e-02,  5.8988e-03,  1.0924e-02,  ...,  4.3354e-02,\n",
      "          3.2155e-02,  1.3577e-04],\n",
      "        [-1.5062e-02, -5.5237e-04,  1.5028e-02,  ...,  2.1683e-02,\n",
      "         -1.5744e-02,  1.0142e-02]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0187,  0.0225, -0.0501,  ...,  0.0272, -0.0014,  0.0084],\n",
      "        [-0.0100, -0.0165, -0.0176,  ...,  0.0415,  0.0304, -0.0290],\n",
      "        [ 0.0234, -0.0145,  0.0112,  ...,  0.0254,  0.0424, -0.0050],\n",
      "        ...,\n",
      "        [-0.0020, -0.0504,  0.0071,  ...,  0.0033,  0.0269,  0.0080],\n",
      "        [ 0.0209, -0.0071,  0.0028,  ..., -0.0081, -0.0211,  0.0025],\n",
      "        [-0.0023,  0.0396,  0.0310,  ..., -0.0006,  0.0006, -0.0082]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0097,  0.0295, -0.0049,  ...,  0.0118, -0.0166, -0.0270],\n",
      "        [ 0.0166, -0.0182,  0.0520,  ...,  0.0218, -0.0070, -0.0064],\n",
      "        [-0.0068,  0.0174, -0.0051,  ...,  0.0358, -0.0350,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0382, -0.0134, -0.0119,  ..., -0.0246, -0.0102, -0.0373],\n",
      "        [ 0.0107, -0.0302, -0.0104,  ..., -0.0123, -0.0025, -0.0122],\n",
      "        [-0.0101,  0.0152,  0.0276,  ..., -0.0101,  0.0296, -0.0201]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0043,  0.0198,  0.0033,  ...,  0.0231, -0.0229, -0.0394],\n",
      "        [-0.0246,  0.0064, -0.0261,  ..., -0.0035, -0.0023, -0.0144],\n",
      "        [-0.0518, -0.0218, -0.0336,  ...,  0.0430,  0.0047, -0.0175],\n",
      "        ...,\n",
      "        [-0.0375,  0.0207, -0.0050,  ..., -0.0404, -0.0267,  0.0150],\n",
      "        [-0.0100,  0.0025,  0.0282,  ..., -0.0051, -0.0114, -0.0005],\n",
      "        [-0.0133, -0.0268,  0.0179,  ...,  0.0254, -0.0101, -0.0033]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.0108,  0.0103,  0.0191,  ...,  0.0068, -0.0221,  0.0316],\n",
      "        [-0.0399,  0.0238,  0.0195,  ...,  0.0069,  0.0368,  0.0177],\n",
      "        [-0.0035, -0.0449, -0.0047,  ...,  0.0405, -0.0182,  0.0298],\n",
      "        ...,\n",
      "        [-0.0199,  0.0122,  0.0089,  ...,  0.0097,  0.0148,  0.0376],\n",
      "        [ 0.0340, -0.0052,  0.0241,  ...,  0.0038,  0.0120, -0.0172],\n",
      "        [-0.0196, -0.0270,  0.0607,  ...,  0.0104, -0.0093,  0.0206]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0045, -0.0279, -0.0068,  ...,  0.0005,  0.0068, -0.0355],\n",
      "        [-0.0368,  0.0155, -0.0016,  ..., -0.0189,  0.0443, -0.0141],\n",
      "        [ 0.0345, -0.0138,  0.0076,  ...,  0.0184,  0.0175, -0.0052],\n",
      "        ...,\n",
      "        [ 0.0211, -0.0100,  0.0305,  ...,  0.0155, -0.0324,  0.0033],\n",
      "        [ 0.0206, -0.0093, -0.0021,  ..., -0.0159, -0.0017,  0.0061],\n",
      "        [ 0.0222,  0.0126, -0.0173,  ...,  0.0291, -0.0383, -0.0003]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0059,  0.0006, -0.0109,  ...,  0.0043, -0.0146,  0.0112],\n",
      "        [ 0.0007, -0.0223,  0.0171,  ..., -0.0102,  0.0249,  0.0037],\n",
      "        [-0.0368,  0.0134,  0.0166,  ...,  0.0043, -0.0221,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0120, -0.0239,  ...,  0.0006,  0.0120, -0.0229],\n",
      "        [-0.0207,  0.0166,  0.0256,  ...,  0.0569,  0.0070,  0.0101],\n",
      "        [ 0.0094,  0.0044, -0.0165,  ..., -0.0067, -0.0068, -0.0136]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0569,  0.0044,  0.0208,  ...,  0.0037,  0.0183,  0.0079],\n",
      "        [-0.0118,  0.0205, -0.0033,  ...,  0.0042, -0.0255,  0.0211],\n",
      "        [ 0.0292,  0.0006,  0.0185,  ..., -0.0406, -0.0157,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0114,  0.0147, -0.0087,  ...,  0.0078,  0.0163,  0.0231],\n",
      "        [-0.0160,  0.0220,  0.0067,  ..., -0.0059,  0.0112,  0.0130],\n",
      "        [-0.0024,  0.0191,  0.0016,  ...,  0.0060,  0.0034,  0.0033]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0026, -0.0053, -0.0100,  ...,  0.0059, -0.0320,  0.0209],\n",
      "        [ 0.0068, -0.0059, -0.0095,  ..., -0.0239,  0.0011, -0.0283],\n",
      "        [-0.0093,  0.0008, -0.0168,  ...,  0.0333, -0.0032, -0.0187],\n",
      "        ...,\n",
      "        [ 0.0171, -0.0064, -0.0246,  ..., -0.0382,  0.0322,  0.0184],\n",
      "        [-0.0265, -0.0045, -0.0146,  ...,  0.0063,  0.0100,  0.0360],\n",
      "        [ 0.0019, -0.0034, -0.0102,  ...,  0.0584,  0.0061,  0.0296]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0038,  0.0067,  ..., -0.0157, -0.0203, -0.0233],\n",
      "        [-0.0183, -0.0288, -0.0344,  ...,  0.0325, -0.0029, -0.0181],\n",
      "        [ 0.0124,  0.0237,  0.0422,  ...,  0.0115,  0.0038,  0.0152],\n",
      "        ...,\n",
      "        [-0.0141,  0.0037,  0.0158,  ...,  0.0057, -0.0158,  0.0055],\n",
      "        [-0.0360,  0.0049, -0.0341,  ..., -0.0005, -0.0034, -0.0649],\n",
      "        [ 0.0041, -0.0398,  0.0123,  ..., -0.0292, -0.0016,  0.0219]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0072,  0.0169,  0.0320,  ..., -0.0247,  0.0229, -0.0231],\n",
      "        [-0.0266,  0.0045,  0.0162,  ...,  0.0105,  0.0169,  0.0051],\n",
      "        [-0.0009,  0.0103,  0.0001,  ..., -0.0123,  0.0323,  0.0126],\n",
      "        ...,\n",
      "        [-0.0218,  0.0399, -0.0147,  ...,  0.0402,  0.0311,  0.0115],\n",
      "        [ 0.0059, -0.0057,  0.0182,  ...,  0.0024, -0.0216,  0.0319],\n",
      "        [-0.0236,  0.0200,  0.0179,  ..., -0.0029,  0.0158,  0.0312]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0156, -0.0017, -0.0133,  ...,  0.0157,  0.0094,  0.0261],\n",
      "        [ 0.0220,  0.0247, -0.0185,  ..., -0.0384, -0.0210,  0.0075],\n",
      "        [ 0.0232, -0.0066, -0.0006,  ...,  0.0212,  0.0079, -0.0237],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0201, -0.0003,  ...,  0.0310,  0.0241, -0.0095],\n",
      "        [-0.0327,  0.0107, -0.0022,  ...,  0.0117, -0.0158,  0.0117],\n",
      "        [ 0.0035, -0.0309,  0.0221,  ..., -0.0211, -0.0039, -0.0060]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0314, -0.0018, -0.0188,  ..., -0.0023, -0.0280, -0.0049],\n",
      "        [-0.0121, -0.0025, -0.0075,  ...,  0.0290, -0.0297,  0.0033]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n",
      "LoRA Parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0024,  0.0010,  0.0165,  ...,  0.0279,  0.0312, -0.0144],\n",
      "        [-0.0200, -0.0317, -0.0283,  ..., -0.0185,  0.0061, -0.0283],\n",
      "        [-0.0298, -0.0285, -0.0335,  ..., -0.0011,  0.0273,  0.0312],\n",
      "        ...,\n",
      "        [-0.0198, -0.0085,  0.0181,  ...,  0.0021, -0.0057, -0.0242],\n",
      "        [ 0.0101,  0.0250,  0.0142,  ..., -0.0259, -0.0229,  0.0079],\n",
      "        [-0.0096, -0.0017, -0.0355,  ...,  0.0143, -0.0227, -0.0359]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0075,  0.0145,  0.0226,  ...,  0.0253, -0.0359,  0.0181],\n",
      "        [ 0.0106, -0.0224,  0.0310,  ...,  0.0296,  0.0122, -0.0271],\n",
      "        [-0.0146, -0.0282,  0.0111,  ...,  0.0158,  0.0124, -0.0249],\n",
      "        ...,\n",
      "        [-0.0007, -0.0067, -0.0013,  ..., -0.0223, -0.0054,  0.0132],\n",
      "        [ 0.0188, -0.0324, -0.0002,  ...,  0.0237, -0.0064,  0.0179],\n",
      "        [ 0.0354,  0.0090, -0.0174,  ..., -0.0087, -0.0108, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0274,  0.0136,  0.0007,  ...,  0.0167,  0.0226,  0.0218],\n",
      "        [-0.0197,  0.0191, -0.0350,  ...,  0.0022, -0.0231,  0.0209],\n",
      "        [ 0.0275,  0.0304, -0.0049,  ..., -0.0196,  0.0093,  0.0031],\n",
      "        ...,\n",
      "        [ 0.0255, -0.0187,  0.0316,  ..., -0.0131, -0.0095, -0.0278],\n",
      "        [-0.0216, -0.0133, -0.0173,  ...,  0.0100, -0.0052,  0.0013],\n",
      "        [-0.0023,  0.0111, -0.0261,  ..., -0.0121, -0.0224, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0335, -0.0112,  0.0320,  ...,  0.0216,  0.0033, -0.0222],\n",
      "        [ 0.0315, -0.0307,  0.0201,  ...,  0.0327,  0.0064, -0.0079],\n",
      "        [-0.0354, -0.0202,  0.0203,  ..., -0.0209, -0.0202, -0.0132],\n",
      "        ...,\n",
      "        [-0.0226,  0.0133, -0.0309,  ..., -0.0026, -0.0178, -0.0150],\n",
      "        [ 0.0288, -0.0130,  0.0261,  ...,  0.0210, -0.0331,  0.0158],\n",
      "        [-0.0033, -0.0247, -0.0003,  ..., -0.0151, -0.0070, -0.0160]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "Classifier Parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.0156, -0.0017, -0.0133,  ...,  0.0157,  0.0094,  0.0261],\n",
      "        [ 0.0220,  0.0247, -0.0185,  ..., -0.0384, -0.0210,  0.0075],\n",
      "        [ 0.0232, -0.0066, -0.0006,  ...,  0.0212,  0.0079, -0.0237],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0201, -0.0003,  ...,  0.0310,  0.0241, -0.0095],\n",
      "        [-0.0327,  0.0107, -0.0022,  ...,  0.0117, -0.0158,  0.0117],\n",
      "        [ 0.0035, -0.0309,  0.0221,  ..., -0.0211, -0.0039, -0.0060]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0314, -0.0018, -0.0188,  ..., -0.0023, -0.0280, -0.0049],\n",
      "        [-0.0121, -0.0025, -0.0075,  ...,  0.0290, -0.0297,  0.0033]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "pretrained_params = [p for name, p in finetune_model.named_parameters() if 'lora' not in name]\n",
    "lora_params = list(finetune_model.lora_parameters())\n",
    "classifier_params = list(finetune_model.classifier_parameters())\n",
    "\n",
    "print(\"Pretrained Parameters:\")\n",
    "for param in pretrained_params:\n",
    "    print(param)\n",
    "\n",
    "print(\"\\nLoRA Parameters:\")\n",
    "for param in lora_params:\n",
    "    print(param)\n",
    "\n",
    "print(\"\\nClassifier Parameters:\")\n",
    "for param in classifier_params:\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, args, n_steps):\n",
    "    base_lr = args.learning_rate  \n",
    "    lora_lr = args.learning_rate * args.lora_weight\n",
    "    classifier_lr = args.learning_rate * args.lora_weight\n",
    "    \n",
    "    pretrained_params = [p for name, p in model.named_parameters() if 'lora' not in name]\n",
    "    lora_params = list(model.lora_parameters())\n",
    "    classifier_params = list(model.classifier_parameters())\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': pretrained_params, 'lr': base_lr},  \n",
    "        {'params': lora_params, 'lr': lora_lr},  \n",
    "        {'params': classifier_params, 'lr': classifier_lr}, \n",
    "    ])\n",
    "\n",
    "    n_warmup_steps = int(n_steps * 0.1)\n",
    "    n_decay_steps = n_steps - n_warmup_steps\n",
    "\n",
    "    warmup = LinearLR(optimizer, \n",
    "                        start_factor=0.01,\n",
    "                        end_factor=1.0,\n",
    "                        total_iters=n_warmup_steps)\n",
    "    \n",
    "    decay = LinearLR(optimizer,\n",
    "                        start_factor=1.0,\n",
    "                        end_factor=0.01,\n",
    "                        total_iters=n_decay_steps)\n",
    "    \n",
    "    scheduler = SequentialLR(optimizer, \n",
    "                                schedulers=[warmup, decay],\n",
    "                                milestones=[n_warmup_steps])\n",
    "\n",
    "    return optimizer, {\"scheduler\": scheduler, \"interval\": \"step\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0271,  0.0132, -0.0369,  ...,  0.0185,  0.0606, -0.0114],\n",
      "        [-0.0268,  0.0478, -0.0033,  ..., -0.0049, -0.0060, -0.0140],\n",
      "        [ 0.0206, -0.0008, -0.0173,  ..., -0.0130, -0.0201,  0.0082],\n",
      "        ...,\n",
      "        [-0.0036, -0.0187,  0.0137,  ...,  0.0163,  0.0063, -0.0082],\n",
      "        [ 0.0373,  0.0002,  0.0166,  ...,  0.0053,  0.0242, -0.0200],\n",
      "        [ 0.0268,  0.0062, -0.0194,  ..., -0.0015,  0.0178, -0.0536]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0071, -0.0145, -0.0272,  ..., -0.0159, -0.0029,  0.0082],\n",
      "        [-0.0306, -0.0028, -0.0202,  ..., -0.0249,  0.0230,  0.0229]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0057, -0.0384,  0.0385,  ..., -0.0010,  0.0101, -0.0095],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0408, -0.0198,  0.0112,  ..., -0.0027, -0.0019, -0.0039],\n",
      "        ...,\n",
      "        [ 0.0092, -0.0146,  0.0118,  ...,  0.0023, -0.0028, -0.0028],\n",
      "        [ 0.0100,  0.0067, -0.0100,  ..., -0.0228, -0.0073, -0.0026],\n",
      "        [ 0.0001,  0.0304,  0.0259,  ..., -0.0137,  0.0071,  0.0142]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0082,  0.0026,  0.0134,  ..., -0.0510,  0.0219,  0.0065],\n",
      "        [ 0.0102,  0.0018,  0.0193,  ...,  0.0227,  0.0027, -0.0196],\n",
      "        [-0.0010,  0.0360, -0.0039,  ..., -0.0019, -0.0342,  0.0142],\n",
      "        ...,\n",
      "        [ 0.0246, -0.0151,  0.0390,  ...,  0.0063, -0.0154,  0.0030],\n",
      "        [-0.0079, -0.0214, -0.0082,  ..., -0.0078,  0.0155, -0.0107],\n",
      "        [ 0.0158, -0.0162, -0.0140,  ...,  0.0116, -0.0171,  0.0061]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0029,  0.0132, -0.0331,  ..., -0.0081, -0.0123,  0.0131],\n",
      "        [ 0.0034, -0.0036, -0.0257,  ..., -0.0109, -0.0086, -0.0025],\n",
      "        [-0.0129, -0.0141, -0.0262,  ...,  0.0343, -0.0156, -0.0272],\n",
      "        ...,\n",
      "        [ 0.0231,  0.0330,  0.0244,  ..., -0.0335,  0.0207,  0.0209],\n",
      "        [-0.0322, -0.0071, -0.0215,  ...,  0.0015,  0.0057, -0.0338],\n",
      "        [ 0.0073,  0.0022, -0.0042,  ...,  0.0356,  0.0167, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0238, -0.0268,  0.0084,  ...,  0.0009, -0.0418,  0.0257],\n",
      "        [ 0.0320, -0.0373, -0.0182,  ...,  0.0032,  0.0063, -0.0156],\n",
      "        [ 0.0064, -0.0043,  0.0132,  ..., -0.0052,  0.0188,  0.0058],\n",
      "        ...,\n",
      "        [-0.0010, -0.0348, -0.0218,  ...,  0.0009,  0.0337,  0.0071],\n",
      "        [ 0.0235, -0.0247,  0.0065,  ...,  0.0018, -0.0051,  0.0242],\n",
      "        [-0.0059,  0.0201,  0.0175,  ..., -0.0153,  0.0088,  0.0081]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 2.8246e-02,  3.2938e-02, -6.8719e-03,  ..., -9.0345e-04,\n",
      "          8.4727e-03,  1.1327e-02],\n",
      "        [-1.7640e-02, -1.8379e-02,  3.5515e-02,  ..., -2.7380e-02,\n",
      "         -1.3887e-02, -4.8511e-02],\n",
      "        [-2.5560e-02,  1.6885e-02,  3.1564e-02,  ...,  2.3159e-02,\n",
      "         -1.5956e-02, -2.6523e-02],\n",
      "        ...,\n",
      "        [ 5.6544e-02, -2.6228e-02, -2.6379e-05,  ..., -2.4326e-04,\n",
      "          2.0445e-02,  1.5658e-02],\n",
      "        [-1.3659e-02, -1.5653e-02,  5.6928e-03,  ..., -3.5445e-02,\n",
      "          3.0079e-02,  1.9533e-02],\n",
      "        [-3.1707e-02,  2.9281e-02, -1.4143e-02,  ..., -3.0435e-02,\n",
      "         -1.4914e-02, -4.2063e-02]], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0011,  0.0094, -0.0280,  ...,  0.0197, -0.0286,  0.0064],\n",
      "        [ 0.0298,  0.0164,  0.0328,  ...,  0.0028, -0.0015, -0.0318],\n",
      "        [-0.0251,  0.0282,  0.0320,  ...,  0.0078, -0.0010,  0.0116],\n",
      "        ...,\n",
      "        [-0.0186, -0.0341,  0.0081,  ...,  0.0289,  0.0023,  0.0072],\n",
      "        [ 0.0294,  0.0109,  0.0355,  ..., -0.0242, -0.0011, -0.0337],\n",
      "        [ 0.0329, -0.0026,  0.0248,  ..., -0.0044, -0.0348,  0.0139]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0031, -0.0366,  0.0206,  ...,  0.0088, -0.0137,  0.0020],\n",
      "        [-0.0184,  0.0064, -0.0247,  ...,  0.0078,  0.0197, -0.0090],\n",
      "        [ 0.0109, -0.0202, -0.0081,  ...,  0.0013, -0.0177, -0.0197],\n",
      "        ...,\n",
      "        [ 0.0024,  0.0252,  0.0212,  ...,  0.0291,  0.0090,  0.0093],\n",
      "        [-0.0040, -0.0289,  0.0342,  ...,  0.0336, -0.0015,  0.0216],\n",
      "        [ 0.0055, -0.0089, -0.0260,  ...,  0.0199, -0.0048,  0.0119]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0221, -0.0072,  0.0046,  ...,  0.0315, -0.0155,  0.0096],\n",
      "        [-0.0091,  0.0102,  0.0047,  ...,  0.0088,  0.0037, -0.0226],\n",
      "        [-0.0091, -0.0167, -0.0283,  ...,  0.0235, -0.0145, -0.0161],\n",
      "        ...,\n",
      "        [ 0.0056, -0.0062, -0.0106,  ..., -0.0062,  0.0151, -0.0242],\n",
      "        [-0.0134, -0.0045,  0.0383,  ..., -0.0336, -0.0014, -0.0166],\n",
      "        [-0.0169,  0.0239, -0.0262,  ...,  0.0055,  0.0202,  0.0143]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0077,  0.0295,  0.0076,  ...,  0.0109,  0.0040,  0.0165],\n",
      "        [-0.0143, -0.0266,  0.0164,  ..., -0.0060, -0.0379, -0.0022],\n",
      "        [ 0.0189,  0.0183,  0.0237,  ...,  0.0105, -0.0301,  0.0323],\n",
      "        ...,\n",
      "        [-0.0051,  0.0131, -0.0007,  ..., -0.0043,  0.0020, -0.0049],\n",
      "        [ 0.0034, -0.0152,  0.0130,  ..., -0.0281,  0.0035,  0.0216],\n",
      "        [-0.0307,  0.0249, -0.0008,  ..., -0.0090, -0.0151,  0.0145]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0094,  0.0260,  0.0001,  ...,  0.0200,  0.0126,  0.0065],\n",
      "        [-0.0038, -0.0284, -0.0005,  ..., -0.0144,  0.0330,  0.0052],\n",
      "        [-0.0203,  0.0032, -0.0179,  ...,  0.0074, -0.0119,  0.0048],\n",
      "        ...,\n",
      "        [ 0.0056,  0.0107,  0.0076,  ...,  0.0014, -0.0229, -0.0024],\n",
      "        [ 0.0178,  0.0134,  0.0129,  ...,  0.0016,  0.0019,  0.0274],\n",
      "        [ 0.0044,  0.0152,  0.0191,  ..., -0.0354, -0.0022, -0.0198]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0243, -0.0102,  0.0010,  ...,  0.0480, -0.0337,  0.0101],\n",
      "        [-0.0016, -0.0041,  0.0324,  ...,  0.0185,  0.0137, -0.0099],\n",
      "        [ 0.0123, -0.0048, -0.0009,  ..., -0.0267, -0.0018, -0.0132],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0014, -0.0104,  ..., -0.0139,  0.0004, -0.0205],\n",
      "        [-0.0128, -0.0099,  0.0169,  ..., -0.0038,  0.0217,  0.0127],\n",
      "        [-0.0027,  0.0357,  0.0096,  ..., -0.0169, -0.0092, -0.0039]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0160, -0.0206, -0.0070,  ..., -0.0160, -0.0156,  0.0267],\n",
      "        [-0.0155, -0.0492,  0.0164,  ...,  0.0063, -0.0038,  0.0153],\n",
      "        [ 0.0247,  0.0112, -0.0024,  ..., -0.0017,  0.0020,  0.0111],\n",
      "        ...,\n",
      "        [-0.0247,  0.0082, -0.0091,  ..., -0.0140,  0.0010, -0.0209],\n",
      "        [-0.0148,  0.0247,  0.0073,  ...,  0.0142, -0.0162,  0.0013],\n",
      "        [-0.0027,  0.0027, -0.0075,  ...,  0.0056,  0.0345, -0.0119]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0329, -0.0016, -0.0260,  ..., -0.0028,  0.0038,  0.0282],\n",
      "        [ 0.0051, -0.0199,  0.0220,  ...,  0.0199,  0.0301,  0.0156],\n",
      "        [-0.0218, -0.0180, -0.0105,  ..., -0.0294, -0.0260,  0.0369],\n",
      "        ...,\n",
      "        [-0.0601, -0.0005, -0.0194,  ...,  0.0262,  0.0102,  0.0120],\n",
      "        [ 0.0335,  0.0105, -0.0175,  ...,  0.0170,  0.0005, -0.0431],\n",
      "        [-0.0058,  0.0190,  0.0014,  ..., -0.0376,  0.0224, -0.0019]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0125, -0.0264,  0.0329,  ...,  0.0353, -0.0204, -0.0040],\n",
      "        [ 0.0340, -0.0248, -0.0154,  ...,  0.0034, -0.0171, -0.0066],\n",
      "        [-0.0320,  0.0081, -0.0074,  ...,  0.0199,  0.0200, -0.0327],\n",
      "        ...,\n",
      "        [-0.0039, -0.0274, -0.0048,  ...,  0.0145,  0.0228,  0.0336],\n",
      "        [ 0.0014,  0.0255,  0.0058,  ...,  0.0259,  0.0042, -0.0005],\n",
      "        [ 0.0144, -0.0292, -0.0118,  ..., -0.0076, -0.0304,  0.0144]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0129,  0.0260, -0.0216,  ..., -0.0005,  0.0093,  0.0548],\n",
      "        [ 0.0574,  0.0175, -0.0439,  ...,  0.0108,  0.0145, -0.0049],\n",
      "        [ 0.0270,  0.0101,  0.0042,  ...,  0.0103,  0.0413,  0.0017],\n",
      "        ...,\n",
      "        [ 0.0151, -0.0198, -0.0165,  ..., -0.0246,  0.0253, -0.0082],\n",
      "        [-0.0344,  0.0041,  0.0167,  ..., -0.0330, -0.0176,  0.0076],\n",
      "        [-0.0022,  0.0193,  0.0209,  ...,  0.0179, -0.0098,  0.0285]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0236, -0.0009,  0.0292,  ...,  0.0134,  0.0046, -0.0031],\n",
      "        [-0.0245,  0.0214,  0.0161,  ..., -0.0123,  0.0122,  0.0066],\n",
      "        [-0.0100, -0.0293, -0.0134,  ...,  0.0231,  0.0112, -0.0036],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0070,  0.0319,  ..., -0.0153, -0.0088, -0.0311],\n",
      "        [-0.0051,  0.0147, -0.0180,  ..., -0.0142,  0.0278, -0.0071],\n",
      "        [ 0.0031, -0.0035, -0.0028,  ..., -0.0062,  0.0002, -0.0033]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0199,  0.0273,  0.0007,  ..., -0.0289, -0.0105,  0.0042],\n",
      "        [-0.0189,  0.0212,  0.0102,  ...,  0.0034, -0.0295,  0.0082],\n",
      "        [ 0.0258, -0.0061,  0.0316,  ..., -0.0236, -0.0349, -0.0301],\n",
      "        ...,\n",
      "        [-0.0150, -0.0140, -0.0013,  ..., -0.0181, -0.0032,  0.0284],\n",
      "        [-0.0138,  0.0020, -0.0346,  ...,  0.0074, -0.0222, -0.0176],\n",
      "        [-0.0037,  0.0035,  0.0217,  ...,  0.0131,  0.0352,  0.0042]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.5587e-02, -3.1238e-02,  8.0434e-03,  ..., -1.7894e-02,\n",
      "         -3.1731e-02,  1.4662e-02],\n",
      "        [ 4.6421e-04,  2.2573e-02, -1.9837e-02,  ...,  1.5914e-02,\n",
      "         -5.4607e-03, -8.7907e-03],\n",
      "        [-7.1242e-03, -1.8948e-02,  4.4769e-05,  ..., -1.3957e-02,\n",
      "         -1.1081e-02, -1.1937e-02],\n",
      "        ...,\n",
      "        [ 6.0005e-03, -4.5031e-03,  3.3220e-03,  ...,  1.8620e-02,\n",
      "         -2.0623e-02, -3.8566e-03],\n",
      "        [ 2.2404e-02,  2.5856e-02, -2.4093e-02,  ...,  1.6144e-02,\n",
      "         -1.1660e-02,  8.5554e-03],\n",
      "        [-1.9406e-02,  4.9204e-03, -3.1098e-02,  ...,  9.6701e-03,\n",
      "          3.6949e-03,  1.4215e-02]], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0122,  0.0203,  0.0322,  ...,  0.0184, -0.0013,  0.0301],\n",
      "        [ 0.0336,  0.0116, -0.0212,  ..., -0.0248,  0.0279,  0.0063],\n",
      "        [ 0.0245, -0.0004,  0.0034,  ..., -0.0205,  0.0090,  0.0166],\n",
      "        ...,\n",
      "        [ 0.0387,  0.0089, -0.0003,  ...,  0.0065, -0.0145, -0.0264],\n",
      "        [-0.0059, -0.0374,  0.0365,  ..., -0.0078,  0.0015, -0.0063],\n",
      "        [ 0.0060,  0.0055, -0.0137,  ..., -0.0130, -0.0444, -0.0062]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0060, -0.0063, -0.0011,  ...,  0.0228, -0.0112,  0.0067],\n",
      "        [-0.0106,  0.0086,  0.0150,  ...,  0.0104,  0.0057,  0.0197],\n",
      "        [-0.0099, -0.0175, -0.0139,  ...,  0.0026, -0.0255,  0.0117],\n",
      "        ...,\n",
      "        [-0.0400,  0.0045,  0.0160,  ...,  0.0060, -0.0133, -0.0361],\n",
      "        [-0.0119, -0.0057,  0.0326,  ...,  0.0216,  0.0136,  0.0043],\n",
      "        [-0.0133,  0.0240,  0.0280,  ...,  0.0050,  0.0439, -0.0193]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[-0.0138, -0.0186,  0.0120,  ...,  0.0124,  0.0012, -0.0069],\n",
      "        [ 0.0051, -0.0113, -0.0015,  ...,  0.0053,  0.0138, -0.0094],\n",
      "        [-0.0571, -0.0027,  0.0097,  ...,  0.0008, -0.0282,  0.0129],\n",
      "        ...,\n",
      "        [ 0.0084,  0.0057, -0.0079,  ...,  0.0164,  0.0289, -0.0149],\n",
      "        [-0.0148, -0.0363, -0.0141,  ..., -0.0117, -0.0250, -0.0181],\n",
      "        [ 0.0124, -0.0179, -0.0158,  ..., -0.0142,  0.0154,  0.0366]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0114, -0.0081, -0.0010,  ...,  0.0225, -0.0028,  0.0062],\n",
      "        [ 0.0208, -0.0077, -0.0230,  ...,  0.0335, -0.0073,  0.0205],\n",
      "        [ 0.0001, -0.0106,  0.0053,  ...,  0.0331,  0.0202,  0.0267],\n",
      "        ...,\n",
      "        [ 0.0034, -0.0310,  0.0059,  ...,  0.0090,  0.0049, -0.0078],\n",
      "        [ 0.0070, -0.0053, -0.0065,  ...,  0.0066, -0.0021, -0.0167],\n",
      "        [-0.0885,  0.0041, -0.0087,  ..., -0.0110, -0.0096, -0.0153]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), Parameter containing:\n",
      "tensor([[ 0.0116, -0.0121, -0.0136,  ...,  0.0119, -0.0050,  0.0138],\n",
      "        [-0.0057,  0.0100,  0.0287,  ..., -0.0256, -0.0032, -0.0205],\n",
      "        [-0.0216, -0.0074, -0.0046,  ...,  0.0217, -0.0081,  0.0233],\n",
      "        ...,\n",
      "        [ 0.0294, -0.0493,  0.0382,  ..., -0.0036, -0.0220, -0.0485],\n",
      "        [-0.0143,  0.0006, -0.0223,  ..., -0.0031, -0.0237, -0.0036],\n",
      "        [ 0.0050,  0.0036,  0.0064,  ...,  0.0026,  0.0208, -0.0319]],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0'), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(list(finetune_model.pretrained_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641282"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(finetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are identical, pretrained weights have been correctly loaded.\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 모델과 파인튜닝 모델의 동일한 레이어의 가중치 비교\n",
    "pretrained_weight = pretrained_model.embeddings.concept_embedding.procedure_embedding.weight\n",
    "finetune_weight = finetune_model.embeddings.concept_embedding.procedure_embedding.weight\n",
    "\n",
    "# 두 가중치가 동일한지 확인\n",
    "if torch.allclose(pretrained_weight, finetune_weight):\n",
    "    print(\"Weights are identical, pretrained weights have been correctly loaded.\")\n",
    "else:\n",
    "    print(\"Weights are different, there may be an issue with loading pretrained weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully.\n",
      "Applying LoRA\n",
      "Train Parameters: 887042\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import traceback\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "# seed_everything(args.seed)\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "itemid2idx = pd.read_pickle(\"datasets/entire_itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "\n",
    "def configure_optimizers(model, args, n_steps):\n",
    "    base_lr = args.learning_rate  \n",
    "    lora_lr = args.learning_rate * args.lora_weight\n",
    "    classifier_lr = args.learning_rate * args.classifier_weight\n",
    "    \n",
    "    pretrained_params = model.pretrained_parameters()\n",
    "    lora_params = model.lora_parameters()\n",
    "    classifier_params = model.classifier_parameters()\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': pretrained_params, 'lr': base_lr, 'weight_decay': 0.01},\n",
    "        {'params': lora_params, 'lr': lora_lr, 'weight_decay': 0.001},\n",
    "        {'params': classifier_params, 'lr': classifier_lr, 'weight_decay': 0.001}\n",
    "    ])\n",
    "\n",
    "\n",
    "    # CosineAnnealingWarmRestarts \n",
    "    # scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
    "    #                                           first_cycle_steps=538,\n",
    "    #                                           cycle_mult=2,\n",
    "    #                                           max_lr=0.0001,\n",
    "    #                                           min_lr=0.000001,\n",
    "    #                                           warmup_steps=54,\n",
    "    #                                           gamma=0.5\n",
    "    #                                           )\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer,\n",
    "    #                                     T_0=538,       \n",
    "    #                                     T_mult=2,     \n",
    "    #                                    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    return optimizer, {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "def calculate_alpha(dataset, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate the alpha values based on the class distribution in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The dataset to calculate class distribution from.\n",
    "    - num_classes: The total number of classes in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - alpha: Tensor containing the alpha values for each class.\n",
    "    \"\"\"\n",
    "    # Get all targets in the dataset\n",
    "    all_targets = [label.item() for *_, label in dataset]\n",
    "\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = Counter(all_targets)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_count = sum(class_counts.values())\n",
    "\n",
    "    # Calculate alpha as the inverse of the class frequency\n",
    "    alpha = torch.zeros(num_classes)\n",
    "    for cls, count in class_counts.items():\n",
    "        alpha[cls] = total_count / (num_classes * count)\n",
    "    \n",
    "    # Normalize alpha to sum to 1\n",
    "    alpha = alpha / alpha.sum()\n",
    "\n",
    "    return alpha\n",
    "class CustomSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.labels = [self.dataset[idx][-1].item() for idx in range(len(self.dataset))]\n",
    "        \n",
    "   \n",
    "        self.class_indices = {label: np.where(np.array(self.labels) == label)[0].tolist() for label in np.unique(self.labels)}\n",
    "        \n",
    " \n",
    "        self.class_probs = {label: len(self.class_indices[label]) / len(self.dataset) for label in np.unique(self.labels)}\n",
    "        \n",
    "    \n",
    "        self.min_class_1_per_batch = 1\n",
    "        self.class_1_indices = self.class_indices[1]\n",
    "        self.other_class_indices_template = {label: self.class_indices[label] for label in self.class_indices if label != 1}\n",
    "\n",
    "   \n",
    "        self.indices = []\n",
    "\n",
    "    def _generate_indices(self):\n",
    "        indices = []\n",
    "        num_batches = len(self.dataset) // self.batch_size\n",
    "        \n",
    "      \n",
    "        class_1_per_batch = np.array_split(self.class_1_indices, num_batches)\n",
    "\n",
    "        other_class_indices = {label: indices_list[:] for label, indices_list in self.other_class_indices_template.items()}\n",
    "        \n",
    "        for batch_num in range(num_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "        \n",
    "            batch_indices.extend(class_1_per_batch[batch_num])\n",
    "            \n",
    "      \n",
    "            remaining_batch_size = self.batch_size - len(batch_indices)\n",
    "            other_samples = []\n",
    "            \n",
    "            for label, indices_list in other_class_indices.items():\n",
    "                if len(indices_list) > 0:\n",
    "                \n",
    "                    if remaining_batch_size > len(indices_list):\n",
    "                        selected_indices = indices_list\n",
    "                    else:\n",
    "                        selected_indices = np.random.choice(indices_list, remaining_batch_size, replace=False).tolist()\n",
    "                    \n",
    "                    other_samples.extend(selected_indices)\n",
    "                    \n",
    "                  \n",
    "                    other_class_indices[label] = [idx for idx in indices_list if idx not in selected_indices]\n",
    "            \n",
    "            np.random.shuffle(other_samples)\n",
    "            batch_indices.extend(other_samples[:remaining_batch_size])\n",
    "        \n",
    "            np.random.shuffle(batch_indices)\n",
    "            indices.extend(batch_indices)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def __iter__(self):\n",
    "   \n",
    "        self.indices = self._generate_indices()\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "pretrained_model = LongformerPretrainNormal(\n",
    "        vocab_size=args.vocab_size,\n",
    "        itemid_size=args.itemid_size,\n",
    "        max_position_embeddings=args.max_position_embeddings,\n",
    "        unit_size=args.unit_size,\n",
    "        continuous_size=args.continuous_size,\n",
    "        task_size=args.task_size,\n",
    "        max_age=args.max_age,\n",
    "        gender_size=args.gender_size,\n",
    "        embedding_size=args.embedding_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        dropout_prob=args.dropout_prob,\n",
    "        gpu_mixed_precision=args.gpu_mixed_precision,\n",
    "    ).to(args.device)\n",
    "pretrain_path = os.path.join(\"./results/\", args.pretrain_path)\n",
    "checkpoint = torch.load(pretrain_path, map_location=args.device, weights_only=True)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.module.'):\n",
    "        new_state_dict[k[14:]] = v  \n",
    "    elif k.startswith('module.'):\n",
    "        new_state_dict[k[7:]] = v \n",
    "    else:\n",
    "        new_state_dict[k] = v  \n",
    "filtered_state_dict = {k: v for k, v in new_state_dict.items() if 'task_embedding' not in k}\n",
    "\n",
    "pretrained_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "print(\"Pre-trained model loaded successfully.\")\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, torch.nn.Embedding):\n",
    "        init.xavier_uniform_(module.weight.data) \n",
    "        \n",
    "\n",
    "pretrained_model.embeddings.task_embedding.apply(initialize_weights)\n",
    "peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,  # Assuming this is for sequence classification\n",
    "            inference_mode=False,  # Set to True if using for inference only\n",
    "            r=8,  # Rank of the low-rank matrices\n",
    "            lora_alpha=16,  # Scaling factor for the low-rank matrices\n",
    "            lora_dropout=args.lora_dropout,  # Dropout probability for LoRA layers\n",
    "            target_modules=[\"query\", \"value\"], # Target attention layers\n",
    ")\n",
    "pretrained_model = get_peft_model(pretrained_model, peft_config)\n",
    "print(\"Applying LoRA\")\n",
    "\n",
    "model = LongformerFinetune(\n",
    "        pretrained_model=pretrained_model,\n",
    "        problem_type=\"single_label_classification\",\n",
    "        num_labels=2,\n",
    "        learning_rate=args.learning_rate,\n",
    "        classifier_dropout=args.classifier_dropout,\n",
    "        use_lora=args.use_lora,\n",
    "    ).to(args.device)\n",
    "print(f\"Train Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "# 데이터셋 생성\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "valid_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"valid\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "custom_sampler = CustomSampler(train_dataset, batch_size=args.batch_size)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            sampler=custom_sampler,  # shuffle should be False if using DistributedSampler\n",
    "                            # shuffle=True,\n",
    "                            pin_memory=args.pin_memory, \n",
    "                            num_workers=args.num_workers,\n",
    "                            )\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                            batch_size=args.batch_size, \n",
    "                            shuffle=False,  # Validation should not be shuffled\n",
    "                            pin_memory=args.pin_memory, \n",
    "                            num_workers=args.num_workers,\n",
    "                            )\n",
    "\n",
    "class_weights = calculate_alpha(train_dataset, args.num_labels)\n",
    "\n",
    "# 샘플링에 사용할 레이블의 인덱스를 찾기 위해 데이터셋의 레이블이 포함된 인덱스 확인\n",
    "labels = [train_dataset[i][-1].item() for i in range(len(train_dataset))]\n",
    "\n",
    "# 질병 샘플(1)의 비율을 정하거나, 일정 수로 지정\n",
    "num_samples_1 = sum(labels)  # 예시: 질병 샘플의 수로 설정\n",
    "target = 1  # 질병 레이블 (1)\n",
    "\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=16,\n",
    "                        sampler = CustomSampler(train_dataset, batch_size=16), \n",
    "                        # shuffle=True,\n",
    "                          pin_memory=args.pin_memory, \n",
    "                          num_workers=args.num_workers)\n",
    "print(len(train_loader))\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(args.device))\n",
    "\n",
    "n_steps = (len(train_dataset) // args.batch_size) * args.epochs\n",
    "optimizer, scheduler = configure_optimizers(model, args, n_steps)\n",
    "\n",
    "model, optimizer, train_loader, valid_loader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_loader, valid_loader, scheduler['scheduler']\n",
    "    )\n",
    "# 배치 처리\n",
    "# ids_0 = 0\n",
    "# ids_1 = 0\n",
    "# ids_2 = 0\n",
    "# ids_3 = 0\n",
    "# for epoch in tqdm(range(0, 30)):\n",
    "#     for step, batch in tqdm(enumerate(train_loader), desc=\"Steps\", total=len(train_loader)):\n",
    "#         batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "#         input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "#         # print(torch.sum(labels).item())\n",
    "#         if step == 0:\n",
    "#             print(labels)\n",
    "#         if torch.sum(labels).item() == 0:\n",
    "#             ids_0 += 1\n",
    "#         elif torch.sum(labels).item() == 1:\n",
    "#             ids_1 += 1\n",
    "#         elif torch.sum(labels).item() == 2:\n",
    "#             ids_2 += 1\n",
    "#         elif torch.sum(labels).item() == 3:\n",
    "#             ids_3 += 1  \n",
    "        \n",
    "\n",
    "# print(ids_0, ids_1, ids_2, ids_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5644674711437565, 1: 4.377924720244151}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    labels = [label.item() for *_, label in dataset]\n",
    "    \n",
    "\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_weights[class_idx] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    return class_weights\n",
    "class_weights = calculate_class_weights(train_dataset)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(class_weights\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: v \u001b[38;5;241m/\u001b[39m total \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m class_weights\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 17\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m calculate_class_weights(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_weights)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m([class_weights[\u001b[38;5;241m0\u001b[39m], class_weights[\u001b[38;5;241m1\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    labels = [label.item() for *_, label in dataset]\n",
    "    \n",
    "\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_weights[class_idx] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    return class_weights\n",
    "def normalize_class_weights(class_weights):\n",
    "    total = sum(class_weights.values())\n",
    "    return {k: v / total for k, v in class_weights.items()}\n",
    "\n",
    "class_weights = calculate_class_weights(train_dataset)\n",
    "print(class_weights)\n",
    "print([class_weights[0], class_weights[1]])\n",
    "normalized_class_weights = normalize_class_weights(class_weights)\n",
    "\n",
    "# Focal Loss의 alpha로 사용하기 위해 리스트로 변환\n",
    "alpha = [normalized_class_weights[0], normalized_class_weights[1]]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Normalized class weights: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0866,  0.0131],\n",
       "        [-0.2563, -0.1538],\n",
       "        [ 0.2018,  0.0390],\n",
       "        [-0.0540,  0.0140],\n",
       "        [-0.0110,  0.3871],\n",
       "        [ 0.0810,  0.2139],\n",
       "        [-0.2117,  0.3357],\n",
       "        [-0.1501, -0.4084]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels): \n",
    "    probs = F.softmax(predictions, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    labels = labels.view(-1) \n",
    "    \n",
    "    preds_np = preds.cpu().detach().numpy()\n",
    "    labels_np = labels.cpu().detach().numpy()\n",
    "    probs_np = probs[:, 1].cpu().detach().numpy()\n",
    "    \n",
    "    # Debugging: Check prediction and label distribution\n",
    "    print(\"Predicted labels:\", np.unique(preds_np, return_counts=True))\n",
    "    print(\"True labels:\", np.unique(labels_np, return_counts=True))\n",
    "    \n",
    "    precision = precision_score(labels_np, preds_np, zero_division=0)\n",
    "    recall = recall_score(labels_np, preds_np, zero_division=0)\n",
    "    f1 = f1_score(labels_np, preds_np, zero_division=0)\n",
    "    \n",
    "    if len(np.unique(labels_np)) > 1:\n",
    "        auroc = roc_auc_score(labels_np, probs_np)\n",
    "        auprc = average_precision_score(labels_np, probs_np)\n",
    "    else:\n",
    "        # If only one class is present, return default values\n",
    "        auroc = 0.5  # Equivalent to random guessing\n",
    "        auprc = 0  # For imbalanced data, AUPRC is often similar to precision\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: (array([0, 1]), array([3, 5]))\n",
      "True labels: (array([0, 1]), array([4, 3]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 15\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(preds_np, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(labels_np, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 15\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(labels_np, preds_np, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(labels_np, preds_np, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2204\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2038\u001b[0m     {\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2065\u001b[0m ):\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \n\u001b[1;32m   2068\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2204\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1561\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1561\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 8]"
     ]
    }
   ],
   "source": [
    "calculate_metrics(logits.view(-1, 2), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   5%|▍         | 51/1076 [00:00<00:06, 162.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  40%|███▉      | 429/1076 [00:04<00:07, 92.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|██████████| 1076/1076 [00:11<00:00, 91.64it/s] \n"
     ]
    }
   ],
   "source": [
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# DataLoader에 샘플러 적용\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=args.batch_size,\n",
    "                          sampler=sampler,  # shuffle=False, shuffle is controlled by sampler\n",
    "                          pin_memory=args.pin_memory, \n",
    "                          num_workers=args.num_workers,\n",
    "                          )\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(train_loader), desc=\"Steps\", total=len(train_loader)):\n",
    "            \n",
    "    batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "    input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "\n",
    "    if sum(labels).item() == 0:\n",
    "        print(\"0\")\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    labels = [label.item() for *_, label in dataset]\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    sample_weights = {cls: count / total_samples for cls, count in class_counts.items()}\n",
    "    sample_weights = [sample_weights[label.item()] for *_, label in dataset]\n",
    "    sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_weights[class_idx] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    return class_weights, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.23984442523768368, 1: 1.0}\n",
      "tensor([0.2398, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    from collections import Counter\n",
    "    \n",
    "    labels = [label.item() for *_, label in dataset]\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_weights[class_idx] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    # Normalize class weights so that the maximum weight is 1\n",
    "    max_weight = max(class_weights.values())\n",
    "    normalized_class_weights = {k: v / max_weight for k, v in class_weights.items()}\n",
    "    \n",
    "    return normalized_class_weights\n",
    "\n",
    "# Example usage\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "class_weights = calculate_class_weights(train_dataset)\n",
    "print(class_weights)\n",
    "weights = torch.tensor([class_weights[i] for i in range(len(class_weights))], dtype=torch.float32).to(args.device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    # 레이블 추출\n",
    "    labels = [label.item() for *_, label in dataset]\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # 클래스 가중치 계산\n",
    "    class_weights = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_weights[class_idx] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    # 클래스 가중치의 반대 비율로 샘플 가중치 설정\n",
    "    max_weight = max(class_weights.values())\n",
    "    inverse_class_weights = {cls: max_weight / class_weights[cls] for cls in class_weights}\n",
    "    \n",
    "    # 각 샘플의 가중치 설정\n",
    "    sample_weights = [inverse_class_weights[label.item()] for *_, label in dataset]\n",
    "    sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "    \n",
    "    return class_weights, sample_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6942, 1665])\n",
      "tensor([0.0001, 0.0006])\n",
      "tensor([0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001])\n"
     ]
    }
   ],
   "source": [
    "train_labels = [label.item() for *_, label in train_dataset]\n",
    "train_class_counts = torch.bincount(torch.tensor(train_labels))\n",
    "print(train_class_counts)\n",
    "train_class_weights = 1.0 / train_class_counts.float()\n",
    "print(train_class_weights)\n",
    "train_sample_weights = train_class_weights[torch.tensor(train_labels)]\n",
    "print(train_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6199222126188418, 1: 2.5846846846846847}\n",
      "tensor([4.1694, 4.1694, 4.1694,  ..., 4.1694, 4.1694, 4.1694])\n"
     ]
    }
   ],
   "source": [
    "class_weights, sample_weights = calculate_class_weights(train_dataset)\n",
    "print(class_weights)\n",
    "print(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1694, 4.1694, 4.1694, 4.1694, 1.0000])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "itemid2idx = pd.read_pickle(\"datasets/entire_itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "\n",
    "\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "\n",
    "# 각 클래스의 빈도 계산\n",
    "train_labels = [label.item() for *_, label in train_dataset]\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "total_samples = len(train_dataset)\n",
    "class_weights = {class_idx: total_samples / (len(class_counts) * count) for class_idx, count in class_counts.items()}\n",
    "weights = [class_weights[label.item()] for *_, label in train_dataset]\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=False)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler, pin_memory=args.pin_memory, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Sampler, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CustomSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # 각 데이터의 라벨을 가져옵니다.\n",
    "        self.labels = [self.dataset[idx][-1].item() for idx in range(len(self.dataset))]\n",
    "        \n",
    "        # 클래스별로 인덱스를 저장합니다.\n",
    "        self.class_indices = {label: np.where(np.array(self.labels) == label)[0].tolist() for label in np.unique(self.labels)}\n",
    "        \n",
    "        # 클래스별 비율을 계산합니다.\n",
    "        self.class_probs = {label: len(self.class_indices[label]) / len(self.dataset) for label in np.unique(self.labels)}\n",
    "        \n",
    "        # 배치당 각 클래스의 샘플 수를 계산합니다.\n",
    "        total_ratio = sum(self.class_probs.values())\n",
    "        self.batch_class_counts = {label: int(round(self.class_probs[label] * self.batch_size / total_ratio)) for label in np.unique(self.labels)}\n",
    "        \n",
    "        # 인덱스를 생성합니다.\n",
    "        self.indices = self._generate_indices()\n",
    "        \n",
    "    def _generate_indices(self):\n",
    "        indices = []\n",
    "        class_counters = {label: 0 for label in self.class_indices.keys()}\n",
    "        num_batches = len(self.dataset) // self.batch_size\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            batch_indices = []\n",
    "            for label, count in self.batch_class_counts.items():\n",
    "                remaining_samples = len(self.class_indices[label]) - class_counters[label]\n",
    "                \n",
    "                if remaining_samples >= count:\n",
    "                    selected_indices = self.class_indices[label][class_counters[label]:class_counters[label] + count]\n",
    "                else:\n",
    "                    selected_indices = self.class_indices[label][class_counters[label]:]\n",
    "                \n",
    "                batch_indices.extend(selected_indices)\n",
    "                class_counters[label] += len(selected_indices)\n",
    "            \n",
    "            np.random.shuffle(batch_indices)\n",
    "            indices.extend(batch_indices)\n",
    "        \n",
    "        # 남은 샘플들을 마지막에 추가 (만약 남는 경우가 있다면)\n",
    "        remaining_indices = []\n",
    "        for label, count in self.batch_class_counts.items():\n",
    "            remaining_indices.extend(self.class_indices[label][class_counters[label]:])\n",
    "        \n",
    "        if remaining_indices:\n",
    "            np.random.shuffle(remaining_indices)\n",
    "            indices.extend(remaining_indices[:self.batch_size])\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "itemid2idx = pd.read_pickle(\"datasets/entire_itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "\n",
    "# Calculate class weights\n",
    "labels = [label.item() for *_, label in train_dataset]\n",
    "class_counts = Counter(labels)\n",
    "total_samples = len(train_dataset)\n",
    "class_weights = {class_idx: total_samples / (len(class_counts) * count) for class_idx, count in class_counts.items()}\n",
    "weights = [class_weights[label] for label in labels]\n",
    "\n",
    "# Create the custom sampler\n",
    "balanced_sampler = CustomSampler(weights, labels, batch_size=args.batch_size)\n",
    "\n",
    "# DataLoader with custom sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=balanced_sampler, pin_memory=args.pin_memory, num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/135 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    }
   ],
   "source": [
    "for step, batch in tqdm(enumerate(train_loader), desc=\"Steps\", total=len(train_loader)):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred: list indices must be integers or slices, not list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for step, batch in tqdm(enumerate(train_loader), desc=\"Steps\", total=len(train_loader)):\n",
    "            \n",
    "        batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "        input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "        print(labels)\n",
    "        if step == 20:\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"Exception occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6199, 2.5847])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([class_weights[i] for i in range(len(class_weights))], dtype=torch.float32)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3848,  0.2786],\n",
       "        [-0.0680, -0.0041],\n",
       "        [-0.6473,  0.1456],\n",
       "        [-0.2334,  0.2933],\n",
       "        [-0.2564,  0.1482],\n",
       "        [-0.3317,  0.3650],\n",
       "        [-0.5295,  0.4612],\n",
       "        [-0.3901,  0.1189]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.1429])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_weights)\n\u001b[1;32m      7\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sj/lib/python3.9/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "class_counts = np.bincount(a)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(class_weights)\n",
    "loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "loss_fct(a, logits.view(-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels): \n",
    "    probs = F.softmax(predictions, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    labels = labels.view(-1) \n",
    "    \n",
    "    preds_np = preds.cpu().detach().numpy()\n",
    "    labels_np = labels.cpu().detach().numpy()\n",
    "    probs_np = probs[:, 1].cpu().detach().numpy()\n",
    "    \n",
    "    precision = precision_score(labels_np, preds_np, zero_division=0)\n",
    "    recall = recall_score(labels_np, preds_np, zero_division=0)\n",
    "    f1 = f1_score(labels_np, preds_np, zero_division=0)\n",
    "    \n",
    "    if len(np.unique(labels_np)) > 1:\n",
    "        auroc = roc_auc_score(labels_np, probs_np)\n",
    "        auprc = average_precision_score(labels_np, probs_np)\n",
    "    else:\n",
    "        # If only one class is present, return default values\n",
    "        auroc = 0.5  # Equivalent to random guessing\n",
    "        auprc = precision  # For imbalanced data, AUPRC is often similar to precision\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4050, 0.5692],\n",
       "        [0.4830, 0.4990],\n",
       "        [0.3436, 0.5363],\n",
       "        [0.4419, 0.5728],\n",
       "        [0.4362, 0.5370],\n",
       "        [0.4178, 0.5903],\n",
       "        [0.3706, 0.6133],\n",
       "        [0.4037, 0.5297]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.125,\n",
       " 'recall': 1.0,\n",
       " 'f1_score': 0.2222222222222222,\n",
       " 'auroc': 0.2857142857142857,\n",
       " 'auprc': 0.16666666666666666}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (1300456600.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch.argmax(F.softmax(logits, dim=1)).cpu().detach().numpy(), dim=1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "torch.argmax(F.softmax(logits, dim=1)).cpu().detach().numpy(), dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = sum(train_loss)  # 배치 크기를 곱한 손실의 총합\n",
    "total_samples = len(data_loader.dataset)  # 총 샘플 수\n",
    "epoch_loss = total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4585, 0.4122, 0.4537, 0.4502, 0.4237, 0.4980, 0.5277, 0.4858],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(logits[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6112, 0.4998, 0.5627, 0.5789, 0.4968, 0.4744, 0.4868, 0.4722],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(logits)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 0 1 0] [1 0 1 0 1 1 1 0]\n",
      "[0.6111798  0.49978867 0.56268066 0.57892156 0.49677992 0.47443974\n",
      " 0.486832   0.47216398]\n"
     ]
    }
   ],
   "source": [
    "preds_np = torch.argmax(logits, dim=1) .cpu().detach().numpy()\n",
    "labels_np = labels.cpu().detach().numpy()\n",
    "print(preds_np, labels_np)\n",
    "\n",
    "probs_positive_np = torch.sigmoid(logits)[:, 1].cpu().detach().numpy()\n",
    "print(probs_positive_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.5333333333333334\n",
      "0.7295238095238095\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(labels_np, preds_np, zero_division=0, average='binary'))\n",
    "print(recall_score(labels_np, preds_np, zero_division=0, average='binary'))\n",
    "print(f1_score(labels_np, preds_np, zero_division=0, average='binary'))\n",
    "print(roc_auc_score(labels_np, probs_positive_np))\n",
    "print(average_precision_score(labels_np, probs_positive_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in tqdm(enumerate(valid_loader), desc=\"Steps\", total=len(valid_loader)):\n",
    "            \n",
    "    batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "    input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "    print(task_token)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"valid\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode='readmission')\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                        batch_size=args.batch_size, \n",
    "                        shuffle=False,  # Validation should not be shuffled\n",
    "                        pin_memory=args.pin_memory, \n",
    "                        num_workers=args.num_workers,\n",
    "                        )\n",
    "for step, batch in tqdm(enumerate(valid_loader), desc=\"Steps\", total=len(valid_loader)):\n",
    "            \n",
    "    batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "    input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "    print(task_token)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Steps:   1%|          | 10/1076 [00:00<00:16, 64.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "itemid2idx = pd.read_pickle(\"datasets/entire_itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "\n",
    "\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "valid_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"valid\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode=args.mode)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=args.pin_memory, \n",
    "                            num_workers=args.num_workers,\n",
    "                            )\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                        batch_size=args.batch_size, \n",
    "                        shuffle=False,  # Validation should not be shuffled\n",
    "                        pin_memory=args.pin_memory, \n",
    "                        num_workers=args.num_workers,\n",
    "                        )\n",
    "finetune_model = finetune_model.to(args.device)\n",
    "\n",
    "for step, batch in tqdm(enumerate(train_loader), desc=\"Steps\", total=len(train_loader)):\n",
    "            \n",
    "    batch = tuple(t.to(args.device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "    input_ids, attention_mask, age_ids, gender_ids, value_ids, unit_ids, time_ids, continuous_ids, position_ids, token_type_ids, task_token, labels = batch\n",
    "    \n",
    "    # outputs = finetune_model(\n",
    "    #     input_ids = input_ids,\n",
    "    #     value_ids = value_ids,\n",
    "    #     unit_ids = unit_ids,\n",
    "    #     time_ids = time_ids,                \n",
    "    #     continuous_ids = continuous_ids,\n",
    "    #     position_ids = position_ids,\n",
    "    #     token_type_ids = token_type_ids,\n",
    "    #     age_ids = age_ids,\n",
    "    #     gender_ids = gender_ids,\n",
    "    #     task_token = task_token,\n",
    "    #     attention_mask=attention_mask,\n",
    "    #     global_attention_mask=None,\n",
    "    #     labels=labels,\n",
    "    #     return_dict=True,\n",
    "    # )\n",
    "    print(labels)\n",
    "    if step == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1076"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"datasets/mortality30_train_token.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8607"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "    \n",
    "# Required parameters\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"pretrain\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"./results\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--checkpoint_dir\", type=str, default=\"./checkpoints\")\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument(\"--vocab_size\", type=int, default=50265)\n",
    "parser.add_argument(\"--itemid_size\", type=int, default=600)\n",
    "parser.add_argument(\"--unit_size\", type=int, default=60)\n",
    "parser.add_argument(\"--continuous_size\", type=int, default=3)\n",
    "parser.add_argument(\"--task_size\", type=int, default=4)\n",
    "parser.add_argument(\"--max_position_embeddings\", type=int, default=4093)\n",
    "parser.add_argument(\"--max_age\", type=int, default=100)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument(\"--pin_memory\", type=bool, default=True)\n",
    "parser.add_argument(\"--nodes\", type=int, default=1)\n",
    "parser.add_argument(\"--gpus\", type=int, default=1)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=200)\n",
    "parser.add_argument(\"--log_every_n_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--acc\", type=int, default=1)\n",
    "parser.add_argument(\"--resume_checkpoint\", type=str, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "from datasets import EHR_Longformer_Dataset\n",
    "from models.model import LongformerPretrain\n",
    "\n",
    "from utils.utils import seed_everything\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemid2idx = pd.read_pickle(\"datasets/itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "train_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"train\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode='icu_los')\n",
    "valid_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"valid\", tokenizer, itemid2idx, unit2idx, use_itemid=True, mode='icu_los')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    try:\n",
    "        print(train_dataset[idx])\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "    if idx == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = 0\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    \n",
    "    if max_num < max(train_dataset[idx][0]):\n",
    "        max_num = max(train_dataset[idx][0])\n",
    "    \n",
    "\n",
    "for idx in tqdm(range(len(valid_dataset))):\n",
    "    \n",
    "    if max_num < max(valid_dataset[idx][0]):\n",
    "        max_num = max(valid_dataset[idx][0])\n",
    "        \n",
    "print(max_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl \n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "icu_los_train = pd.read_pickle(\"datasets/icu_los_train_token.pkl\")\n",
    "admission_los_train = pd.read_pickle(\"datasets/admission_los_train_token.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6274 2332\n"
     ]
    }
   ],
   "source": [
    "label_0 = 0\n",
    "label_1 = 0\n",
    "for key in icu_los_train.keys():\n",
    "    # print(icu_los_train[key]['label'])\n",
    "    if icu_los_train[key]['label'] == 0:\n",
    "        label_0 += 1\n",
    "    elif icu_los_train[key]['label'] == 1:\n",
    "        label_1 += 1\n",
    "        \n",
    "print(label_0, label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5292 3314\n"
     ]
    }
   ],
   "source": [
    "label_0 = 0\n",
    "label_1 = 0\n",
    "for key in admission_los_train.keys():\n",
    "    # print(icu_los_train[key]['label'])\n",
    "    if admission_los_train[key]['label'] == 0:\n",
    "        label_0 += 1\n",
    "    elif admission_los_train[key]['label'] == 1:\n",
    "        label_1 += 1\n",
    "        \n",
    "print(label_0, label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAHS3/anaconda3/envs/sj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "itemid2idx = pd.read_pickle(\"datasets/entire_itemid2idx.pkl\")\n",
    "unit2idx = pd.read_pickle(\"datasets/unit2idx.pkl\")\n",
    "\n",
    "\n",
    "test_dataset = EHR_Longformer_Dataset(Path(\"./datasets\"), \"test\", tokenizer, itemid2idx, unit2idx, use_itemid=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, args.batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   4,   30,   38,  ...,    0,    0,    0],\n",
      "        [   4,   30,    4,  ...,    0,    0,    0],\n",
      "        [1325,    4,   30,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,   27,   28,  ...,    0,    0,    0],\n",
      "        [   4,   27,   28,  ...,    0,    0,    0],\n",
      "        [-150,    4,   30,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([[48],\n",
      "        [26],\n",
      "        [57],\n",
      "        [77],\n",
      "        [60],\n",
      "        [88],\n",
      "        [68],\n",
      "        [63]]), tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]]), tensor([[ 128.,   32.,   92.,  ...,    0.,    0.,    0.],\n",
      "        [ 106.,   23.,  103.,  ...,    0.,    0.,    0.],\n",
      "        [   1.,   74.,   15.,  ...,    0.,    0.,    0.],\n",
      "        ...,\n",
      "        [ 110.,   49.,   62.,  ...,    0.,    0.,    0.],\n",
      "        [  52.,   90.,  118.,  ...,    0.,    0.,    0.],\n",
      "        [-150.,   61.,   15.,  ...,    0.,    0.,    0.]]), tensor([[   6,    2,    4,  ...,    0,    0,    0],\n",
      "        [   6,    2,    6,  ...,    0,    0,    0],\n",
      "        [  13,    6,    2,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   7,    7,    7,  ...,    0,    0,    0],\n",
      "        [   6,    7,    7,  ...,    0,    0,    0],\n",
      "        [-150,    6,    2,  ...,    0,    0,    0]]), tensor([[ 5.7000e-03,  5.7000e-03,  6.4000e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.5200e-02,  1.5200e-02,  1.5900e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.3700e-01,  1.3900e-01,  1.3900e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.1000e-03,  2.1000e-03,  2.1000e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 6.8000e-03,  6.8000e-03,  6.8000e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.5000e+02,  3.5400e-02,  3.5400e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), tensor([[   2,    2,    2,  ...,    0,    0,    0],\n",
      "        [   2,    2,    2,  ...,    0,    0,    0],\n",
      "        [   2,    2,    2,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,    2,    2,  ...,    0,    0,    0],\n",
      "        [   2,    2,    2,  ...,    0,    0,    0],\n",
      "        [-150,    2,    2,  ...,    0,    0,    0]]), tensor([[1, 1, 2,  ..., 0, 0, 0],\n",
      "        [1, 1, 2,  ..., 0, 0, 0],\n",
      "        [1, 2, 2,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 2, 2,  ..., 0, 0, 0]]), tensor([[   3,    3,    3,  ...,    0,    0,    0],\n",
      "        [   3,    3,    3,  ...,    0,    0,    0],\n",
      "        [   1,    3,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   3,    3,    3,  ...,    0,    0,    0],\n",
      "        [   3,    3,    3,  ...,    0,    0,    0],\n",
      "        [-150,    3,    3,  ...,    0,    0,    0]]), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]), tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [ 339, -100, -100,  ..., -100, -100, -100]])]\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from sklearn.metrics import precision_score\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedType\n",
    "import os\n",
    "from utils.utils import seed_everything\n",
    "from transformers import LongformerTokenizer\n",
    "from datasets import EHR_Longformer_Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from models.longformernormal import LongformerPretrainNormal\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ExponentialLR, LambdaLR, CosineAnnealingWarmRestarts\n",
    "from pretrain_train import train\n",
    "import logging\n",
    "import sys\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "model = LongformerPretrainNormal(\n",
    "        vocab_size=args.vocab_size,\n",
    "        itemid_size=args.itemid_size,\n",
    "        max_position_embeddings=args.max_position_embeddings,\n",
    "        unit_size=args.unit_size,\n",
    "        continuous_size=args.continuous_size,\n",
    "        task_size=args.task_size,\n",
    "        max_age=args.max_age,\n",
    "        gender_size=args.gender_size,\n",
    "        embedding_size=args.embedding_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        dropout_prob=args.dropout_prob,\n",
    "        gpu_mixed_precision=args.gpu_mixed_precision,\n",
    "    ).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
